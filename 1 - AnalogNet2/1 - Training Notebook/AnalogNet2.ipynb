{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnalogNet2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdC_0RMn0XBU",
        "colab_type": "text"
      },
      "source": [
        "New AnalogNet2 architecture reaches 97.3% testing acuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVThWzoLpglS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Htp8g-JA-cDj"
      },
      "source": [
        "#0. Import Data / Utils functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gOnOnLg_-cDd",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KXvDhE_-cDR",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Simulate an input binarization\n",
        "x_train = np.minimum(x_train, 100) // 100 * 120\n",
        "x_test = np.minimum(x_test, 100) // 100 * 120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8L65mkhvSE22"
      },
      "source": [
        "# 1. Network definition and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8LSJT-ZQSE3K"
      },
      "source": [
        "## 1.1 Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OdxuyRYGSE3c",
        "colab": {}
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ym31WeJlSE3k",
        "colab": {}
      },
      "source": [
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "83puxCWmSE3q",
        "colab": {}
      },
      "source": [
        "def network(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1,\n",
        "                     activation_fn=None, biases_initializer=None,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv = binarize_tensor_differentiable(conv, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_DBWynlSE3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4940d4e2-ceb7-40fe-8bf2-74c9b735dda9"
      },
      "source": [
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0903 10:25:14.266215 139742096562048 deprecation.py:323] From <ipython-input-6-bf2c30eb0215>:16: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0903 10:25:14.483978 139742096562048 deprecation.py:323] From <ipython-input-6-bf2c30eb0215>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0903 10:25:14.486419 139742096562048 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CAW7lRxjSE31",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFbHGaWKSE35",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OShYxcw1SE3_",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 25."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbHBQHQYSE4I",
        "colab": {}
      },
      "source": [
        "def customRegularizerConv(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_CONV*np.pi*(x-ROUNDING_STEP_CONV/2))+1.\n",
        "\n",
        "def customRegularizerBias(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_BIAS*np.pi*(x-ROUNDING_STEP_BIAS/2))+1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_967wsFTSE4V",
        "colab": {}
      },
      "source": [
        "NUM_PARAM_REG = 30. + 27. + 27. + 27. + 3.\n",
        "reg_losses = 27./NUM_PARAM_REG *tf.reduce_mean(customRegularizerConv(w1))\n",
        "reg_losses += 3./NUM_PARAM_REG*tf.reduce_mean(customRegularizerBias(thresh))\n",
        "\n",
        "reg_factor_ph = tf.placeholder(tf.float32)\n",
        "loss_with_reg_op = loss_op + reg_factor_ph * reg_losses\n",
        "\n",
        "lr_ph = tf.placeholder(tf.float32)\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr_ph, name='Adam_reg')\n",
        "opt_op = opt.minimize(loss_with_reg_op)\n",
        "sess.run(tf.variables_initializer(opt.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQ2PuUmGSE4b"
      },
      "source": [
        "## 1.2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mXGVPG6kSE4c"
      },
      "source": [
        "Evolving params:\n",
        "- lr_ph : learning rate\n",
        "- reg_factor_ph : regularization factor -> this time, set directly to final value in the second learning stage\n",
        "- bin_rate_ph : binarization rate -> this time, set directly to final value in the third learning stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0DClys1uSE4e",
        "colab": {}
      },
      "source": [
        "INPUT_SCALING = 1. # So that intermediate values stay in the -127, +127 range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lfZn2VmNSE4h",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pddgA9M2SE4l",
        "colab": {}
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "# Bin then reg\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_sBtsD9SE4p",
        "outputId": "a6a27d93-950e-449f-8bf3-348b0fe44064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "    \n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "    \n",
        "    _, current_loss = sess.run([opt_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  lr_ph: lr_feed,\n",
        "                                  reg_factor_ph: reg_factor_feed,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "  \n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 2.8012129905819894\n",
            "Testing Acc.: 0.6512999993562698\n",
            "Epoch 2 completed, average training loss is 0.8566882961988449\n",
            "Testing Acc.: 0.7731000012159348\n",
            "Epoch 3 completed, average training loss is 0.647887048125267\n",
            "Testing Acc.: 0.8019999974966049\n",
            "Epoch 4 completed, average training loss is 0.5606415886431932\n",
            "Testing Acc.: 0.8558999979496003\n",
            "Epoch 5 completed, average training loss is 0.48402199673155943\n",
            "Testing Acc.: 0.8655999982357026\n",
            "Epoch 6 completed, average training loss is 0.4552480389922857\n",
            "Testing Acc.: 0.8712000000476837\n",
            "Epoch 7 completed, average training loss is 0.42331065754095715\n",
            "Testing Acc.: 0.881800000667572\n",
            "Epoch 8 completed, average training loss is 0.39558583254615465\n",
            "Testing Acc.: 0.8994000029563903\n",
            "Epoch 9 completed, average training loss is 0.3749013516306877\n",
            "Testing Acc.: 0.8851999986171722\n",
            "Epoch 10 completed, average training loss is 0.3537087134023507\n",
            "Testing Acc.: 0.9027000015974045\n",
            "Epoch 11 completed, average training loss is 0.333351688409845\n",
            "Testing Acc.: 0.9093000030517578\n",
            "Epoch 12 completed, average training loss is 0.3296041987463832\n",
            "Testing Acc.: 0.90780000269413\n",
            "Epoch 13 completed, average training loss is 0.314291556874911\n",
            "Testing Acc.: 0.8987000042200088\n",
            "Epoch 14 completed, average training loss is 0.30289437419424453\n",
            "Testing Acc.: 0.9212000018358231\n",
            "Epoch 15 completed, average training loss is 0.29267024020353954\n",
            "Testing Acc.: 0.9208000016212463\n",
            "Epoch 16 completed, average training loss is 0.2889155341684818\n",
            "Testing Acc.: 0.9276000022888183\n",
            "Epoch 17 completed, average training loss is 0.2870436125869552\n",
            "Testing Acc.: 0.910200001001358\n",
            "Epoch 18 completed, average training loss is 0.2715984385336439\n",
            "Testing Acc.: 0.9212000030279159\n",
            "Epoch 19 completed, average training loss is 0.2666226302832365\n",
            "Testing Acc.: 0.9293000042438507\n",
            "Epoch 20 completed, average training loss is 0.26798740797986587\n",
            "Testing Acc.: 0.931900001168251\n",
            "Epoch 21 completed, average training loss is 0.2569011707479755\n",
            "Testing Acc.: 0.9312999987602234\n",
            "Epoch 22 completed, average training loss is 0.24749772590895494\n",
            "Testing Acc.: 0.9371000051498413\n",
            "Epoch 23 completed, average training loss is 0.24816597579667965\n",
            "Testing Acc.: 0.8981000012159348\n",
            "Epoch 24 completed, average training loss is 0.24142983013143143\n",
            "Testing Acc.: 0.9429000031948089\n",
            "Epoch 25 completed, average training loss is 0.24179091695696114\n",
            "Testing Acc.: 0.94200000166893\n",
            "Epoch 26 completed, average training loss is 0.23800626318901777\n",
            "Testing Acc.: 0.9308000022172928\n",
            "Epoch 27 completed, average training loss is 0.224592053865393\n",
            "Testing Acc.: 0.9388000047206879\n",
            "Epoch 28 completed, average training loss is 0.22541271378596625\n",
            "Testing Acc.: 0.9422000044584274\n",
            "Epoch 29 completed, average training loss is 0.2129016866410772\n",
            "Testing Acc.: 0.9260000026226044\n",
            "Epoch 30 completed, average training loss is 0.2069907275152703\n",
            "Testing Acc.: 0.9389000022411347\n",
            "Epoch 31 completed, average training loss is 0.2117593840385477\n",
            "Testing Acc.: 0.944200005531311\n",
            "Epoch 32 completed, average training loss is 0.2029144862977167\n",
            "Testing Acc.: 0.9483000040054321\n",
            "Epoch 33 completed, average training loss is 0.20077623271693787\n",
            "Testing Acc.: 0.9467000025510788\n",
            "Epoch 34 completed, average training loss is 0.19628816290137668\n",
            "Testing Acc.: 0.938700003027916\n",
            "Epoch 35 completed, average training loss is 0.19931744216009975\n",
            "Testing Acc.: 0.944200005531311\n",
            "Epoch 36 completed, average training loss is 0.19405326519782345\n",
            "Testing Acc.: 0.9483000040054321\n",
            "Epoch 37 completed, average training loss is 0.1919026265790065\n",
            "Testing Acc.: 0.9381000053882599\n",
            "Epoch 38 completed, average training loss is 0.18968970241645972\n",
            "Testing Acc.: 0.9469000017642974\n",
            "Epoch 39 completed, average training loss is 0.18401607311020296\n",
            "Testing Acc.: 0.942100003361702\n",
            "Epoch 40 completed, average training loss is 0.1861583592618505\n",
            "Testing Acc.: 0.948600001335144\n",
            "Epoch 41 completed, average training loss is 0.18260323906938236\n",
            "Testing Acc.: 0.9484000021219253\n",
            "Epoch 42 completed, average training loss is 0.18471954175581534\n",
            "Testing Acc.: 0.9476000046730042\n",
            "Epoch 43 completed, average training loss is 0.18015888879075648\n",
            "Testing Acc.: 0.9412000024318695\n",
            "Epoch 44 completed, average training loss is 0.17805705996230245\n",
            "Testing Acc.: 0.9513000053167343\n",
            "Epoch 45 completed, average training loss is 0.1892857174575329\n",
            "Testing Acc.: 0.951800001859665\n",
            "Epoch 46 completed, average training loss is 0.17461185560872158\n",
            "Testing Acc.: 0.9504000002145767\n",
            "Epoch 47 completed, average training loss is 0.17265995467702547\n",
            "Testing Acc.: 0.9444000035524368\n",
            "Epoch 48 completed, average training loss is 0.17261176962405445\n",
            "Testing Acc.: 0.9531000012159347\n",
            "Epoch 49 completed, average training loss is 0.17087820113015673\n",
            "Testing Acc.: 0.9385000032186508\n",
            "Epoch 50 completed, average training loss is 0.17069244937350353\n",
            "Testing Acc.: 0.954700003862381\n",
            "Epoch 51 completed, average training loss is 0.1678991756712397\n",
            "Testing Acc.: 0.9503000020980835\n",
            "Epoch 52 completed, average training loss is 0.1643993742329379\n",
            "Testing Acc.: 0.9584000015258789\n",
            "Epoch 53 completed, average training loss is 0.16301440146441262\n",
            "Testing Acc.: 0.9594000035524368\n",
            "Epoch 54 completed, average training loss is 0.16567942540161312\n",
            "Testing Acc.: 0.9492000007629394\n",
            "Epoch 55 completed, average training loss is 0.16509192469840248\n",
            "Testing Acc.: 0.9482000041007995\n",
            "Epoch 56 completed, average training loss is 0.16540807615965605\n",
            "Testing Acc.: 0.9362000024318695\n",
            "Epoch 57 completed, average training loss is 0.15978082725778223\n",
            "Testing Acc.: 0.9507000011205673\n",
            "Epoch 58 completed, average training loss is 0.1616137854754925\n",
            "Testing Acc.: 0.9577000039815903\n",
            "Epoch 59 completed, average training loss is 0.15863416848083337\n",
            "Testing Acc.: 0.9437000036239624\n",
            "Epoch 60 completed, average training loss is 0.15850839526702962\n",
            "Testing Acc.: 0.9556000012159348\n",
            "Epoch 61 completed, average training loss is 0.15976966872500878\n",
            "Testing Acc.: 0.9560999995470048\n",
            "Epoch 62 completed, average training loss is 0.15853584646247326\n",
            "Testing Acc.: 0.9509000033140182\n",
            "Epoch 63 completed, average training loss is 0.1546565017197281\n",
            "Testing Acc.: 0.9636000061035156\n",
            "Epoch 64 completed, average training loss is 0.15768138013159236\n",
            "Testing Acc.: 0.9451000034809113\n",
            "Epoch 65 completed, average training loss is 0.15190908506512643\n",
            "Testing Acc.: 0.954600002169609\n",
            "Epoch 66 completed, average training loss is 0.15238695309807856\n",
            "Testing Acc.: 0.9569000059366226\n",
            "Epoch 67 completed, average training loss is 0.1558464633549253\n",
            "Testing Acc.: 0.9555000054836273\n",
            "Epoch 68 completed, average training loss is 0.1538870966496567\n",
            "Testing Acc.: 0.9474000024795532\n",
            "Epoch 69 completed, average training loss is 0.1513940625358373\n",
            "Testing Acc.: 0.9553000038862228\n",
            "Epoch 70 completed, average training loss is 0.1515317274319629\n",
            "Testing Acc.: 0.9639000064134597\n",
            "Epoch 71 completed, average training loss is 0.1479868049857517\n",
            "Testing Acc.: 0.957500005364418\n",
            "Epoch 72 completed, average training loss is 0.15184253229449193\n",
            "Testing Acc.: 0.9591000038385391\n",
            "Epoch 73 completed, average training loss is 0.1469507118035108\n",
            "Testing Acc.: 0.9589000052213669\n",
            "Epoch 74 completed, average training loss is 0.14618098364211618\n",
            "Testing Acc.: 0.9594000047445297\n",
            "Epoch 75 completed, average training loss is 0.14543290461724004\n",
            "Testing Acc.: 0.9585000050067901\n",
            "Epoch 76 completed, average training loss is 0.14442996301688255\n",
            "Testing Acc.: 0.9602000051736832\n",
            "Epoch 77 completed, average training loss is 0.1397976130526513\n",
            "Testing Acc.: 0.9634000039100648\n",
            "Epoch 78 completed, average training loss is 0.14074969246673089\n",
            "Testing Acc.: 0.9644000065326691\n",
            "Epoch 79 completed, average training loss is 0.14014774152698617\n",
            "Testing Acc.: 0.9608000051975251\n",
            "Epoch 80 completed, average training loss is 0.13944704023345064\n",
            "Testing Acc.: 0.9537000030279159\n",
            "Epoch 81 completed, average training loss is 0.13879883131633203\n",
            "Testing Acc.: 0.9640000027418136\n",
            "Epoch 82 completed, average training loss is 0.13389098576890926\n",
            "Testing Acc.: 0.9442000043392181\n",
            "Epoch 83 completed, average training loss is 0.136809403821826\n",
            "Testing Acc.: 0.9650000059604644\n",
            "Epoch 84 completed, average training loss is 0.13441746312193573\n",
            "Testing Acc.: 0.9606000059843063\n",
            "Epoch 85 completed, average training loss is 0.13287051574327052\n",
            "Testing Acc.: 0.9630000030994416\n",
            "Epoch 86 completed, average training loss is 0.13252336940728127\n",
            "Testing Acc.: 0.9621000051498413\n",
            "Epoch 87 completed, average training loss is 0.13449423058579366\n",
            "Testing Acc.: 0.9617000031471252\n",
            "Epoch 88 completed, average training loss is 0.1288114603733023\n",
            "Testing Acc.: 0.9654000061750412\n",
            "Epoch 89 completed, average training loss is 0.13324804802425205\n",
            "Testing Acc.: 0.9637000048160553\n",
            "Epoch 90 completed, average training loss is 0.13197180395325026\n",
            "Testing Acc.: 0.9611000072956085\n",
            "Epoch 91 completed, average training loss is 0.1292746303975582\n",
            "Testing Acc.: 0.9623000037670135\n",
            "Epoch 92 completed, average training loss is 0.12960499173030257\n",
            "Testing Acc.: 0.9673000073432922\n",
            "Epoch 93 completed, average training loss is 0.13140963406302034\n",
            "Testing Acc.: 0.9618000030517578\n",
            "Epoch 94 completed, average training loss is 0.1296175804361701\n",
            "Testing Acc.: 0.9638000053167343\n",
            "Epoch 95 completed, average training loss is 0.12681359188941618\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 96 completed, average training loss is 0.1267420272808522\n",
            "Testing Acc.: 0.9579000020027161\n",
            "Epoch 97 completed, average training loss is 0.12750422781333326\n",
            "Testing Acc.: 0.9642000031471253\n",
            "Epoch 98 completed, average training loss is 0.1249187697066615\n",
            "Testing Acc.: 0.9590000057220459\n",
            "Epoch 99 completed, average training loss is 0.12665590018499642\n",
            "Testing Acc.: 0.9649000054597855\n",
            "Epoch 100 completed, average training loss is 0.12688029217533767\n",
            "Testing Acc.: 0.9633000040054321\n",
            "Epoch 101 completed, average training loss is 0.12615016180401048\n",
            "Testing Acc.: 0.9596000039577484\n",
            "Epoch 102 completed, average training loss is 0.12494434876057009\n",
            "Testing Acc.: 0.9685000044107437\n",
            "Epoch 103 completed, average training loss is 0.12355596121400594\n",
            "Testing Acc.: 0.9607000058889389\n",
            "Epoch 104 completed, average training loss is 0.1259695313544944\n",
            "Testing Acc.: 0.9653000032901764\n",
            "Epoch 105 completed, average training loss is 0.12553100384150942\n",
            "Testing Acc.: 0.9678000068664551\n",
            "Epoch 106 completed, average training loss is 0.12371064714156091\n",
            "Testing Acc.: 0.9564000034332275\n",
            "Epoch 107 completed, average training loss is 0.12179190595323841\n",
            "Testing Acc.: 0.9635000067949295\n",
            "Epoch 108 completed, average training loss is 0.12365277011878789\n",
            "Testing Acc.: 0.9614000058174134\n",
            "Epoch 109 completed, average training loss is 0.12370165216891715\n",
            "Testing Acc.: 0.965400002002716\n",
            "Epoch 110 completed, average training loss is 0.12094803518305222\n",
            "Testing Acc.: 0.9600000035762787\n",
            "Epoch 111 completed, average training loss is 0.12181760150939226\n",
            "Testing Acc.: 0.9679000061750412\n",
            "Epoch 112 completed, average training loss is 0.11982253794558347\n",
            "Testing Acc.: 0.9596000021696091\n",
            "Epoch 113 completed, average training loss is 0.1229585951504608\n",
            "Testing Acc.: 0.9633000034093857\n",
            "Epoch 114 completed, average training loss is 0.11783734926022589\n",
            "Testing Acc.: 0.9673000073432922\n",
            "Epoch 115 completed, average training loss is 0.12283278355064492\n",
            "Testing Acc.: 0.9561000025272369\n",
            "Epoch 116 completed, average training loss is 0.11871698390382032\n",
            "Testing Acc.: 0.9651000052690506\n",
            "Epoch 117 completed, average training loss is 0.11840739972578983\n",
            "Testing Acc.: 0.9564000034332275\n",
            "Epoch 118 completed, average training loss is 0.11827186659909784\n",
            "Testing Acc.: 0.9638000059127808\n",
            "Epoch 119 completed, average training loss is 0.11867203208462646\n",
            "Testing Acc.: 0.9638000082969665\n",
            "Epoch 120 completed, average training loss is 0.11806486934113006\n",
            "Testing Acc.: 0.9641000044345855\n",
            "Epoch 121 completed, average training loss is 0.117189715251637\n",
            "Testing Acc.: 0.9676000052690505\n",
            "Epoch 122 completed, average training loss is 0.11651604337462534\n",
            "Testing Acc.: 0.9608000028133392\n",
            "Epoch 123 completed, average training loss is 0.12006506567510465\n",
            "Testing Acc.: 0.9671000051498413\n",
            "Epoch 124 completed, average training loss is 0.11712612548843027\n",
            "Testing Acc.: 0.9614000058174134\n",
            "Epoch 125 completed, average training loss is 0.11599553556802372\n",
            "Testing Acc.: 0.963200005888939\n",
            "Epoch 126 completed, average training loss is 0.11712271300144493\n",
            "Testing Acc.: 0.969000004529953\n",
            "Epoch 127 completed, average training loss is 0.1179922556411475\n",
            "Testing Acc.: 0.9665000069141388\n",
            "Epoch 128 completed, average training loss is 0.11453349402950455\n",
            "Testing Acc.: 0.9598000067472457\n",
            "Epoch 129 completed, average training loss is 0.11656984047498554\n",
            "Testing Acc.: 0.9602000063657761\n",
            "Epoch 130 completed, average training loss is 0.11630371773615479\n",
            "Testing Acc.: 0.9688000065088272\n",
            "Epoch 131 completed, average training loss is 0.11656627371441573\n",
            "Testing Acc.: 0.964400007724762\n",
            "Epoch 132 completed, average training loss is 0.11383719550135235\n",
            "Testing Acc.: 0.9671000075340271\n",
            "Epoch 133 completed, average training loss is 0.11616830402829995\n",
            "Testing Acc.: 0.9672000050544739\n",
            "Epoch 134 completed, average training loss is 0.11465010256196062\n",
            "Testing Acc.: 0.9704000079631805\n",
            "Epoch 135 completed, average training loss is 0.11577763754253587\n",
            "Testing Acc.: 0.9630000084638596\n",
            "Epoch 136 completed, average training loss is 0.11180801279221972\n",
            "Testing Acc.: 0.9689000087976456\n",
            "Epoch 137 completed, average training loss is 0.1129908044760426\n",
            "Testing Acc.: 0.9682000082731247\n",
            "Epoch 138 completed, average training loss is 0.11274879997440924\n",
            "Testing Acc.: 0.9668000078201294\n",
            "Epoch 139 completed, average training loss is 0.11234909898911913\n",
            "Testing Acc.: 0.9554000025987626\n",
            "Epoch 140 completed, average training loss is 0.11262640935989718\n",
            "Testing Acc.: 0.9645000094175339\n",
            "Epoch 141 completed, average training loss is 0.11403562130872161\n",
            "Testing Acc.: 0.9583000040054321\n",
            "Epoch 142 completed, average training loss is 0.11094287212084358\n",
            "Testing Acc.: 0.9696000063419342\n",
            "Epoch 143 completed, average training loss is 0.111656834819975\n",
            "Testing Acc.: 0.9664000058174134\n",
            "Epoch 144 completed, average training loss is 0.11277373687674602\n",
            "Testing Acc.: 0.9684000062942505\n",
            "Epoch 145 completed, average training loss is 0.11270230196571598\n",
            "Testing Acc.: 0.9620000022649765\n",
            "Epoch 146 completed, average training loss is 0.11105364476330579\n",
            "Testing Acc.: 0.966700006723404\n",
            "Epoch 147 completed, average training loss is 0.1097123216992865\n",
            "Testing Acc.: 0.9629000037908554\n",
            "Epoch 148 completed, average training loss is 0.1080983091176798\n",
            "Testing Acc.: 0.9651000064611435\n",
            "Epoch 149 completed, average training loss is 0.11280445572764923\n",
            "Testing Acc.: 0.969200005531311\n",
            "Epoch 150 completed, average training loss is 0.10967340453372647\n",
            "Testing Acc.: 0.9702000039815902\n",
            "Epoch 151 completed, average training loss is 0.10849325938615947\n",
            "Testing Acc.: 0.9703000074625016\n",
            "Epoch 152 completed, average training loss is 0.11064284346687298\n",
            "Testing Acc.: 0.963300005197525\n",
            "Epoch 153 completed, average training loss is 0.11176336710030833\n",
            "Testing Acc.: 0.9675000035762786\n",
            "Epoch 154 completed, average training loss is 0.108140469424737\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 155 completed, average training loss is 0.11128598554835965\n",
            "Testing Acc.: 0.9665000027418137\n",
            "Epoch 156 completed, average training loss is 0.10749805262157072\n",
            "Testing Acc.: 0.9657000082731247\n",
            "Epoch 157 completed, average training loss is 0.11022440617438406\n",
            "Testing Acc.: 0.9659000080823898\n",
            "Epoch 158 completed, average training loss is 0.10738209538782637\n",
            "Testing Acc.: 0.9609000045061111\n",
            "Epoch 159 completed, average training loss is 0.11144178349835178\n",
            "Testing Acc.: 0.9694000059366226\n",
            "Epoch 160 completed, average training loss is 0.10830433156186094\n",
            "Testing Acc.: 0.9629000073671341\n",
            "Epoch 161 completed, average training loss is 0.10556510798477878\n",
            "Testing Acc.: 0.9684000074863434\n",
            "Epoch 162 completed, average training loss is 0.10638664211922635\n",
            "Testing Acc.: 0.9503000062704087\n",
            "Epoch 163 completed, average training loss is 0.11037519776728004\n",
            "Testing Acc.: 0.9651000076532363\n",
            "Epoch 164 completed, average training loss is 0.10959892219553391\n",
            "Testing Acc.: 0.968600006699562\n",
            "Epoch 165 completed, average training loss is 0.1084535628495117\n",
            "Testing Acc.: 0.9694000053405761\n",
            "Epoch 166 completed, average training loss is 0.10729839126113802\n",
            "Testing Acc.: 0.9655000060796738\n",
            "Epoch 167 completed, average training loss is 0.10610263245180249\n",
            "Testing Acc.: 0.9682000052928924\n",
            "Epoch 168 completed, average training loss is 0.10337748270171385\n",
            "Testing Acc.: 0.9644000071287155\n",
            "Epoch 169 completed, average training loss is 0.10675801114644855\n",
            "Testing Acc.: 0.9657000064849853\n",
            "Epoch 170 completed, average training loss is 0.10426517597244432\n",
            "Testing Acc.: 0.9658000057935715\n",
            "Epoch 171 completed, average training loss is 0.1041729357233271\n",
            "Testing Acc.: 0.9666000038385392\n",
            "Epoch 172 completed, average training loss is 0.10499042961125572\n",
            "Testing Acc.: 0.968100004196167\n",
            "Epoch 173 completed, average training loss is 0.10358055479824543\n",
            "Testing Acc.: 0.9665000051259994\n",
            "Epoch 174 completed, average training loss is 0.10314051383330176\n",
            "Testing Acc.: 0.9696000081300735\n",
            "Epoch 175 completed, average training loss is 0.10507598399184644\n",
            "Testing Acc.: 0.9685000061988831\n",
            "Epoch 176 completed, average training loss is 0.10153015244131287\n",
            "Testing Acc.: 0.9710000038146973\n",
            "Epoch 177 completed, average training loss is 0.10162861837695042\n",
            "Testing Acc.: 0.9672000050544739\n",
            "Epoch 178 completed, average training loss is 0.10265186700349052\n",
            "Testing Acc.: 0.9666000044345856\n",
            "Epoch 179 completed, average training loss is 0.10172700938923905\n",
            "Testing Acc.: 0.9683000057935714\n",
            "Epoch 180 completed, average training loss is 0.10159148722421378\n",
            "Testing Acc.: 0.9705000042915344\n",
            "Epoch 181 completed, average training loss is 0.10122533388901502\n",
            "Testing Acc.: 0.9726000040769577\n",
            "Epoch 182 completed, average training loss is 0.10210171073675156\n",
            "Testing Acc.: 0.9679000055789948\n",
            "Epoch 183 completed, average training loss is 0.09999744870777552\n",
            "Testing Acc.: 0.9717000049352645\n",
            "Epoch 184 completed, average training loss is 0.10020316261022041\n",
            "Testing Acc.: 0.9700000041723251\n",
            "Epoch 185 completed, average training loss is 0.09904258819762618\n",
            "Testing Acc.: 0.9630000078678131\n",
            "Epoch 186 completed, average training loss is 0.09882831553307672\n",
            "Testing Acc.: 0.9607000023126602\n",
            "Epoch 187 completed, average training loss is 0.09710460478284706\n",
            "Testing Acc.: 0.9704000055789948\n",
            "Epoch 188 completed, average training loss is 0.09779409076242397\n",
            "Testing Acc.: 0.969500007033348\n",
            "Epoch 189 completed, average training loss is 0.09649899877452602\n",
            "Testing Acc.: 0.9700000047683716\n",
            "Epoch 190 completed, average training loss is 0.0969815355197837\n",
            "Testing Acc.: 0.9696000021696091\n",
            "Epoch 191 completed, average training loss is 0.09607198439460869\n",
            "Testing Acc.: 0.9701000034809113\n",
            "Epoch 192 completed, average training loss is 0.0949076504074037\n",
            "Testing Acc.: 0.9714000046253204\n",
            "Epoch 193 completed, average training loss is 0.0968207269286116\n",
            "Testing Acc.: 0.9712000072002411\n",
            "Epoch 194 completed, average training loss is 0.09449835761915892\n",
            "Testing Acc.: 0.9735000050067901\n",
            "Epoch 195 completed, average training loss is 0.09602471841499209\n",
            "Testing Acc.: 0.9696000051498413\n",
            "Epoch 196 completed, average training loss is 0.09330355562114467\n",
            "Testing Acc.: 0.973800003528595\n",
            "Epoch 197 completed, average training loss is 0.09360614301015933\n",
            "Testing Acc.: 0.9712000042200089\n",
            "Epoch 198 completed, average training loss is 0.09261414834763855\n",
            "Testing Acc.: 0.975900006890297\n",
            "Epoch 199 completed, average training loss is 0.09244238065710912\n",
            "Testing Acc.: 0.9734000051021576\n",
            "Epoch 200 completed, average training loss is 0.09185487605631351\n",
            "Testing Acc.: 0.973200004696846\n",
            "Epoch 201 completed, average training loss is 0.11216426946222782\n",
            "Testing Acc.: 0.9657000082731247\n",
            "Epoch 202 completed, average training loss is 0.10715021525509655\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 203 completed, average training loss is 0.10467421816661954\n",
            "Testing Acc.: 0.9653000050783157\n",
            "Epoch 204 completed, average training loss is 0.10325606827779363\n",
            "Testing Acc.: 0.9660000050067902\n",
            "Epoch 205 completed, average training loss is 0.10353795594225328\n",
            "Testing Acc.: 0.9660000044107437\n",
            "Epoch 206 completed, average training loss is 0.10189017367859682\n",
            "Testing Acc.: 0.9681000024080276\n",
            "Epoch 207 completed, average training loss is 0.10081811357444773\n",
            "Testing Acc.: 0.9675000011920929\n",
            "Epoch 208 completed, average training loss is 0.10036319572168093\n",
            "Testing Acc.: 0.9667000031471252\n",
            "Epoch 209 completed, average training loss is 0.10031619616163273\n",
            "Testing Acc.: 0.9679000049829483\n",
            "Epoch 210 completed, average training loss is 0.10088916620860497\n",
            "Testing Acc.: 0.9669000047445298\n",
            "Epoch 211 completed, average training loss is 0.10048495432982842\n",
            "Testing Acc.: 0.9693000042438507\n",
            "Epoch 212 completed, average training loss is 0.1001264148702224\n",
            "Testing Acc.: 0.9688000065088272\n",
            "Epoch 213 completed, average training loss is 0.09920780185299614\n",
            "Testing Acc.: 0.9682000041007995\n",
            "Epoch 214 completed, average training loss is 0.09878373939078301\n",
            "Testing Acc.: 0.9688000029325485\n",
            "Epoch 215 completed, average training loss is 0.09791929666884243\n",
            "Testing Acc.: 0.9669000059366226\n",
            "Epoch 216 completed, average training loss is 0.09930999601104608\n",
            "Testing Acc.: 0.9659000045061111\n",
            "Epoch 217 completed, average training loss is 0.09848064591487249\n",
            "Testing Acc.: 0.9679000037908554\n",
            "Epoch 218 completed, average training loss is 0.09857447184311847\n",
            "Testing Acc.: 0.9699000048637391\n",
            "Epoch 219 completed, average training loss is 0.09826893718137096\n",
            "Testing Acc.: 0.9702000033855438\n",
            "Epoch 220 completed, average training loss is 0.09726463253299396\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 221 completed, average training loss is 0.09729773624334484\n",
            "Testing Acc.: 0.9700000023841858\n",
            "Epoch 222 completed, average training loss is 0.09710982121216755\n",
            "Testing Acc.: 0.9675000029802322\n",
            "Epoch 223 completed, average training loss is 0.09701902452235421\n",
            "Testing Acc.: 0.9685000061988831\n",
            "Epoch 224 completed, average training loss is 0.0968024381576106\n",
            "Testing Acc.: 0.9689000034332276\n",
            "Epoch 225 completed, average training loss is 0.09695287335664034\n",
            "Testing Acc.: 0.971200003027916\n",
            "Epoch 226 completed, average training loss is 0.09786488226925333\n",
            "Testing Acc.: 0.9655000048875809\n",
            "Epoch 227 completed, average training loss is 0.09541260911617427\n",
            "Testing Acc.: 0.9647000032663345\n",
            "Epoch 228 completed, average training loss is 0.09765176706016064\n",
            "Testing Acc.: 0.9684000056982041\n",
            "Epoch 229 completed, average training loss is 0.09511364927360168\n",
            "Testing Acc.: 0.9682000035047531\n",
            "Epoch 230 completed, average training loss is 0.096091942563653\n",
            "Testing Acc.: 0.968900004029274\n",
            "Epoch 231 completed, average training loss is 0.0952520225259165\n",
            "Testing Acc.: 0.9671000063419342\n",
            "Epoch 232 completed, average training loss is 0.09550137628180286\n",
            "Testing Acc.: 0.966900006532669\n",
            "Epoch 233 completed, average training loss is 0.0952383034164086\n",
            "Testing Acc.: 0.9670000046491622\n",
            "Epoch 234 completed, average training loss is 0.09527866890498747\n",
            "Testing Acc.: 0.9706000036001206\n",
            "Epoch 235 completed, average training loss is 0.09522273420821875\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 236 completed, average training loss is 0.0960888513814037\n",
            "Testing Acc.: 0.96980000436306\n",
            "Epoch 237 completed, average training loss is 0.09550416665462157\n",
            "Testing Acc.: 0.9684000027179718\n",
            "Epoch 238 completed, average training loss is 0.09589552511073028\n",
            "Testing Acc.: 0.9695000052452087\n",
            "Epoch 239 completed, average training loss is 0.09459364235245933\n",
            "Testing Acc.: 0.9678000026941299\n",
            "Epoch 240 completed, average training loss is 0.09468745717623582\n",
            "Testing Acc.: 0.9704000043869019\n",
            "Epoch 241 completed, average training loss is 0.09345182821309815\n",
            "Testing Acc.: 0.9697000026702881\n",
            "Epoch 242 completed, average training loss is 0.09390262244269251\n",
            "Testing Acc.: 0.9689000052213669\n",
            "Epoch 243 completed, average training loss is 0.09461305869122347\n",
            "Testing Acc.: 0.9690000039339065\n",
            "Epoch 244 completed, average training loss is 0.09543354485960057\n",
            "Testing Acc.: 0.9710000032186508\n",
            "Epoch 245 completed, average training loss is 0.09344623726870244\n",
            "Testing Acc.: 0.9680000030994416\n",
            "Epoch 246 completed, average training loss is 0.09284817101589093\n",
            "Testing Acc.: 0.9675000035762786\n",
            "Epoch 247 completed, average training loss is 0.09344394897576422\n",
            "Testing Acc.: 0.9685000044107437\n",
            "Epoch 248 completed, average training loss is 0.09386530310846865\n",
            "Testing Acc.: 0.9683000028133393\n",
            "Epoch 249 completed, average training loss is 0.09395221427083016\n",
            "Testing Acc.: 0.9682000035047531\n",
            "Epoch 250 completed, average training loss is 0.09430479370057583\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 251 completed, average training loss is 0.09328985067084432\n",
            "Testing Acc.: 0.9703000050783157\n",
            "Epoch 252 completed, average training loss is 0.09339576416648925\n",
            "Testing Acc.: 0.9689000052213669\n",
            "Epoch 253 completed, average training loss is 0.09328301048371941\n",
            "Testing Acc.: 0.9664000028371811\n",
            "Epoch 254 completed, average training loss is 0.09400716972537339\n",
            "Testing Acc.: 0.9691000068187714\n",
            "Epoch 255 completed, average training loss is 0.09398310109817733\n",
            "Testing Acc.: 0.9712000042200089\n",
            "Epoch 256 completed, average training loss is 0.09234493091547241\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 257 completed, average training loss is 0.09375819747491429\n",
            "Testing Acc.: 0.9681000065803528\n",
            "Epoch 258 completed, average training loss is 0.0933352827684333\n",
            "Testing Acc.: 0.9695000040531159\n",
            "Epoch 259 completed, average training loss is 0.0933503804045419\n",
            "Testing Acc.: 0.9663000017404556\n",
            "Epoch 260 completed, average training loss is 0.09224160818848759\n",
            "Testing Acc.: 0.9693000054359436\n",
            "Epoch 261 completed, average training loss is 0.0923243970113496\n",
            "Testing Acc.: 0.9703000038862228\n",
            "Epoch 262 completed, average training loss is 0.09302687587837379\n",
            "Testing Acc.: 0.9712000054121017\n",
            "Epoch 263 completed, average training loss is 0.09159692080381016\n",
            "Testing Acc.: 0.9703000032901764\n",
            "Epoch 264 completed, average training loss is 0.09220410178725919\n",
            "Testing Acc.: 0.970900005698204\n",
            "Epoch 265 completed, average training loss is 0.09194110652897507\n",
            "Testing Acc.: 0.9691000044345855\n",
            "Epoch 266 completed, average training loss is 0.09203818850840131\n",
            "Testing Acc.: 0.9682000023126602\n",
            "Epoch 267 completed, average training loss is 0.09245484459213912\n",
            "Testing Acc.: 0.9696000021696091\n",
            "Epoch 268 completed, average training loss is 0.09203379422736664\n",
            "Testing Acc.: 0.9720000040531158\n",
            "Epoch 269 completed, average training loss is 0.09285654383401076\n",
            "Testing Acc.: 0.9703000038862228\n",
            "Epoch 270 completed, average training loss is 0.09149501705697427\n",
            "Testing Acc.: 0.9699000030755996\n",
            "Epoch 271 completed, average training loss is 0.09258687923196703\n",
            "Testing Acc.: 0.9712000036239624\n",
            "Epoch 272 completed, average training loss is 0.09020406680647283\n",
            "Testing Acc.: 0.9682000052928924\n",
            "Epoch 273 completed, average training loss is 0.09110279524388412\n",
            "Testing Acc.: 0.9700000017881394\n",
            "Epoch 274 completed, average training loss is 0.09051650920572381\n",
            "Testing Acc.: 0.96730000436306\n",
            "Epoch 275 completed, average training loss is 0.09135055664073055\n",
            "Testing Acc.: 0.970500003695488\n",
            "Epoch 276 completed, average training loss is 0.09010198769314835\n",
            "Testing Acc.: 0.9699000030755996\n",
            "Epoch 277 completed, average training loss is 0.0907607454744478\n",
            "Testing Acc.: 0.9685000056028366\n",
            "Epoch 278 completed, average training loss is 0.09062103914096951\n",
            "Testing Acc.: 0.9684000039100646\n",
            "Epoch 279 completed, average training loss is 0.09141788095701486\n",
            "Testing Acc.: 0.9687000030279159\n",
            "Epoch 280 completed, average training loss is 0.09067286029302825\n",
            "Testing Acc.: 0.9660000032186509\n",
            "Epoch 281 completed, average training loss is 0.09164434546759972\n",
            "Testing Acc.: 0.9692000043392182\n",
            "Epoch 282 completed, average training loss is 0.09112333708908409\n",
            "Testing Acc.: 0.9695000040531159\n",
            "Epoch 283 completed, average training loss is 0.09171144259317468\n",
            "Testing Acc.: 0.9699000006914139\n",
            "Epoch 284 completed, average training loss is 0.09034429108258336\n",
            "Testing Acc.: 0.9722000056505203\n",
            "Epoch 285 completed, average training loss is 0.08992008655487249\n",
            "Testing Acc.: 0.9706000024080277\n",
            "Epoch 286 completed, average training loss is 0.09129384400167813\n",
            "Testing Acc.: 0.9707000017166137\n",
            "Epoch 287 completed, average training loss is 0.09100083313727131\n",
            "Testing Acc.: 0.9707000017166137\n",
            "Epoch 288 completed, average training loss is 0.08976879928261042\n",
            "Testing Acc.: 0.9719000035524368\n",
            "Epoch 289 completed, average training loss is 0.08974428907036781\n",
            "Testing Acc.: 0.9700000035762787\n",
            "Epoch 290 completed, average training loss is 0.08976617858667547\n",
            "Testing Acc.: 0.9697000032663345\n",
            "Epoch 291 completed, average training loss is 0.08942742077168077\n",
            "Testing Acc.: 0.9699000048637391\n",
            "Epoch 292 completed, average training loss is 0.08909526322968304\n",
            "Testing Acc.: 0.9735000026226044\n",
            "Epoch 293 completed, average training loss is 0.08993654473219066\n",
            "Testing Acc.: 0.9718000066280365\n",
            "Epoch 294 completed, average training loss is 0.08932559137387822\n",
            "Testing Acc.: 0.9702000057697296\n",
            "Epoch 295 completed, average training loss is 0.09044929565085719\n",
            "Testing Acc.: 0.9716000038385392\n",
            "Epoch 296 completed, average training loss is 0.09036882620770484\n",
            "Testing Acc.: 0.9708000051975251\n",
            "Epoch 297 completed, average training loss is 0.09102661369989316\n",
            "Testing Acc.: 0.9726000028848648\n",
            "Epoch 298 completed, average training loss is 0.08935291810271641\n",
            "Testing Acc.: 0.9714000058174134\n",
            "Epoch 299 completed, average training loss is 0.08919429157394916\n",
            "Testing Acc.: 0.971900001168251\n",
            "Epoch 300 completed, average training loss is 0.08961386893487845\n",
            "Testing Acc.: 0.9719000035524368\n",
            "Epoch 301 completed, average training loss is 0.08958531285015245\n",
            "Testing Acc.: 0.9717000025510788\n",
            "Epoch 302 completed, average training loss is 0.08946013227881243\n",
            "Testing Acc.: 0.9678000050783158\n",
            "Epoch 303 completed, average training loss is 0.08874954609510799\n",
            "Testing Acc.: 0.9716000056266785\n",
            "Epoch 304 completed, average training loss is 0.08969403692986816\n",
            "Testing Acc.: 0.9711000025272369\n",
            "Epoch 305 completed, average training loss is 0.09033512259523074\n",
            "Testing Acc.: 0.9715000039339066\n",
            "Epoch 306 completed, average training loss is 0.0885133773398896\n",
            "Testing Acc.: 0.9708000034093857\n",
            "Epoch 307 completed, average training loss is 0.08925493647810072\n",
            "Testing Acc.: 0.9700000047683716\n",
            "Epoch 308 completed, average training loss is 0.08791321985268344\n",
            "Testing Acc.: 0.9711000025272369\n",
            "Epoch 309 completed, average training loss is 0.08964352353631208\n",
            "Testing Acc.: 0.9708000034093857\n",
            "Epoch 310 completed, average training loss is 0.08922903132469703\n",
            "Testing Acc.: 0.9723000019788742\n",
            "Epoch 311 completed, average training loss is 0.08858279764807472\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 312 completed, average training loss is 0.08821092819018911\n",
            "Testing Acc.: 0.9721000051498413\n",
            "Epoch 313 completed, average training loss is 0.08898444246500731\n",
            "Testing Acc.: 0.972200003862381\n",
            "Epoch 314 completed, average training loss is 0.08791115165532877\n",
            "Testing Acc.: 0.9676000040769577\n",
            "Epoch 315 completed, average training loss is 0.08867629397970934\n",
            "Testing Acc.: 0.9724000042676926\n",
            "Epoch 316 completed, average training loss is 0.08901762021084626\n",
            "Testing Acc.: 0.968600002527237\n",
            "Epoch 317 completed, average training loss is 0.08888666978882005\n",
            "Testing Acc.: 0.9686000049114227\n",
            "Epoch 318 completed, average training loss is 0.08791595112842818\n",
            "Testing Acc.: 0.9723000025749207\n",
            "Epoch 319 completed, average training loss is 0.08768419102455179\n",
            "Testing Acc.: 0.9704000037908554\n",
            "Epoch 320 completed, average training loss is 0.08735523795864235\n",
            "Testing Acc.: 0.972100003361702\n",
            "Epoch 321 completed, average training loss is 0.08924325485015289\n",
            "Testing Acc.: 0.971500004529953\n",
            "Epoch 322 completed, average training loss is 0.08818879812334975\n",
            "Testing Acc.: 0.9714000046253204\n",
            "Epoch 323 completed, average training loss is 0.08722441617554674\n",
            "Testing Acc.: 0.9709000045061111\n",
            "Epoch 324 completed, average training loss is 0.08898974999086931\n",
            "Testing Acc.: 0.9734000009298325\n",
            "Epoch 325 completed, average training loss is 0.08807723752999057\n",
            "Testing Acc.: 0.9656000077724457\n",
            "Epoch 326 completed, average training loss is 0.08806148887611925\n",
            "Testing Acc.: 0.9685000044107437\n",
            "Epoch 327 completed, average training loss is 0.0873770789274325\n",
            "Testing Acc.: 0.9707000029087066\n",
            "Epoch 328 completed, average training loss is 0.08818506499985233\n",
            "Testing Acc.: 0.9707000023126602\n",
            "Epoch 329 completed, average training loss is 0.08800638869094352\n",
            "Testing Acc.: 0.972900003194809\n",
            "Epoch 330 completed, average training loss is 0.08836922189531227\n",
            "Testing Acc.: 0.9701000034809113\n",
            "Epoch 331 completed, average training loss is 0.08790656379579256\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 332 completed, average training loss is 0.08757509038938831\n",
            "Testing Acc.: 0.9694000029563904\n",
            "Epoch 333 completed, average training loss is 0.08933635196415707\n",
            "Testing Acc.: 0.9718000048398971\n",
            "Epoch 334 completed, average training loss is 0.08679810707302144\n",
            "Testing Acc.: 0.9714000028371811\n",
            "Epoch 335 completed, average training loss is 0.08699768004007638\n",
            "Testing Acc.: 0.9711000037193298\n",
            "Epoch 336 completed, average training loss is 0.08787450454197825\n",
            "Testing Acc.: 0.9716000038385392\n",
            "Epoch 337 completed, average training loss is 0.0864806206136321\n",
            "Testing Acc.: 0.9712000000476837\n",
            "Epoch 338 completed, average training loss is 0.08739597918310513\n",
            "Testing Acc.: 0.9727000033855439\n",
            "Epoch 339 completed, average training loss is 0.08741572697026034\n",
            "Testing Acc.: 0.9694000029563904\n",
            "Epoch 340 completed, average training loss is 0.08687733732474347\n",
            "Testing Acc.: 0.968600006699562\n",
            "Epoch 341 completed, average training loss is 0.08756058677099646\n",
            "Testing Acc.: 0.9720000034570694\n",
            "Epoch 342 completed, average training loss is 0.08652853526951124\n",
            "Testing Acc.: 0.9700000041723251\n",
            "Epoch 343 completed, average training loss is 0.08662086113976936\n",
            "Testing Acc.: 0.9709000051021576\n",
            "Epoch 344 completed, average training loss is 0.0863821388144667\n",
            "Testing Acc.: 0.9721000009775161\n",
            "Epoch 345 completed, average training loss is 0.08631220120160531\n",
            "Testing Acc.: 0.9707000041007996\n",
            "Epoch 346 completed, average training loss is 0.08689107538821797\n",
            "Testing Acc.: 0.9725000017881393\n",
            "Epoch 347 completed, average training loss is 0.0867078561289236\n",
            "Testing Acc.: 0.9723000025749207\n",
            "Epoch 348 completed, average training loss is 0.08695429223434378\n",
            "Testing Acc.: 0.968000003695488\n",
            "Epoch 349 completed, average training loss is 0.08671895646179716\n",
            "Testing Acc.: 0.9731000036001205\n",
            "Epoch 350 completed, average training loss is 0.08730996816419065\n",
            "Testing Acc.: 0.9687000042200089\n",
            "Epoch 351 completed, average training loss is 0.08627254849765449\n",
            "Testing Acc.: 0.9716000038385392\n",
            "Epoch 352 completed, average training loss is 0.08727460233184199\n",
            "Testing Acc.: 0.9710000044107437\n",
            "Epoch 353 completed, average training loss is 0.08673908840011184\n",
            "Testing Acc.: 0.9715000039339066\n",
            "Epoch 354 completed, average training loss is 0.08694302677642554\n",
            "Testing Acc.: 0.9694000047445297\n",
            "Epoch 355 completed, average training loss is 0.08606417203632494\n",
            "Testing Acc.: 0.9714000046253204\n",
            "Epoch 356 completed, average training loss is 0.08600850162561983\n",
            "Testing Acc.: 0.971200003027916\n",
            "Epoch 357 completed, average training loss is 0.08664500775126119\n",
            "Testing Acc.: 0.973000003695488\n",
            "Epoch 358 completed, average training loss is 0.08628452653065324\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 359 completed, average training loss is 0.08715842580888421\n",
            "Testing Acc.: 0.9698000019788742\n",
            "Epoch 360 completed, average training loss is 0.08548948088971277\n",
            "Testing Acc.: 0.9714000052213669\n",
            "Epoch 361 completed, average training loss is 0.08646845018491149\n",
            "Testing Acc.: 0.9695000046491623\n",
            "Epoch 362 completed, average training loss is 0.08581519161816686\n",
            "Testing Acc.: 0.9717000037431717\n",
            "Epoch 363 completed, average training loss is 0.08664065513449411\n",
            "Testing Acc.: 0.9710000038146973\n",
            "Epoch 364 completed, average training loss is 0.08717057297006249\n",
            "Testing Acc.: 0.9699000036716461\n",
            "Epoch 365 completed, average training loss is 0.08574669428169727\n",
            "Testing Acc.: 0.9733000028133393\n",
            "Epoch 366 completed, average training loss is 0.08605154155908773\n",
            "Testing Acc.: 0.970000006556511\n",
            "Epoch 367 completed, average training loss is 0.08511805489348868\n",
            "Testing Acc.: 0.966500004529953\n",
            "Epoch 368 completed, average training loss is 0.0862812558018292\n",
            "Testing Acc.: 0.9715000033378601\n",
            "Epoch 369 completed, average training loss is 0.08601811872640004\n",
            "Testing Acc.: 0.9722000002861023\n",
            "Epoch 370 completed, average training loss is 0.0861800491794323\n",
            "Testing Acc.: 0.9737000042200088\n",
            "Epoch 371 completed, average training loss is 0.08529880450417598\n",
            "Testing Acc.: 0.9714000034332275\n",
            "Epoch 372 completed, average training loss is 0.08708231793250888\n",
            "Testing Acc.: 0.970900005698204\n",
            "Epoch 373 completed, average training loss is 0.08538392780426268\n",
            "Testing Acc.: 0.9716000026464462\n",
            "Epoch 374 completed, average training loss is 0.08653459895324583\n",
            "Testing Acc.: 0.9700000029802323\n",
            "Epoch 375 completed, average training loss is 0.08444253596787651\n",
            "Testing Acc.: 0.9686000031232834\n",
            "Epoch 376 completed, average training loss is 0.08546879929490388\n",
            "Testing Acc.: 0.9707000035047532\n",
            "Epoch 377 completed, average training loss is 0.08455429777580624\n",
            "Testing Acc.: 0.9716000038385392\n",
            "Epoch 378 completed, average training loss is 0.08512730175939699\n",
            "Testing Acc.: 0.9700000041723251\n",
            "Epoch 379 completed, average training loss is 0.08651655524037778\n",
            "Testing Acc.: 0.9737000054121018\n",
            "Epoch 380 completed, average training loss is 0.0857757522802179\n",
            "Testing Acc.: 0.9723000025749207\n",
            "Epoch 381 completed, average training loss is 0.08573403846782943\n",
            "Testing Acc.: 0.9702000045776367\n",
            "Epoch 382 completed, average training loss is 0.08519149894826114\n",
            "Testing Acc.: 0.9714000034332275\n",
            "Epoch 383 completed, average training loss is 0.08511857314268127\n",
            "Testing Acc.: 0.9713000029325485\n",
            "Epoch 384 completed, average training loss is 0.08428754923244318\n",
            "Testing Acc.: 0.972100003361702\n",
            "Epoch 385 completed, average training loss is 0.08603902923408896\n",
            "Testing Acc.: 0.9719000035524368\n",
            "Epoch 386 completed, average training loss is 0.08430568073876202\n",
            "Testing Acc.: 0.9723000049591064\n",
            "Epoch 387 completed, average training loss is 0.08558377481997013\n",
            "Testing Acc.: 0.9702000021934509\n",
            "Epoch 388 completed, average training loss is 0.08578362700529396\n",
            "Testing Acc.: 0.96480000436306\n",
            "Epoch 389 completed, average training loss is 0.08469698117269824\n",
            "Testing Acc.: 0.9703000050783157\n",
            "Epoch 390 completed, average training loss is 0.08515558590491613\n",
            "Testing Acc.: 0.9695000022649765\n",
            "Epoch 391 completed, average training loss is 0.08540102467872203\n",
            "Testing Acc.: 0.9685000050067901\n",
            "Epoch 392 completed, average training loss is 0.08541535049676895\n",
            "Testing Acc.: 0.9721000027656556\n",
            "Epoch 393 completed, average training loss is 0.08466211977725227\n",
            "Testing Acc.: 0.9695000046491623\n",
            "Epoch 394 completed, average training loss is 0.08457442944403738\n",
            "Testing Acc.: 0.9721000015735626\n",
            "Epoch 395 completed, average training loss is 0.08467141451469312\n",
            "Testing Acc.: 0.9731000024080276\n",
            "Epoch 396 completed, average training loss is 0.08568808758476128\n",
            "Testing Acc.: 0.9699000054597855\n",
            "Epoch 397 completed, average training loss is 0.08533456429063033\n",
            "Testing Acc.: 0.9717000025510788\n",
            "Epoch 398 completed, average training loss is 0.08480018864075343\n",
            "Testing Acc.: 0.9719000029563903\n",
            "Epoch 399 completed, average training loss is 0.08509474992596855\n",
            "Testing Acc.: 0.9710000044107437\n",
            "Epoch 400 completed, average training loss is 0.08430094087962062\n",
            "Testing Acc.: 0.9718000018596649\n",
            "Epoch 401 completed, average training loss is 0.14247674542634436\n",
            "Testing Acc.: 0.9721000039577484\n",
            "Epoch 402 completed, average training loss is 0.0888719585413734\n",
            "Testing Acc.: 0.9701000010967255\n",
            "Epoch 403 completed, average training loss is 0.08729587651633967\n",
            "Testing Acc.: 0.9700000005960464\n",
            "Epoch 404 completed, average training loss is 0.08608482281832645\n",
            "Testing Acc.: 0.970900000333786\n",
            "Epoch 405 completed, average training loss is 0.08477079510688781\n",
            "Testing Acc.: 0.9717000019550324\n",
            "Epoch 406 completed, average training loss is 0.08440900690155104\n",
            "Testing Acc.: 0.9707999998331069\n",
            "Epoch 407 completed, average training loss is 0.08374027563879888\n",
            "Testing Acc.: 0.9709000015258789\n",
            "Epoch 408 completed, average training loss is 0.08366868018483122\n",
            "Testing Acc.: 0.9698000013828277\n",
            "Epoch 409 completed, average training loss is 0.08346336322991799\n",
            "Testing Acc.: 0.9712000018358231\n",
            "Epoch 410 completed, average training loss is 0.08258805876287321\n",
            "Testing Acc.: 0.9700999999046326\n",
            "Epoch 411 completed, average training loss is 0.08301507180711876\n",
            "Testing Acc.: 0.9713000029325485\n",
            "Epoch 412 completed, average training loss is 0.08237662929265449\n",
            "Testing Acc.: 0.9713000029325485\n",
            "Epoch 413 completed, average training loss is 0.08202071532607079\n",
            "Testing Acc.: 0.9705000019073486\n",
            "Epoch 414 completed, average training loss is 0.08177440002017344\n",
            "Testing Acc.: 0.9717000025510788\n",
            "Epoch 415 completed, average training loss is 0.08186340895015747\n",
            "Testing Acc.: 0.972100003361702\n",
            "Epoch 416 completed, average training loss is 0.08133497588802129\n",
            "Testing Acc.: 0.9714000016450882\n",
            "Epoch 417 completed, average training loss is 0.08157950711436569\n",
            "Testing Acc.: 0.9719000035524368\n",
            "Epoch 418 completed, average training loss is 0.08079707097262144\n",
            "Testing Acc.: 0.9730000030994416\n",
            "Epoch 419 completed, average training loss is 0.08057958774811899\n",
            "Testing Acc.: 0.9707000035047532\n",
            "Epoch 420 completed, average training loss is 0.0807286562025547\n",
            "Testing Acc.: 0.9721000021696091\n",
            "Epoch 421 completed, average training loss is 0.08086278195803365\n",
            "Testing Acc.: 0.9723000025749207\n",
            "Epoch 422 completed, average training loss is 0.0807085252351438\n",
            "Testing Acc.: 0.9724000024795533\n",
            "Epoch 423 completed, average training loss is 0.08082129778340459\n",
            "Testing Acc.: 0.9717000013589859\n",
            "Epoch 424 completed, average training loss is 0.0805994304564471\n",
            "Testing Acc.: 0.9719000017642975\n",
            "Epoch 425 completed, average training loss is 0.0799559841894855\n",
            "Testing Acc.: 0.9723000007867814\n",
            "Epoch 426 completed, average training loss is 0.07965823704299206\n",
            "Testing Acc.: 0.9725999999046325\n",
            "Epoch 427 completed, average training loss is 0.07981925559307759\n",
            "Testing Acc.: 0.9723000013828278\n",
            "Epoch 428 completed, average training loss is 0.07948946612110983\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 429 completed, average training loss is 0.07930577429166684\n",
            "Testing Acc.: 0.9726000016927719\n",
            "Epoch 430 completed, average training loss is 0.07954674593793849\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 431 completed, average training loss is 0.07975789939674238\n",
            "Testing Acc.: 0.9727000004053116\n",
            "Epoch 432 completed, average training loss is 0.07953539618756622\n",
            "Testing Acc.: 0.9717000049352645\n",
            "Epoch 433 completed, average training loss is 0.07889487728709355\n",
            "Testing Acc.: 0.9703000050783157\n",
            "Epoch 434 completed, average training loss is 0.07913797076791525\n",
            "Testing Acc.: 0.9732000035047531\n",
            "Epoch 435 completed, average training loss is 0.07917909026922038\n",
            "Testing Acc.: 0.9729000002145767\n",
            "Epoch 436 completed, average training loss is 0.07941076813420901\n",
            "Testing Acc.: 0.9727000004053116\n",
            "Epoch 437 completed, average training loss is 0.07850802469688158\n",
            "Testing Acc.: 0.9725000005960465\n",
            "Epoch 438 completed, average training loss is 0.07855580077584212\n",
            "Testing Acc.: 0.9718000030517578\n",
            "Epoch 439 completed, average training loss is 0.07852410278671111\n",
            "Testing Acc.: 0.9727000015974044\n",
            "Epoch 440 completed, average training loss is 0.07838675764466946\n",
            "Testing Acc.: 0.9707000029087066\n",
            "Epoch 441 completed, average training loss is 0.07869309969712049\n",
            "Testing Acc.: 0.9725000023841858\n",
            "Epoch 442 completed, average training loss is 0.07819971229260167\n",
            "Testing Acc.: 0.9718000030517578\n",
            "Epoch 443 completed, average training loss is 0.07796748723834752\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 444 completed, average training loss is 0.07793907544187581\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 445 completed, average training loss is 0.07796731220015014\n",
            "Testing Acc.: 0.9718000036478043\n",
            "Epoch 446 completed, average training loss is 0.07816945845649267\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 447 completed, average training loss is 0.07732988978968933\n",
            "Testing Acc.: 0.9704000043869019\n",
            "Epoch 448 completed, average training loss is 0.07822292893271272\n",
            "Testing Acc.: 0.9721000045537949\n",
            "Epoch 449 completed, average training loss is 0.07788776808573554\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 450 completed, average training loss is 0.07793159740899379\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 451 completed, average training loss is 0.07788196198021372\n",
            "Testing Acc.: 0.9729000025987625\n",
            "Epoch 452 completed, average training loss is 0.07743616974990195\n",
            "Testing Acc.: 0.9728000050783158\n",
            "Epoch 453 completed, average training loss is 0.0775837541045621\n",
            "Testing Acc.: 0.9724000024795533\n",
            "Epoch 454 completed, average training loss is 0.07775995683157816\n",
            "Testing Acc.: 0.9714000028371811\n",
            "Epoch 455 completed, average training loss is 0.07780084499468407\n",
            "Testing Acc.: 0.9725000029802322\n",
            "Epoch 456 completed, average training loss is 0.07722631171656151\n",
            "Testing Acc.: 0.9723000007867814\n",
            "Epoch 457 completed, average training loss is 0.07764804046213006\n",
            "Testing Acc.: 0.9726000034809112\n",
            "Epoch 458 completed, average training loss is 0.07701627205281208\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 459 completed, average training loss is 0.07725934742949903\n",
            "Testing Acc.: 0.9723000013828278\n",
            "Epoch 460 completed, average training loss is 0.077848259160916\n",
            "Testing Acc.: 0.9736000025272369\n",
            "Epoch 461 completed, average training loss is 0.0770345878150935\n",
            "Testing Acc.: 0.9717000013589859\n",
            "Epoch 462 completed, average training loss is 0.07674016812505821\n",
            "Testing Acc.: 0.9729000025987625\n",
            "Epoch 463 completed, average training loss is 0.07732747062575072\n",
            "Testing Acc.: 0.9728000026941299\n",
            "Epoch 464 completed, average training loss is 0.07720449645693103\n",
            "Testing Acc.: 0.9727000015974044\n",
            "Epoch 465 completed, average training loss is 0.07685916365589947\n",
            "Testing Acc.: 0.9726000028848648\n",
            "Epoch 466 completed, average training loss is 0.07746260927058757\n",
            "Testing Acc.: 0.9726000016927719\n",
            "Epoch 467 completed, average training loss is 0.07681252141405517\n",
            "Testing Acc.: 0.9721000015735626\n",
            "Epoch 468 completed, average training loss is 0.07657170939880113\n",
            "Testing Acc.: 0.9728000038862228\n",
            "Epoch 469 completed, average training loss is 0.07681411322982361\n",
            "Testing Acc.: 0.972200003862381\n",
            "Epoch 470 completed, average training loss is 0.07696811880140254\n",
            "Testing Acc.: 0.9730000048875809\n",
            "Epoch 471 completed, average training loss is 0.07671748781499142\n",
            "Testing Acc.: 0.9732999992370606\n",
            "Epoch 472 completed, average training loss is 0.07679164399082462\n",
            "Testing Acc.: 0.9719000023603439\n",
            "Epoch 473 completed, average training loss is 0.07679592247353867\n",
            "Testing Acc.: 0.9720000016689301\n",
            "Epoch 474 completed, average training loss is 0.07621282982019087\n",
            "Testing Acc.: 0.971300002336502\n",
            "Epoch 475 completed, average training loss is 0.07643269668333233\n",
            "Testing Acc.: 0.9729000025987625\n",
            "Epoch 476 completed, average training loss is 0.07619044162798673\n",
            "Testing Acc.: 0.9737000018358231\n",
            "Epoch 477 completed, average training loss is 0.07613544619564588\n",
            "Testing Acc.: 0.971900001168251\n",
            "Epoch 478 completed, average training loss is 0.07614951395584892\n",
            "Testing Acc.: 0.9721000015735626\n",
            "Epoch 479 completed, average training loss is 0.07623738873827582\n",
            "Testing Acc.: 0.9729000025987625\n",
            "Epoch 480 completed, average training loss is 0.07653083333590378\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 481 completed, average training loss is 0.07692680781862388\n",
            "Testing Acc.: 0.9730000019073486\n",
            "Epoch 482 completed, average training loss is 0.0759481705337142\n",
            "Testing Acc.: 0.9736000007390976\n",
            "Epoch 483 completed, average training loss is 0.0766459608124569\n",
            "Testing Acc.: 0.9718000042438507\n",
            "Epoch 484 completed, average training loss is 0.07598859982875486\n",
            "Testing Acc.: 0.9724000036716461\n",
            "Epoch 485 completed, average training loss is 0.0754726219169485\n",
            "Testing Acc.: 0.9741000026464463\n",
            "Epoch 486 completed, average training loss is 0.07593398281993965\n",
            "Testing Acc.: 0.9726000034809112\n",
            "Epoch 487 completed, average training loss is 0.0759251336970677\n",
            "Testing Acc.: 0.9732000023126602\n",
            "Epoch 488 completed, average training loss is 0.07667016031686216\n",
            "Testing Acc.: 0.9734000015258789\n",
            "Epoch 489 completed, average training loss is 0.07596301103709266\n",
            "Testing Acc.: 0.9720000016689301\n",
            "Epoch 490 completed, average training loss is 0.07573962890698265\n",
            "Testing Acc.: 0.9726000010967255\n",
            "Epoch 491 completed, average training loss is 0.07598108209824811\n",
            "Testing Acc.: 0.9737000018358231\n",
            "Epoch 492 completed, average training loss is 0.0755294424155727\n",
            "Testing Acc.: 0.9718000024557114\n",
            "Epoch 493 completed, average training loss is 0.07598479590456311\n",
            "Testing Acc.: 0.9729000020027161\n",
            "Epoch 494 completed, average training loss is 0.07539813865286608\n",
            "Testing Acc.: 0.9730000042915344\n",
            "Epoch 495 completed, average training loss is 0.07629020336084068\n",
            "Testing Acc.: 0.9727000027894974\n",
            "Epoch 496 completed, average training loss is 0.07503536632284522\n",
            "Testing Acc.: 0.9720000022649765\n",
            "Epoch 497 completed, average training loss is 0.0756685517091925\n",
            "Testing Acc.: 0.9724000030755997\n",
            "Epoch 498 completed, average training loss is 0.07524346963269636\n",
            "Testing Acc.: 0.9737000042200088\n",
            "Epoch 499 completed, average training loss is 0.07574649898956219\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 500 completed, average training loss is 0.07567014527196686\n",
            "Testing Acc.: 0.9731000024080276\n",
            "Epoch 501 completed, average training loss is 0.07535014748573303\n",
            "Testing Acc.: 0.9730000030994416\n",
            "Epoch 502 completed, average training loss is 0.0747851015239333\n",
            "Testing Acc.: 0.973600001335144\n",
            "Epoch 503 completed, average training loss is 0.07553383486500631\n",
            "Testing Acc.: 0.9714000046253204\n",
            "Epoch 504 completed, average training loss is 0.07516655937613298\n",
            "Testing Acc.: 0.9727000015974044\n",
            "Epoch 505 completed, average training loss is 0.07553482038434595\n",
            "Testing Acc.: 0.9730000013113022\n",
            "Epoch 506 completed, average training loss is 0.07512348119635134\n",
            "Testing Acc.: 0.9727000051736832\n",
            "Epoch 507 completed, average training loss is 0.07506018469032522\n",
            "Testing Acc.: 0.9736000019311905\n",
            "Epoch 508 completed, average training loss is 0.07457099087846776\n",
            "Testing Acc.: 0.9740000033378601\n",
            "Epoch 509 completed, average training loss is 0.07485993872784699\n",
            "Testing Acc.: 0.9729000014066697\n",
            "Epoch 510 completed, average training loss is 0.07495056658051907\n",
            "Testing Acc.: 0.9724000036716461\n",
            "Epoch 511 completed, average training loss is 0.07482442148107414\n",
            "Testing Acc.: 0.9737000036239624\n",
            "Epoch 512 completed, average training loss is 0.07513574629478778\n",
            "Testing Acc.: 0.9719000017642975\n",
            "Epoch 513 completed, average training loss is 0.07477671846592178\n",
            "Testing Acc.: 0.9715000015497207\n",
            "Epoch 514 completed, average training loss is 0.07505759817703317\n",
            "Testing Acc.: 0.9724000036716461\n",
            "Epoch 515 completed, average training loss is 0.07484633147638912\n",
            "Testing Acc.: 0.9728000020980835\n",
            "Epoch 516 completed, average training loss is 0.07477610178524628\n",
            "Testing Acc.: 0.9719000047445298\n",
            "Epoch 517 completed, average training loss is 0.0746331734318907\n",
            "Testing Acc.: 0.9722000032663345\n",
            "Epoch 518 completed, average training loss is 0.07472755336202681\n",
            "Testing Acc.: 0.971300003528595\n",
            "Epoch 519 completed, average training loss is 0.07471786071468765\n",
            "Testing Acc.: 0.9736000025272369\n",
            "Epoch 520 completed, average training loss is 0.0746967836190015\n",
            "Testing Acc.: 0.9737999999523163\n",
            "Epoch 521 completed, average training loss is 0.0746576334303245\n",
            "Testing Acc.: 0.9718000042438507\n",
            "Epoch 522 completed, average training loss is 0.07437620850124707\n",
            "Testing Acc.: 0.9726999998092651\n",
            "Epoch 523 completed, average training loss is 0.07454154269459347\n",
            "Testing Acc.: 0.9725000005960465\n",
            "Epoch 524 completed, average training loss is 0.0745680897271571\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 525 completed, average training loss is 0.07446005381488552\n",
            "Testing Acc.: 0.9721000015735626\n",
            "Epoch 526 completed, average training loss is 0.0748230924232242\n",
            "Testing Acc.: 0.973000003695488\n",
            "Epoch 527 completed, average training loss is 0.07469514165151243\n",
            "Testing Acc.: 0.971900001168251\n",
            "Epoch 528 completed, average training loss is 0.0747094911787038\n",
            "Testing Acc.: 0.9733000022172927\n",
            "Epoch 529 completed, average training loss is 0.07462829009979031\n",
            "Testing Acc.: 0.9724000030755997\n",
            "Epoch 530 completed, average training loss is 0.07448193117510528\n",
            "Testing Acc.: 0.9728000026941299\n",
            "Epoch 531 completed, average training loss is 0.07444880394653107\n",
            "Testing Acc.: 0.9727000033855439\n",
            "Epoch 532 completed, average training loss is 0.07426065740485986\n",
            "Testing Acc.: 0.9732000035047531\n",
            "Epoch 533 completed, average training loss is 0.07426036358810961\n",
            "Testing Acc.: 0.9713000029325485\n",
            "Epoch 534 completed, average training loss is 0.07401496753872683\n",
            "Testing Acc.: 0.9725000011920929\n",
            "Epoch 535 completed, average training loss is 0.07419154459765802\n",
            "Testing Acc.: 0.9720000034570694\n",
            "Epoch 536 completed, average training loss is 0.07393830069728817\n",
            "Testing Acc.: 0.972600000500679\n",
            "Epoch 537 completed, average training loss is 0.07423061162854235\n",
            "Testing Acc.: 0.972700001001358\n",
            "Epoch 538 completed, average training loss is 0.07443054390605539\n",
            "Testing Acc.: 0.972200002670288\n",
            "Epoch 539 completed, average training loss is 0.07409145547387501\n",
            "Testing Acc.: 0.9733000028133393\n",
            "Epoch 540 completed, average training loss is 0.07459068476843338\n",
            "Testing Acc.: 0.9733000010251999\n",
            "Epoch 541 completed, average training loss is 0.07417395415172602\n",
            "Testing Acc.: 0.9716000014543533\n",
            "Epoch 542 completed, average training loss is 0.07390983306647589\n",
            "Testing Acc.: 0.972800001502037\n",
            "Epoch 543 completed, average training loss is 0.07373260443021233\n",
            "Testing Acc.: 0.9720000016689301\n",
            "Epoch 544 completed, average training loss is 0.07448416741099209\n",
            "Testing Acc.: 0.9729000002145767\n",
            "Epoch 545 completed, average training loss is 0.07415841398295014\n",
            "Testing Acc.: 0.9739000022411346\n",
            "Epoch 546 completed, average training loss is 0.07429209623873854\n",
            "Testing Acc.: 0.9726000028848648\n",
            "Epoch 547 completed, average training loss is 0.0750543228117749\n",
            "Testing Acc.: 0.9724000024795533\n",
            "Epoch 548 completed, average training loss is 0.07288013625890016\n",
            "Testing Acc.: 0.9731000024080276\n",
            "Epoch 549 completed, average training loss is 0.07380063894903287\n",
            "Testing Acc.: 0.9724000024795533\n",
            "Epoch 550 completed, average training loss is 0.07380071709048934\n",
            "Testing Acc.: 0.9739000016450882\n",
            "Epoch 551 completed, average training loss is 0.07342015624977648\n",
            "Testing Acc.: 0.9717000013589859\n",
            "Epoch 552 completed, average training loss is 0.07363612012704834\n",
            "Testing Acc.: 0.9715000015497207\n",
            "Epoch 553 completed, average training loss is 0.07333032290606449\n",
            "Testing Acc.: 0.9723000013828278\n",
            "Epoch 554 completed, average training loss is 0.07296615530891964\n",
            "Testing Acc.: 0.9704000025987625\n",
            "Epoch 555 completed, average training loss is 0.07369449514895678\n",
            "Testing Acc.: 0.9735000002384185\n",
            "Epoch 556 completed, average training loss is 0.07357494394139698\n",
            "Testing Acc.: 0.9733999997377396\n",
            "Epoch 557 completed, average training loss is 0.07455643186190476\n",
            "Testing Acc.: 0.9742000025510787\n",
            "Epoch 558 completed, average training loss is 0.07361140078632161\n",
            "Testing Acc.: 0.9729000037908554\n",
            "Epoch 559 completed, average training loss is 0.07323674763397624\n",
            "Testing Acc.: 0.9735000014305115\n",
            "Epoch 560 completed, average training loss is 0.07346841463974367\n",
            "Testing Acc.: 0.9722000032663345\n",
            "Epoch 561 completed, average training loss is 0.0734944217139855\n",
            "Testing Acc.: 0.9729000002145767\n",
            "Epoch 562 completed, average training loss is 0.07320663104842727\n",
            "Testing Acc.: 0.9737000036239624\n",
            "Epoch 563 completed, average training loss is 0.07324520026566461\n",
            "Testing Acc.: 0.9730000030994416\n",
            "Epoch 564 completed, average training loss is 0.0731152733291189\n",
            "Testing Acc.: 0.9728000020980835\n",
            "Epoch 565 completed, average training loss is 0.07347536619907866\n",
            "Testing Acc.: 0.9724000018835067\n",
            "Epoch 566 completed, average training loss is 0.07329801204691952\n",
            "Testing Acc.: 0.972900003194809\n",
            "Epoch 567 completed, average training loss is 0.07369260861072689\n",
            "Testing Acc.: 0.9725\n",
            "Epoch 568 completed, average training loss is 0.07371603194701795\n",
            "Testing Acc.: 0.9726000016927719\n",
            "Epoch 569 completed, average training loss is 0.07334270741247262\n",
            "Testing Acc.: 0.9718000018596649\n",
            "Epoch 570 completed, average training loss is 0.07327246333550042\n",
            "Testing Acc.: 0.9713000029325485\n",
            "Epoch 571 completed, average training loss is 0.07323435999841119\n",
            "Testing Acc.: 0.971600005030632\n",
            "Epoch 572 completed, average training loss is 0.073493554314288\n",
            "Testing Acc.: 0.9722000032663345\n",
            "Epoch 573 completed, average training loss is 0.07270272333640605\n",
            "Testing Acc.: 0.9737000030279159\n",
            "Epoch 574 completed, average training loss is 0.0734429295702527\n",
            "Testing Acc.: 0.9733000022172927\n",
            "Epoch 575 completed, average training loss is 0.07317496725125239\n",
            "Testing Acc.: 0.9732000017166138\n",
            "Epoch 576 completed, average training loss is 0.07325220988752941\n",
            "Testing Acc.: 0.9737000018358231\n",
            "Epoch 577 completed, average training loss is 0.07300746535571913\n",
            "Testing Acc.: 0.9729000008106232\n",
            "Epoch 578 completed, average training loss is 0.07345765776311358\n",
            "Testing Acc.: 0.9723000019788742\n",
            "Epoch 579 completed, average training loss is 0.07340559268603102\n",
            "Testing Acc.: 0.9734000015258789\n",
            "Epoch 580 completed, average training loss is 0.07353312484687194\n",
            "Testing Acc.: 0.9733000004291534\n",
            "Epoch 581 completed, average training loss is 0.07266218655509875\n",
            "Testing Acc.: 0.9727000015974044\n",
            "Epoch 582 completed, average training loss is 0.07288411763496697\n",
            "Testing Acc.: 0.9736000037193299\n",
            "Epoch 583 completed, average training loss is 0.07338741016030932\n",
            "Testing Acc.: 0.9721000021696091\n",
            "Epoch 584 completed, average training loss is 0.07269641274198269\n",
            "Testing Acc.: 0.9728000020980835\n",
            "Epoch 585 completed, average training loss is 0.07273596378586565\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 586 completed, average training loss is 0.07297789186627293\n",
            "Testing Acc.: 0.9725000011920929\n",
            "Epoch 587 completed, average training loss is 0.07305679069133475\n",
            "Testing Acc.: 0.973500000834465\n",
            "Epoch 588 completed, average training loss is 0.07302108500152826\n",
            "Testing Acc.: 0.9703000038862228\n",
            "Epoch 589 completed, average training loss is 0.07279294215918829\n",
            "Testing Acc.: 0.9738000023365021\n",
            "Epoch 590 completed, average training loss is 0.07267224646445053\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 591 completed, average training loss is 0.0728559443882356\n",
            "Testing Acc.: 0.9737000024318695\n",
            "Epoch 592 completed, average training loss is 0.07320517687437435\n",
            "Testing Acc.: 0.9716000026464462\n",
            "Epoch 593 completed, average training loss is 0.07292501587343092\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 594 completed, average training loss is 0.0727400802836443\n",
            "Testing Acc.: 0.9728000032901764\n",
            "Epoch 595 completed, average training loss is 0.07248636790784076\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 596 completed, average training loss is 0.07241019775547708\n",
            "Testing Acc.: 0.9722000002861023\n",
            "Epoch 597 completed, average training loss is 0.07227387306590875\n",
            "Testing Acc.: 0.9736000031232834\n",
            "Epoch 598 completed, average training loss is 0.07308859881091243\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 599 completed, average training loss is 0.07261475064558909\n",
            "Testing Acc.: 0.9729000014066697\n",
            "Epoch 600 completed, average training loss is 0.07214291953093682\n",
            "Testing Acc.: 0.9724000024795533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7Y5lj0USE4t",
        "outputId": "c34f39aa-2b09-4765-8b83-0270ceb49eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ^^^^ Above reads:\n",
        "### Epoch 600 completed, average training loss is 0.07214291953093682\n",
        "### Testing Acc.: 0.9724000024795533\n",
        "\n",
        "\n",
        "# Save model, for not having to fully retrain it each time.\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, 'AnalogNet2/model.ckpt')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AnalogNet2/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq-uQ3fPSE4x",
        "outputId": "66a755cd-7f96-4dbd-bf49-eb7899be011f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9724000024795533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C3TzktgYSE41",
        "outputId": "b09b61cb-5f9d-4aad-97b7-829cf1f890f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!zip -r AnalogNet2.zip AnalogNet2"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: AnalogNet2/ (stored 0%)\n",
            "  adding: AnalogNet2/model.ckpt.data-00000-of-00001 (deflated 25%)\n",
            "  adding: AnalogNet2/model.ckpt.index (deflated 40%)\n",
            "  adding: AnalogNet2/checkpoint (deflated 42%)\n",
            "  adding: AnalogNet2/model.ckpt.meta (deflated 88%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1B3LcU1fSE45"
      },
      "source": [
        "## 1.3 Weights rounding: 97.46%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "77c6e76d-604b-434b-f704-3278de688d23",
        "id": "pPCCsZY0SE46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!unzip AnalogNet2.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  unquantised.zip\n",
            "replace unquantised/model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR8IboGzSE4-",
        "outputId": "e8aad46d-b856-4881-b423-0f918e186898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)\n",
        "\n",
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool\n",
        "\n",
        "def network(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1,\n",
        "                     activation_fn=None, biases_initializer=None,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv = binarize_tensor_differentiable(conv, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out\n",
        "\n",
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "ckpt = tf.train.get_checkpoint_state('AnalogNet2')\n",
        "saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0903 11:05:21.840816 139742096562048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9724000024795533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b9IxLn_OSE5B",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhApSA71SE5H",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 4."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OVVIfSCISE5J",
        "outputId": "7d70491b-cf9a-491f-815f-1816edcf3c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Rounding operation\n",
        "rounding_weights1_op = tf.assign(w1, \n",
        "                          tf.round(w1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_thresh_op = tf.assign(thresh,\n",
        "                          tf.round(thresh/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "_ = sess.run([rounding_weights1_op, rounding_thresh_op])\n",
        "\n",
        "\n",
        "# Show final distribution of weights\n",
        "w1_values, thresh_values = sess.run([w1, thresh])\n",
        "\n",
        "kernel_values = (list(w1_values.flatten()) + list(thresh_values.flatten()))\n",
        "fig = plt.figure()\n",
        "plt.scatter(kernel_values, [1]*len(kernel_values))\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9724000024795533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRIA/x8yNcuIe0LGgwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1IDcpJB6SE5R"
      },
      "source": [
        "## 1.4 Retrain FC: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yHmVrXzuSE5S",
        "colab": {}
      },
      "source": [
        "LR = 0.001/4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sGHYUyi1SE5X",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('dense1', reuse=True) as scope_conv:\n",
        "  fc1_k = tf.get_variable('kernel')\n",
        "  fc1_b = tf.get_variable('bias')\n",
        "\n",
        "with tf.variable_scope('dense2', reuse=True) as scope_conv:\n",
        "  fc2_k = tf.get_variable('kernel')\n",
        "  fc2_b = tf.get_variable('bias')\n",
        "\n",
        "opt_fc = tf.train.AdamOptimizer(learning_rate=LR, name='Adam_reg')\n",
        "opt_fc_op = opt_fc.minimize(loss_op, var_list=[fc1_k, fc1_b,\n",
        "                                               fc2_k, fc2_b])\n",
        "\n",
        "sess.run(tf.variables_initializer(opt_fc.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mbrY1SOySE5d",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4SDogzb0SE5f",
        "outputId": "ce9b5f5e-ea2b-4321-86c2-64c45b6ecb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bin_rate_feed = MAX_BIN_RATE + 1\n",
        "\n",
        "for epoch in range(EPOCHS*2):\n",
        "\n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "\n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "\n",
        "    _, current_loss = sess.run([opt_fc_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "\n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 0.07271423984629412\n",
            "Testing Acc.: 0.9721000015735626\n",
            "Epoch 2 completed, average training loss is 0.07231792200123892\n",
            "Testing Acc.: 0.9722000020742416\n",
            "Epoch 3 completed, average training loss is 0.0725571571670783\n",
            "Testing Acc.: 0.9729000025987625\n",
            "Epoch 4 completed, average training loss is 0.07230805087834596\n",
            "Testing Acc.: 0.9734000015258789\n",
            "Epoch 5 completed, average training loss is 0.07237334372087692\n",
            "Testing Acc.: 0.9728000009059906\n",
            "Epoch 6 completed, average training loss is 0.07222244912525638\n",
            "Testing Acc.: 0.9726000016927719\n",
            "Epoch 7 completed, average training loss is 0.07303836764379715\n",
            "Testing Acc.: 0.9732000023126602\n",
            "Epoch 8 completed, average training loss is 0.07253301594406367\n",
            "Testing Acc.: 0.9710000020265579\n",
            "Epoch 9 completed, average training loss is 0.07235471939202398\n",
            "Testing Acc.: 0.9729000008106232\n",
            "Epoch 10 completed, average training loss is 0.07223454114049672\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 11 completed, average training loss is 0.07263130307197571\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 12 completed, average training loss is 0.07187394409440458\n",
            "Testing Acc.: 0.9736000037193299\n",
            "Epoch 13 completed, average training loss is 0.07252254541808119\n",
            "Testing Acc.: 0.9734000015258789\n",
            "Epoch 14 completed, average training loss is 0.07232838363658327\n",
            "Testing Acc.: 0.9729000002145767\n",
            "Epoch 15 completed, average training loss is 0.07232932028360665\n",
            "Testing Acc.: 0.9725000029802322\n",
            "Epoch 16 completed, average training loss is 0.07234169060985247\n",
            "Testing Acc.: 0.9727000033855439\n",
            "Epoch 17 completed, average training loss is 0.0721067974359418\n",
            "Testing Acc.: 0.9731000006198883\n",
            "Epoch 18 completed, average training loss is 0.07244136138974379\n",
            "Testing Acc.: 0.9729000014066697\n",
            "Epoch 19 completed, average training loss is 0.07194824054604396\n",
            "Testing Acc.: 0.9720000034570694\n",
            "Epoch 20 completed, average training loss is 0.07204111232655123\n",
            "Testing Acc.: 0.9725000035762786\n",
            "Epoch 21 completed, average training loss is 0.07146165966521949\n",
            "Testing Acc.: 0.9721000009775161\n",
            "Epoch 22 completed, average training loss is 0.07211035664814214\n",
            "Testing Acc.: 0.9738000011444092\n",
            "Epoch 23 completed, average training loss is 0.07149497586845731\n",
            "Testing Acc.: 0.9726000022888184\n",
            "Epoch 24 completed, average training loss is 0.07203062328199546\n",
            "Testing Acc.: 0.9716000020503998\n",
            "Epoch 25 completed, average training loss is 0.07259358904014031\n",
            "Testing Acc.: 0.9731000000238419\n",
            "Epoch 26 completed, average training loss is 0.0715548018924892\n",
            "Testing Acc.: 0.9734000033140182\n",
            "Epoch 27 completed, average training loss is 0.07150128157188496\n",
            "Testing Acc.: 0.9720000022649765\n",
            "Epoch 28 completed, average training loss is 0.07183879088920851\n",
            "Testing Acc.: 0.9729000014066697\n",
            "Epoch 29 completed, average training loss is 0.07234161374314378\n",
            "Testing Acc.: 0.9731000024080276\n",
            "Epoch 30 completed, average training loss is 0.0722482323522369\n",
            "Testing Acc.: 0.9737000000476838\n",
            "Epoch 31 completed, average training loss is 0.07144940872211009\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 32 completed, average training loss is 0.0718635861769629\n",
            "Testing Acc.: 0.9728000026941299\n",
            "Epoch 33 completed, average training loss is 0.0718014124998202\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 34 completed, average training loss is 0.07243505643447862\n",
            "Testing Acc.: 0.9741000002622604\n",
            "Epoch 35 completed, average training loss is 0.07120641552377492\n",
            "Testing Acc.: 0.9718000042438507\n",
            "Epoch 36 completed, average training loss is 0.0719449094372491\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 37 completed, average training loss is 0.07174188467053076\n",
            "Testing Acc.: 0.9731000036001205\n",
            "Epoch 38 completed, average training loss is 0.07243336452404037\n",
            "Testing Acc.: 0.9734000039100646\n",
            "Epoch 39 completed, average training loss is 0.071242873321753\n",
            "Testing Acc.: 0.9740000015497208\n",
            "Epoch 40 completed, average training loss is 0.07197108693964159\n",
            "Testing Acc.: 0.9721000045537949\n",
            "Epoch 41 completed, average training loss is 0.07166757753196483\n",
            "Testing Acc.: 0.9718000036478043\n",
            "Epoch 42 completed, average training loss is 0.07199243505252526\n",
            "Testing Acc.: 0.9732000005245208\n",
            "Epoch 43 completed, average training loss is 0.0721415182730804\n",
            "Testing Acc.: 0.9733000016212463\n",
            "Epoch 44 completed, average training loss is 0.07133082341557989\n",
            "Testing Acc.: 0.9724000018835067\n",
            "Epoch 45 completed, average training loss is 0.07156662695342675\n",
            "Testing Acc.: 0.972200003862381\n",
            "Epoch 46 completed, average training loss is 0.07094245303732653\n",
            "Testing Acc.: 0.9732000029087067\n",
            "Epoch 47 completed, average training loss is 0.07192023849114776\n",
            "Testing Acc.: 0.9730000019073486\n",
            "Epoch 48 completed, average training loss is 0.0715098929606999\n",
            "Testing Acc.: 0.973500002026558\n",
            "Epoch 49 completed, average training loss is 0.07134050676443925\n",
            "Testing Acc.: 0.9731000024080276\n",
            "Epoch 50 completed, average training loss is 0.07200139953754842\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 51 completed, average training loss is 0.07176644654556488\n",
            "Testing Acc.: 0.9731000018119812\n",
            "Epoch 52 completed, average training loss is 0.07139788126961018\n",
            "Testing Acc.: 0.9725\n",
            "Epoch 53 completed, average training loss is 0.07167486627586186\n",
            "Testing Acc.: 0.9733000034093857\n",
            "Epoch 54 completed, average training loss is 0.0713083343859762\n",
            "Testing Acc.: 0.9737000006437302\n",
            "Epoch 55 completed, average training loss is 0.07108780801917115\n",
            "Testing Acc.: 0.9732000017166138\n",
            "Epoch 56 completed, average training loss is 0.07133774431111913\n",
            "Testing Acc.: 0.9715000033378601\n",
            "Epoch 57 completed, average training loss is 0.07100385614205151\n",
            "Testing Acc.: 0.9727000021934509\n",
            "Epoch 58 completed, average training loss is 0.07174188873032109\n",
            "Testing Acc.: 0.9723000013828278\n",
            "Epoch 59 completed, average training loss is 0.07093887375590081\n",
            "Testing Acc.: 0.9740000021457672\n",
            "Epoch 60 completed, average training loss is 0.07172324780219545\n",
            "Testing Acc.: 0.9719000041484833\n",
            "Epoch 61 completed, average training loss is 0.07090421157966678\n",
            "Testing Acc.: 0.9716000014543533\n",
            "Epoch 62 completed, average training loss is 0.07175256047863514\n",
            "Testing Acc.: 0.973500000834465\n",
            "Epoch 63 completed, average training loss is 0.07152124591171742\n",
            "Testing Acc.: 0.9736000019311905\n",
            "Epoch 64 completed, average training loss is 0.07134230274319028\n",
            "Testing Acc.: 0.9742000025510787\n",
            "Epoch 65 completed, average training loss is 0.07132199667549381\n",
            "Testing Acc.: 0.9731000018119812\n",
            "Epoch 66 completed, average training loss is 0.07083659523709987\n",
            "Testing Acc.: 0.9735000026226044\n",
            "Epoch 67 completed, average training loss is 0.07125386825917909\n",
            "Testing Acc.: 0.9730000019073486\n",
            "Epoch 68 completed, average training loss is 0.07104940585714455\n",
            "Testing Acc.: 0.9738000011444092\n",
            "Epoch 69 completed, average training loss is 0.0714243762086456\n",
            "Testing Acc.: 0.9738000029325485\n",
            "Epoch 70 completed, average training loss is 0.07106491787359119\n",
            "Testing Acc.: 0.973800003528595\n",
            "Epoch 71 completed, average training loss is 0.07118311663235848\n",
            "Testing Acc.: 0.9733000034093857\n",
            "Epoch 72 completed, average training loss is 0.07101766146874676\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 73 completed, average training loss is 0.07073762336823468\n",
            "Testing Acc.: 0.9730000030994416\n",
            "Epoch 74 completed, average training loss is 0.07115443036581079\n",
            "Testing Acc.: 0.972700001001358\n",
            "Epoch 75 completed, average training loss is 0.07087720501935109\n",
            "Testing Acc.: 0.9738000023365021\n",
            "Epoch 76 completed, average training loss is 0.07113130879355595\n",
            "Testing Acc.: 0.9744000029563904\n",
            "Epoch 77 completed, average training loss is 0.07120609747245908\n",
            "Testing Acc.: 0.9718000042438507\n",
            "Epoch 78 completed, average training loss is 0.07118125131295529\n",
            "Testing Acc.: 0.9728000026941299\n",
            "Epoch 79 completed, average training loss is 0.07089245136904841\n",
            "Testing Acc.: 0.9728000032901764\n",
            "Epoch 80 completed, average training loss is 0.07093522099700446\n",
            "Testing Acc.: 0.9745000040531159\n",
            "Epoch 81 completed, average training loss is 0.07101421327795833\n",
            "Testing Acc.: 0.9740000027418136\n",
            "Epoch 82 completed, average training loss is 0.07075008320699756\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 83 completed, average training loss is 0.07081568461532393\n",
            "Testing Acc.: 0.9733000028133393\n",
            "Epoch 84 completed, average training loss is 0.07062417454939957\n",
            "Testing Acc.: 0.9723000025749207\n",
            "Epoch 85 completed, average training loss is 0.07145462691939125\n",
            "Testing Acc.: 0.9734000021219253\n",
            "Epoch 86 completed, average training loss is 0.07102838612549628\n",
            "Testing Acc.: 0.9736000025272369\n",
            "Epoch 87 completed, average training loss is 0.07125661311515917\n",
            "Testing Acc.: 0.9734000009298325\n",
            "Epoch 88 completed, average training loss is 0.0706996043647329\n",
            "Testing Acc.: 0.9730000025033951\n",
            "Epoch 89 completed, average training loss is 0.07058237803634256\n",
            "Testing Acc.: 0.9735000026226044\n",
            "Epoch 90 completed, average training loss is 0.0707669668768843\n",
            "Testing Acc.: 0.9727000027894974\n",
            "Epoch 91 completed, average training loss is 0.07033481903218974\n",
            "Testing Acc.: 0.9727000039815903\n",
            "Epoch 92 completed, average training loss is 0.07114842746484404\n",
            "Testing Acc.: 0.9721000027656556\n",
            "Epoch 93 completed, average training loss is 0.07074119818086426\n",
            "Testing Acc.: 0.9735000026226044\n",
            "Epoch 94 completed, average training loss is 0.07067710967967286\n",
            "Testing Acc.: 0.9735000002384185\n",
            "Epoch 95 completed, average training loss is 0.07147964442459245\n",
            "Testing Acc.: 0.973000003695488\n",
            "Epoch 96 completed, average training loss is 0.07047372741935154\n",
            "Testing Acc.: 0.9726000034809112\n",
            "Epoch 97 completed, average training loss is 0.07010388071027895\n",
            "Testing Acc.: 0.9745000034570694\n",
            "Epoch 98 completed, average training loss is 0.07112539200888325\n",
            "Testing Acc.: 0.9705000030994415\n",
            "Epoch 99 completed, average training loss is 0.07116971425246447\n",
            "Testing Acc.: 0.9731000012159348\n",
            "Epoch 100 completed, average training loss is 0.07029436646339794\n",
            "Testing Acc.: 0.9734000039100646\n",
            "Testing Acc.: 0.9734000039100646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQExdNNnSE5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b9ae634c-1436-4485-a6de-b6d331a81a28"
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE)\n",
        "test_accuracy_bin_rate(5000)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9734000039100646\n",
            "Testing Acc.: 0.9734000039100646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl6HHcQuz9bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get final distribution of weights\n",
        "w1_values, thresh_values = sess.run([w1, thresh])\n",
        "\n",
        "with open('NP_KERNELS_WEIGHTS.pck', 'wb') as f:\n",
        "  pickle.dump((w1_values, thresh_values), f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantised Net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Htp8g-JA-cDj",
        "8L65mkhvSE22",
        "8LSJT-ZQSE3K",
        "uQ2PuUmGSE4b",
        "1B3LcU1fSE45",
        "1IDcpJB6SE5R",
        "cP44vuN7VVp7",
        "a54c6vgKVVqA",
        "cfR_HMiAVVrK",
        "xX_5Sr18VVrr",
        "piuWiN0hVVsE",
        "O9lj-aI4UTeV",
        "j_LMAykNUTeZ",
        "YYpNBHwzUTfV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Eii-wqfL9p",
        "colab_type": "text"
      },
      "source": [
        "This notebook gathers experiments on 2 layer networks architecture: two full layers of 3 3x3 kernels, with some quantization of the intermediate results. \n",
        "1.   Without quantisation, testing accuracy of 97.6%.\n",
        "2.   With 4 bits quantisation, testing accuracy of 97.0%\n",
        "3.   With 3 bits quantisation, testing accuracy of 94.9% when fully trained on its own, 96.9% when re-using the 4 bits convolution kernels and retraining the fully connected layers only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Nw_wwuSj2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Htp8g-JA-cDj"
      },
      "source": [
        "#0. Import Data / Utils functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gOnOnLg_-cDd",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KXvDhE_-cDR",
        "outputId": "16cd0371-4c0d-4c13-ca8e-8468798b9280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Simulate an input binarization\n",
        "x_train = np.minimum(x_train, 100) // 100 * 120\n",
        "x_test = np.minimum(x_test, 100) // 100 * 120"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8L65mkhvSE22"
      },
      "source": [
        "# 1. No quantisation: 97.6%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8LSJT-ZQSE3K"
      },
      "source": [
        "## 1.1 Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OdxuyRYGSE3c",
        "colab": {}
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ym31WeJlSE3k",
        "colab": {}
      },
      "source": [
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "83puxCWmSE3q",
        "colab": {}
      },
      "source": [
        "def network1(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  conv2 = slim.conv2d(conv, 3, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2')\n",
        "  \n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_DBWynlSE3v",
        "colab": {}
      },
      "source": [
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network1(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CAW7lRxjSE31",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFbHGaWKSE35",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2', reuse=True) as scope_conv:\n",
        "  w2 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OShYxcw1SE3_",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 25."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbHBQHQYSE4I",
        "colab": {}
      },
      "source": [
        "def customRegularizerConv(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_CONV*np.pi*(x-ROUNDING_STEP_CONV/2))+1.\n",
        "\n",
        "def customRegularizerBias(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_BIAS*np.pi*(x-ROUNDING_STEP_BIAS/2))+1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_967wsFTSE4V",
        "colab": {}
      },
      "source": [
        "NUM_PARAM_REG = 30. + 27. + 27. + 27. + 3.\n",
        "reg_losses = 27./NUM_PARAM_REG *tf.reduce_mean(customRegularizerConv(w1))\n",
        "reg_losses += 3./NUM_PARAM_REG *tf.reduce_mean(customRegularizerBias(b1))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2))\n",
        "reg_losses += 3./NUM_PARAM_REG*tf.reduce_mean(customRegularizerBias(thresh))\n",
        "\n",
        "reg_factor_ph = tf.placeholder(tf.float32)\n",
        "loss_with_reg_op = loss_op + reg_factor_ph * reg_losses\n",
        "\n",
        "lr_ph = tf.placeholder(tf.float32)\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr_ph, name='Adam_reg')\n",
        "opt_op = opt.minimize(loss_with_reg_op)\n",
        "sess.run(tf.variables_initializer(opt.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQ2PuUmGSE4b"
      },
      "source": [
        "## 1.2 Training: 97.59%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mXGVPG6kSE4c"
      },
      "source": [
        "Evolving params:\n",
        "- lr_ph : learning rate\n",
        "- reg_factor_ph : regularization factor -> this time, set directly to final value in the second learning stage\n",
        "- bin_rate_ph : binarization rate -> this time, set directly to final value in the third learning stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0DClys1uSE4e",
        "colab": {}
      },
      "source": [
        "INPUT_SCALING = 0.7 # So that intermediate values stay in the -127, +127 range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lfZn2VmNSE4h",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pddgA9M2SE4l",
        "colab": {}
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "# Bin then reg\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_sBtsD9SE4p",
        "outputId": "9e9d1400-9426-43b4-87e9-37cf457298df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "    \n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "    \n",
        "    _, current_loss = sess.run([opt_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  lr_ph: lr_feed,\n",
        "                                  reg_factor_ph: reg_factor_feed,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "  \n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 1.7334648538629214\n",
            "Testing Acc.: 0.7845999985933304\n",
            "Epoch 2 completed, average training loss is 0.5435956929872433\n",
            "Testing Acc.: 0.8663000005483628\n",
            "Epoch 3 completed, average training loss is 0.3844096157451471\n",
            "Testing Acc.: 0.902500001192093\n",
            "Epoch 4 completed, average training loss is 0.3174646234760682\n",
            "Testing Acc.: 0.9223000049591065\n",
            "Epoch 5 completed, average training loss is 0.27289525032043455\n",
            "Testing Acc.: 0.9250000017881393\n",
            "Epoch 6 completed, average training loss is 0.24903189885119598\n",
            "Testing Acc.: 0.9339000028371811\n",
            "Epoch 7 completed, average training loss is 0.22191264590869347\n",
            "Testing Acc.: 0.936400004029274\n",
            "Epoch 8 completed, average training loss is 0.2013370345843335\n",
            "Testing Acc.: 0.9428000026941299\n",
            "Epoch 9 completed, average training loss is 0.1876854702581962\n",
            "Testing Acc.: 0.9503000062704087\n",
            "Epoch 10 completed, average training loss is 0.17077568740583957\n",
            "Testing Acc.: 0.9519000071287155\n",
            "Epoch 11 completed, average training loss is 0.1692703819088638\n",
            "Testing Acc.: 0.9461000043153763\n",
            "Epoch 12 completed, average training loss is 0.15615388052538037\n",
            "Testing Acc.: 0.9578000050783158\n",
            "Epoch 13 completed, average training loss is 0.15092024052515626\n",
            "Testing Acc.: 0.9578000056743622\n",
            "Epoch 14 completed, average training loss is 0.14529230083649358\n",
            "Testing Acc.: 0.9530000066757203\n",
            "Epoch 15 completed, average training loss is 0.1435137616035839\n",
            "Testing Acc.: 0.955000005364418\n",
            "Epoch 16 completed, average training loss is 0.13566902226613214\n",
            "Testing Acc.: 0.9636000031232834\n",
            "Epoch 17 completed, average training loss is 0.1349442545355608\n",
            "Testing Acc.: 0.9573000049591065\n",
            "Epoch 18 completed, average training loss is 0.13160101025986173\n",
            "Testing Acc.: 0.9625000029802322\n",
            "Epoch 19 completed, average training loss is 0.1279940733453259\n",
            "Testing Acc.: 0.9644000065326691\n",
            "Epoch 20 completed, average training loss is 0.12503057225917777\n",
            "Testing Acc.: 0.9642000079154969\n",
            "Epoch 21 completed, average training loss is 0.12534323863995572\n",
            "Testing Acc.: 0.9648000055551529\n",
            "Epoch 22 completed, average training loss is 0.1207131769383947\n",
            "Testing Acc.: 0.9697000074386597\n",
            "Epoch 23 completed, average training loss is 0.11838210771170755\n",
            "Testing Acc.: 0.962400004863739\n",
            "Epoch 24 completed, average training loss is 0.11760611640444646\n",
            "Testing Acc.: 0.9632000052928924\n",
            "Epoch 25 completed, average training loss is 0.1172673034761101\n",
            "Testing Acc.: 0.9660000067949295\n",
            "Epoch 26 completed, average training loss is 0.11343104704283177\n",
            "Testing Acc.: 0.9671000069379807\n",
            "Epoch 27 completed, average training loss is 0.11108570974785835\n",
            "Testing Acc.: 0.9602000087499618\n",
            "Epoch 28 completed, average training loss is 0.11170811037377765\n",
            "Testing Acc.: 0.9696000051498413\n",
            "Epoch 29 completed, average training loss is 0.11066575766230623\n",
            "Testing Acc.: 0.9694000041484833\n",
            "Epoch 30 completed, average training loss is 0.10532542683028927\n",
            "Testing Acc.: 0.9623000031709671\n",
            "Epoch 31 completed, average training loss is 0.10927662280388177\n",
            "Testing Acc.: 0.9658000046014785\n",
            "Epoch 32 completed, average training loss is 0.10807387614777933\n",
            "Testing Acc.: 0.9641000044345855\n",
            "Epoch 33 completed, average training loss is 0.10577482753743728\n",
            "Testing Acc.: 0.9668000042438507\n",
            "Epoch 34 completed, average training loss is 0.10381445100841423\n",
            "Testing Acc.: 0.9731000053882599\n",
            "Epoch 35 completed, average training loss is 0.1039971166367953\n",
            "Testing Acc.: 0.9671000081300736\n",
            "Epoch 36 completed, average training loss is 0.10383234903216362\n",
            "Testing Acc.: 0.9686000061035156\n",
            "Epoch 37 completed, average training loss is 0.1016267801898842\n",
            "Testing Acc.: 0.9688000047206878\n",
            "Epoch 38 completed, average training loss is 0.10486432904067139\n",
            "Testing Acc.: 0.9685000050067901\n",
            "Epoch 39 completed, average training loss is 0.10341277551216384\n",
            "Testing Acc.: 0.9698000067472458\n",
            "Epoch 40 completed, average training loss is 0.10163043526622156\n",
            "Testing Acc.: 0.9728000062704086\n",
            "Epoch 41 completed, average training loss is 0.10112732001580298\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 42 completed, average training loss is 0.09719486193110545\n",
            "Testing Acc.: 0.9744000059366226\n",
            "Epoch 43 completed, average training loss is 0.09767162247250477\n",
            "Testing Acc.: 0.9739000064134598\n",
            "Epoch 44 completed, average training loss is 0.09891317304068556\n",
            "Testing Acc.: 0.9707000029087066\n",
            "Epoch 45 completed, average training loss is 0.09591415719750027\n",
            "Testing Acc.: 0.9731000071763992\n",
            "Epoch 46 completed, average training loss is 0.09700321170967072\n",
            "Testing Acc.: 0.9722000050544739\n",
            "Epoch 47 completed, average training loss is 0.09604740965800981\n",
            "Testing Acc.: 0.9695000046491623\n",
            "Epoch 48 completed, average training loss is 0.09351294512860477\n",
            "Testing Acc.: 0.9719000047445298\n",
            "Epoch 49 completed, average training loss is 0.09571094759584715\n",
            "Testing Acc.: 0.9724000066518783\n",
            "Epoch 50 completed, average training loss is 0.09278700139140711\n",
            "Testing Acc.: 0.9643000048398972\n",
            "Epoch 51 completed, average training loss is 0.09544187789317221\n",
            "Testing Acc.: 0.9727000033855439\n",
            "Epoch 52 completed, average training loss is 0.09196166343366106\n",
            "Testing Acc.: 0.9719000071287155\n",
            "Epoch 53 completed, average training loss is 0.09171919078721355\n",
            "Testing Acc.: 0.9737000042200088\n",
            "Epoch 54 completed, average training loss is 0.09164859383056562\n",
            "Testing Acc.: 0.9711000043153762\n",
            "Epoch 55 completed, average training loss is 0.0898966985894367\n",
            "Testing Acc.: 0.9744000065326691\n",
            "Epoch 56 completed, average training loss is 0.0919508485061427\n",
            "Testing Acc.: 0.9696000051498413\n",
            "Epoch 57 completed, average training loss is 0.08965470177354291\n",
            "Testing Acc.: 0.9726000034809112\n",
            "Epoch 58 completed, average training loss is 0.08808489405550063\n",
            "Testing Acc.: 0.9717000043392181\n",
            "Epoch 59 completed, average training loss is 0.08793664892592157\n",
            "Testing Acc.: 0.9734000033140182\n",
            "Epoch 60 completed, average training loss is 0.08604867327803124\n",
            "Testing Acc.: 0.9765000069141387\n",
            "Epoch 61 completed, average training loss is 0.08805082697576533\n",
            "Testing Acc.: 0.9727000039815903\n",
            "Epoch 62 completed, average training loss is 0.08659353343614687\n",
            "Testing Acc.: 0.972900003194809\n",
            "Epoch 63 completed, average training loss is 0.08624117359519005\n",
            "Testing Acc.: 0.9701000040769577\n",
            "Epoch 64 completed, average training loss is 0.08778582043014467\n",
            "Testing Acc.: 0.9752000045776367\n",
            "Epoch 65 completed, average training loss is 0.08478936643960576\n",
            "Testing Acc.: 0.9738000059127807\n",
            "Epoch 66 completed, average training loss is 0.08596462084756543\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 67 completed, average training loss is 0.08319776442600414\n",
            "Testing Acc.: 0.9753000074625016\n",
            "Epoch 68 completed, average training loss is 0.0823436912878727\n",
            "Testing Acc.: 0.974700003862381\n",
            "Epoch 69 completed, average training loss is 0.08384656519784282\n",
            "Testing Acc.: 0.9754000061750412\n",
            "Epoch 70 completed, average training loss is 0.0831859788725463\n",
            "Testing Acc.: 0.9777000069618225\n",
            "Epoch 71 completed, average training loss is 0.08127293983319153\n",
            "Testing Acc.: 0.9728000080585479\n",
            "Epoch 72 completed, average training loss is 0.08202866040558243\n",
            "Testing Acc.: 0.9783000046014786\n",
            "Epoch 73 completed, average training loss is 0.08116515981111054\n",
            "Testing Acc.: 0.9752000033855438\n",
            "Epoch 74 completed, average training loss is 0.08099140998131285\n",
            "Testing Acc.: 0.9787000054121018\n",
            "Epoch 75 completed, average training loss is 0.08082142551429569\n",
            "Testing Acc.: 0.9709000062942504\n",
            "Epoch 76 completed, average training loss is 0.07853190886167188\n",
            "Testing Acc.: 0.975000006556511\n",
            "Epoch 77 completed, average training loss is 0.07758476203773171\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 78 completed, average training loss is 0.07900742962335547\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 79 completed, average training loss is 0.077694659717381\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 80 completed, average training loss is 0.07932805719552562\n",
            "Testing Acc.: 0.9711000049114227\n",
            "Epoch 81 completed, average training loss is 0.07728832165089747\n",
            "Testing Acc.: 0.9769000065326691\n",
            "Epoch 82 completed, average training loss is 0.07690961364268635\n",
            "Testing Acc.: 0.975100005865097\n",
            "Epoch 83 completed, average training loss is 0.07699732610179733\n",
            "Testing Acc.: 0.9724000042676926\n",
            "Epoch 84 completed, average training loss is 0.07721976808582744\n",
            "Testing Acc.: 0.9780000072717666\n",
            "Epoch 85 completed, average training loss is 0.07733282749541104\n",
            "Testing Acc.: 0.9777000057697296\n",
            "Epoch 86 completed, average training loss is 0.07474782229401171\n",
            "Testing Acc.: 0.9769000041484833\n",
            "Epoch 87 completed, average training loss is 0.07445378606207669\n",
            "Testing Acc.: 0.9729000037908554\n",
            "Epoch 88 completed, average training loss is 0.07320050242628592\n",
            "Testing Acc.: 0.9756000083684921\n",
            "Epoch 89 completed, average training loss is 0.0748936244364207\n",
            "Testing Acc.: 0.9795000088214875\n",
            "Epoch 90 completed, average training loss is 0.07358580555766821\n",
            "Testing Acc.: 0.9743000066280365\n",
            "Epoch 91 completed, average training loss is 0.07549351191769044\n",
            "Testing Acc.: 0.9765000081062317\n",
            "Epoch 92 completed, average training loss is 0.07281352919759229\n",
            "Testing Acc.: 0.9774000078439713\n",
            "Epoch 93 completed, average training loss is 0.07265295158528412\n",
            "Testing Acc.: 0.9777000051736832\n",
            "Epoch 94 completed, average training loss is 0.0738004506053403\n",
            "Testing Acc.: 0.9746000075340271\n",
            "Epoch 95 completed, average training loss is 0.0720945460042761\n",
            "Testing Acc.: 0.9768000084161759\n",
            "Epoch 96 completed, average training loss is 0.074502477493758\n",
            "Testing Acc.: 0.9717000055313111\n",
            "Epoch 97 completed, average training loss is 0.07265666871719684\n",
            "Testing Acc.: 0.975500009059906\n",
            "Epoch 98 completed, average training loss is 0.07128067812959973\n",
            "Testing Acc.: 0.9781000089645385\n",
            "Epoch 99 completed, average training loss is 0.07165250551654026\n",
            "Testing Acc.: 0.9802000081539154\n",
            "Epoch 100 completed, average training loss is 0.07194930387816081\n",
            "Testing Acc.: 0.9803000110387802\n",
            "Epoch 101 completed, average training loss is 0.07097156254885097\n",
            "Testing Acc.: 0.978600007891655\n",
            "Epoch 102 completed, average training loss is 0.07117178976923848\n",
            "Testing Acc.: 0.9781000095605851\n",
            "Epoch 103 completed, average training loss is 0.07253430464382594\n",
            "Testing Acc.: 0.978700008392334\n",
            "Epoch 104 completed, average training loss is 0.07190892867278308\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 105 completed, average training loss is 0.07018921595222007\n",
            "Testing Acc.: 0.9807000082731246\n",
            "Epoch 106 completed, average training loss is 0.07189052511627475\n",
            "Testing Acc.: 0.9761000055074692\n",
            "Epoch 107 completed, average training loss is 0.07043646811507642\n",
            "Testing Acc.: 0.9792000079154968\n",
            "Epoch 108 completed, average training loss is 0.07097610802389681\n",
            "Testing Acc.: 0.9756000071763993\n",
            "Epoch 109 completed, average training loss is 0.07090225845652944\n",
            "Testing Acc.: 0.9785000067949295\n",
            "Epoch 110 completed, average training loss is 0.06970277797120313\n",
            "Testing Acc.: 0.9664000034332275\n",
            "Epoch 111 completed, average training loss is 0.06874017343701173\n",
            "Testing Acc.: 0.975200007557869\n",
            "Epoch 112 completed, average training loss is 0.06840731265799453\n",
            "Testing Acc.: 0.9777000069618225\n",
            "Epoch 113 completed, average training loss is 0.06927107050083578\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 114 completed, average training loss is 0.06845956869229364\n",
            "Testing Acc.: 0.9788000077009201\n",
            "Epoch 115 completed, average training loss is 0.07026791320492824\n",
            "Testing Acc.: 0.9772000104188919\n",
            "Epoch 116 completed, average training loss is 0.06961984569284443\n",
            "Testing Acc.: 0.9749000078439712\n",
            "Epoch 117 completed, average training loss is 0.06888382751572256\n",
            "Testing Acc.: 0.9793000060319901\n",
            "Epoch 118 completed, average training loss is 0.06836887888998414\n",
            "Testing Acc.: 0.9793000096082687\n",
            "Epoch 119 completed, average training loss is 0.06882027144970683\n",
            "Testing Acc.: 0.9772000068426132\n",
            "Epoch 120 completed, average training loss is 0.06899142362177373\n",
            "Testing Acc.: 0.978200010061264\n",
            "Epoch 121 completed, average training loss is 0.06622550065629185\n",
            "Testing Acc.: 0.9774000078439713\n",
            "Epoch 122 completed, average training loss is 0.06800382696169739\n",
            "Testing Acc.: 0.9796000075340271\n",
            "Epoch 123 completed, average training loss is 0.06600554476492107\n",
            "Testing Acc.: 0.980900011062622\n",
            "Epoch 124 completed, average training loss is 0.06797552834982828\n",
            "Testing Acc.: 0.9790000087022781\n",
            "Epoch 125 completed, average training loss is 0.06708658938296139\n",
            "Testing Acc.: 0.9751000076532363\n",
            "Epoch 126 completed, average training loss is 0.0665660121718732\n",
            "Testing Acc.: 0.9774000072479248\n",
            "Epoch 127 completed, average training loss is 0.06752459685861444\n",
            "Testing Acc.: 0.9787000066041946\n",
            "Epoch 128 completed, average training loss is 0.0687175681007405\n",
            "Testing Acc.: 0.975800006389618\n",
            "Epoch 129 completed, average training loss is 0.06666094824050864\n",
            "Testing Acc.: 0.9792000073194503\n",
            "Epoch 130 completed, average training loss is 0.06742040210558722\n",
            "Testing Acc.: 0.979300007224083\n",
            "Epoch 131 completed, average training loss is 0.06497761320943633\n",
            "Testing Acc.: 0.9792000067234039\n",
            "Epoch 132 completed, average training loss is 0.06597839846586187\n",
            "Testing Acc.: 0.9806000107526779\n",
            "Epoch 133 completed, average training loss is 0.06555467391813484\n",
            "Testing Acc.: 0.979300007224083\n",
            "Epoch 134 completed, average training loss is 0.06580115174719443\n",
            "Testing Acc.: 0.9790000075101852\n",
            "Epoch 135 completed, average training loss is 0.06545317715305525\n",
            "Testing Acc.: 0.9760000067949295\n",
            "Epoch 136 completed, average training loss is 0.06624456276185811\n",
            "Testing Acc.: 0.9803000086545944\n",
            "Epoch 137 completed, average training loss is 0.06837871454345683\n",
            "Testing Acc.: 0.979400007724762\n",
            "Epoch 138 completed, average training loss is 0.06639081198954955\n",
            "Testing Acc.: 0.9794000083208084\n",
            "Epoch 139 completed, average training loss is 0.0653851394308731\n",
            "Testing Acc.: 0.9800000089406967\n",
            "Epoch 140 completed, average training loss is 0.06495616860610122\n",
            "Testing Acc.: 0.9777000081539154\n",
            "Epoch 141 completed, average training loss is 0.06426440541710084\n",
            "Testing Acc.: 0.9796000093221664\n",
            "Epoch 142 completed, average training loss is 0.06275230708842476\n",
            "Testing Acc.: 0.9824000090360642\n",
            "Epoch 143 completed, average training loss is 0.06570980563604584\n",
            "Testing Acc.: 0.979500008225441\n",
            "Epoch 144 completed, average training loss is 0.06436944477415334\n",
            "Testing Acc.: 0.9768000084161759\n",
            "Epoch 145 completed, average training loss is 0.06496088394817585\n",
            "Testing Acc.: 0.9784000045061112\n",
            "Epoch 146 completed, average training loss is 0.06469017177587374\n",
            "Testing Acc.: 0.9764000111818314\n",
            "Epoch 147 completed, average training loss is 0.06417583245706435\n",
            "Testing Acc.: 0.9796000099182129\n",
            "Epoch 148 completed, average training loss is 0.06340835471618145\n",
            "Testing Acc.: 0.9773000103235244\n",
            "Epoch 149 completed, average training loss is 0.06631865183978031\n",
            "Testing Acc.: 0.9768000054359436\n",
            "Epoch 150 completed, average training loss is 0.06349400829873048\n",
            "Testing Acc.: 0.9786000072956085\n",
            "Epoch 151 completed, average training loss is 0.06338533831139405\n",
            "Testing Acc.: 0.9753000086545944\n",
            "Epoch 152 completed, average training loss is 0.06351901738011899\n",
            "Testing Acc.: 0.9788000077009201\n",
            "Epoch 153 completed, average training loss is 0.06405293729505501\n",
            "Testing Acc.: 0.9815000081062317\n",
            "Epoch 154 completed, average training loss is 0.06348516593376795\n",
            "Testing Acc.: 0.9775000077486038\n",
            "Epoch 155 completed, average training loss is 0.06257705441404444\n",
            "Testing Acc.: 0.9775000077486038\n",
            "Epoch 156 completed, average training loss is 0.06296824789062763\n",
            "Testing Acc.: 0.9816000097990036\n",
            "Epoch 157 completed, average training loss is 0.06320531076169572\n",
            "Testing Acc.: 0.9799000102281571\n",
            "Epoch 158 completed, average training loss is 0.0639734106611771\n",
            "Testing Acc.: 0.9811000084877014\n",
            "Epoch 159 completed, average training loss is 0.06146992535563186\n",
            "Testing Acc.: 0.9763000059127808\n",
            "Epoch 160 completed, average training loss is 0.06271604124185008\n",
            "Testing Acc.: 0.9786000061035156\n",
            "Epoch 161 completed, average training loss is 0.06210650354934236\n",
            "Testing Acc.: 0.9796000069379807\n",
            "Epoch 162 completed, average training loss is 0.06176755687454715\n",
            "Testing Acc.: 0.9810000079870224\n",
            "Epoch 163 completed, average training loss is 0.06203807795847145\n",
            "Testing Acc.: 0.9819000071287155\n",
            "Epoch 164 completed, average training loss is 0.06339637702136922\n",
            "Testing Acc.: 0.9775000065565109\n",
            "Epoch 165 completed, average training loss is 0.061774724601612735\n",
            "Testing Acc.: 0.9800000083446503\n",
            "Epoch 166 completed, average training loss is 0.06223443015672577\n",
            "Testing Acc.: 0.9796000099182129\n",
            "Epoch 167 completed, average training loss is 0.06259814143956949\n",
            "Testing Acc.: 0.977700007557869\n",
            "Epoch 168 completed, average training loss is 0.06277576282154769\n",
            "Testing Acc.: 0.9792000073194503\n",
            "Epoch 169 completed, average training loss is 0.062384829389241836\n",
            "Testing Acc.: 0.9735000056028366\n",
            "Epoch 170 completed, average training loss is 0.061290840731235224\n",
            "Testing Acc.: 0.9801000076532363\n",
            "Epoch 171 completed, average training loss is 0.06278738485571618\n",
            "Testing Acc.: 0.9797000080347061\n",
            "Epoch 172 completed, average training loss is 0.061940415906331814\n",
            "Testing Acc.: 0.9791000068187714\n",
            "Epoch 173 completed, average training loss is 0.061552400331711395\n",
            "Testing Acc.: 0.9801000082492828\n",
            "Epoch 174 completed, average training loss is 0.060536063926604884\n",
            "Testing Acc.: 0.9792000073194503\n",
            "Epoch 175 completed, average training loss is 0.06154030714378071\n",
            "Testing Acc.: 0.9754000067710876\n",
            "Epoch 176 completed, average training loss is 0.06071648359570342\n",
            "Testing Acc.: 0.978600007891655\n",
            "Epoch 177 completed, average training loss is 0.060259830141440034\n",
            "Testing Acc.: 0.9768000078201294\n",
            "Epoch 178 completed, average training loss is 0.06230201390183841\n",
            "Testing Acc.: 0.9774000066518783\n",
            "Epoch 179 completed, average training loss is 0.05990820333361626\n",
            "Testing Acc.: 0.9787000077962875\n",
            "Epoch 180 completed, average training loss is 0.061161404030475146\n",
            "Testing Acc.: 0.9813000082969665\n",
            "Epoch 181 completed, average training loss is 0.060477866164486235\n",
            "Testing Acc.: 0.980200007557869\n",
            "Epoch 182 completed, average training loss is 0.06058937172948693\n",
            "Testing Acc.: 0.9782000076770783\n",
            "Epoch 183 completed, average training loss is 0.059455201245533926\n",
            "Testing Acc.: 0.9815000087022782\n",
            "Epoch 184 completed, average training loss is 0.05989453437039629\n",
            "Testing Acc.: 0.9796000093221664\n",
            "Epoch 185 completed, average training loss is 0.060859400482537844\n",
            "Testing Acc.: 0.9810000079870224\n",
            "Epoch 186 completed, average training loss is 0.059714435495746634\n",
            "Testing Acc.: 0.9802000081539154\n",
            "Epoch 187 completed, average training loss is 0.060685346785467116\n",
            "Testing Acc.: 0.980500010251999\n",
            "Epoch 188 completed, average training loss is 0.059761183719383555\n",
            "Testing Acc.: 0.9766000068187713\n",
            "Epoch 189 completed, average training loss is 0.059686144690494984\n",
            "Testing Acc.: 0.9764000099897384\n",
            "Epoch 190 completed, average training loss is 0.059989658327928436\n",
            "Testing Acc.: 0.9790000087022781\n",
            "Epoch 191 completed, average training loss is 0.05820178169097441\n",
            "Testing Acc.: 0.9795000064373016\n",
            "Epoch 192 completed, average training loss is 0.05884979417043117\n",
            "Testing Acc.: 0.9787000072002411\n",
            "Epoch 193 completed, average training loss is 0.05983427196469469\n",
            "Testing Acc.: 0.9802000069618225\n",
            "Epoch 194 completed, average training loss is 0.05939309391037872\n",
            "Testing Acc.: 0.9808000069856644\n",
            "Epoch 195 completed, average training loss is 0.0592106964928098\n",
            "Testing Acc.: 0.9783000069856643\n",
            "Epoch 196 completed, average training loss is 0.06045176932549415\n",
            "Testing Acc.: 0.9797000074386597\n",
            "Epoch 197 completed, average training loss is 0.05980984993240175\n",
            "Testing Acc.: 0.9791000080108643\n",
            "Epoch 198 completed, average training loss is 0.05883244413572053\n",
            "Testing Acc.: 0.9801000082492828\n",
            "Epoch 199 completed, average training loss is 0.05877782175162186\n",
            "Testing Acc.: 0.9794000065326691\n",
            "Epoch 200 completed, average training loss is 0.05885994964861311\n",
            "Testing Acc.: 0.9813000065088272\n",
            "Epoch 201 completed, average training loss is 0.1151142300114346\n",
            "Testing Acc.: 0.9695000046491623\n",
            "Epoch 202 completed, average training loss is 0.10291567541503657\n",
            "Testing Acc.: 0.9674000042676926\n",
            "Epoch 203 completed, average training loss is 0.09694436923600733\n",
            "Testing Acc.: 0.9713000053167343\n",
            "Epoch 204 completed, average training loss is 0.09315162552675853\n",
            "Testing Acc.: 0.9714000064134598\n",
            "Epoch 205 completed, average training loss is 0.09167163693346084\n",
            "Testing Acc.: 0.970100005865097\n",
            "Epoch 206 completed, average training loss is 0.08807499022688717\n",
            "Testing Acc.: 0.9732000082731247\n",
            "Epoch 207 completed, average training loss is 0.08769371047926446\n",
            "Testing Acc.: 0.9725000071525574\n",
            "Epoch 208 completed, average training loss is 0.08665849676045279\n",
            "Testing Acc.: 0.9718000048398971\n",
            "Epoch 209 completed, average training loss is 0.08576153957828259\n",
            "Testing Acc.: 0.9727000039815903\n",
            "Epoch 210 completed, average training loss is 0.08448498139468331\n",
            "Testing Acc.: 0.9726000046730041\n",
            "Epoch 211 completed, average training loss is 0.08382516315439716\n",
            "Testing Acc.: 0.9705000048875809\n",
            "Epoch 212 completed, average training loss is 0.08290997545855741\n",
            "Testing Acc.: 0.9739000087976456\n",
            "Epoch 213 completed, average training loss is 0.08275804014721265\n",
            "Testing Acc.: 0.9723000067472458\n",
            "Epoch 214 completed, average training loss is 0.08181985554440568\n",
            "Testing Acc.: 0.9747000068426133\n",
            "Epoch 215 completed, average training loss is 0.08152385495603084\n",
            "Testing Acc.: 0.9757000076770782\n",
            "Epoch 216 completed, average training loss is 0.08106437040880943\n",
            "Testing Acc.: 0.9740000069141388\n",
            "Epoch 217 completed, average training loss is 0.080343692896422\n",
            "Testing Acc.: 0.9742000073194503\n",
            "Epoch 218 completed, average training loss is 0.07895931352706005\n",
            "Testing Acc.: 0.9739000064134598\n",
            "Epoch 219 completed, average training loss is 0.07955624062956\n",
            "Testing Acc.: 0.9738000065088273\n",
            "Epoch 220 completed, average training loss is 0.0797321714561743\n",
            "Testing Acc.: 0.9746000057458878\n",
            "Epoch 221 completed, average training loss is 0.07893260948825627\n",
            "Testing Acc.: 0.9747000056505203\n",
            "Epoch 222 completed, average training loss is 0.07824314848675082\n",
            "Testing Acc.: 0.975100005865097\n",
            "Epoch 223 completed, average training loss is 0.07808790253552919\n",
            "Testing Acc.: 0.9740000069141388\n",
            "Epoch 224 completed, average training loss is 0.07868346572543183\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 225 completed, average training loss is 0.07747402631910517\n",
            "Testing Acc.: 0.9738000053167343\n",
            "Epoch 226 completed, average training loss is 0.07695014543210467\n",
            "Testing Acc.: 0.9741000068187714\n",
            "Epoch 227 completed, average training loss is 0.07717453192376221\n",
            "Testing Acc.: 0.9769000083208084\n",
            "Epoch 228 completed, average training loss is 0.07714099432341755\n",
            "Testing Acc.: 0.9721000069379806\n",
            "Epoch 229 completed, average training loss is 0.07818516103938843\n",
            "Testing Acc.: 0.9748000061511993\n",
            "Epoch 230 completed, average training loss is 0.07679146801587194\n",
            "Testing Acc.: 0.972400004863739\n",
            "Epoch 231 completed, average training loss is 0.07697002539411187\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 232 completed, average training loss is 0.07619791216216981\n",
            "Testing Acc.: 0.9727000069618225\n",
            "Epoch 233 completed, average training loss is 0.07660090941974583\n",
            "Testing Acc.: 0.9746000063419342\n",
            "Epoch 234 completed, average training loss is 0.07590733036864548\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 235 completed, average training loss is 0.07615106345697617\n",
            "Testing Acc.: 0.9717000055313111\n",
            "Epoch 236 completed, average training loss is 0.0754765641503036\n",
            "Testing Acc.: 0.9739000064134598\n",
            "Epoch 237 completed, average training loss is 0.07503510190251594\n",
            "Testing Acc.: 0.9744000071287156\n",
            "Epoch 238 completed, average training loss is 0.07503119790771355\n",
            "Testing Acc.: 0.9726000040769577\n",
            "Epoch 239 completed, average training loss is 0.07563787642323101\n",
            "Testing Acc.: 0.9728000050783158\n",
            "Epoch 240 completed, average training loss is 0.07554251570875446\n",
            "Testing Acc.: 0.9717000037431717\n",
            "Epoch 241 completed, average training loss is 0.07474090438568964\n",
            "Testing Acc.: 0.9734000051021576\n",
            "Epoch 242 completed, average training loss is 0.07539635936263948\n",
            "Testing Acc.: 0.9730000048875809\n",
            "Epoch 243 completed, average training loss is 0.07559579201663534\n",
            "Testing Acc.: 0.9741000056266784\n",
            "Epoch 244 completed, average training loss is 0.07464886212333416\n",
            "Testing Acc.: 0.9716000062227249\n",
            "Epoch 245 completed, average training loss is 0.07434587497264147\n",
            "Testing Acc.: 0.9757000088691712\n",
            "Epoch 246 completed, average training loss is 0.07501916719212508\n",
            "Testing Acc.: 0.9750000071525574\n",
            "Epoch 247 completed, average training loss is 0.074187835745203\n",
            "Testing Acc.: 0.9752000081539154\n",
            "Epoch 248 completed, average training loss is 0.07431354340321074\n",
            "Testing Acc.: 0.9752000051736832\n",
            "Epoch 249 completed, average training loss is 0.07434200248137737\n",
            "Testing Acc.: 0.97180000603199\n",
            "Epoch 250 completed, average training loss is 0.07358339270266394\n",
            "Testing Acc.: 0.9740000051259995\n",
            "Epoch 251 completed, average training loss is 0.07372649560061593\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 252 completed, average training loss is 0.07426861936381708\n",
            "Testing Acc.: 0.9726000064611435\n",
            "Epoch 253 completed, average training loss is 0.07411788030837974\n",
            "Testing Acc.: 0.9734000033140182\n",
            "Epoch 254 completed, average training loss is 0.07444466177374125\n",
            "Testing Acc.: 0.9761000055074692\n",
            "Epoch 255 completed, average training loss is 0.07375388317741453\n",
            "Testing Acc.: 0.9751000046730042\n",
            "Epoch 256 completed, average training loss is 0.07353146233052636\n",
            "Testing Acc.: 0.9730000054836273\n",
            "Epoch 257 completed, average training loss is 0.07342100946465507\n",
            "Testing Acc.: 0.9713000047206879\n",
            "Epoch 258 completed, average training loss is 0.07342524683801457\n",
            "Testing Acc.: 0.9711000031232834\n",
            "Epoch 259 completed, average training loss is 0.07309651851421223\n",
            "Testing Acc.: 0.9737000066041946\n",
            "Epoch 260 completed, average training loss is 0.0733516511359873\n",
            "Testing Acc.: 0.9729000061750412\n",
            "Epoch 261 completed, average training loss is 0.07389777502277867\n",
            "Testing Acc.: 0.9752000057697296\n",
            "Epoch 262 completed, average training loss is 0.07332538911995168\n",
            "Testing Acc.: 0.9737000060081482\n",
            "Epoch 263 completed, average training loss is 0.07258631566927458\n",
            "Testing Acc.: 0.9753000068664551\n",
            "Epoch 264 completed, average training loss is 0.07329439306398854\n",
            "Testing Acc.: 0.9724000054597854\n",
            "Epoch 265 completed, average training loss is 0.07258667929408451\n",
            "Testing Acc.: 0.9756000071763993\n",
            "Epoch 266 completed, average training loss is 0.07325690135553789\n",
            "Testing Acc.: 0.9741000068187714\n",
            "Epoch 267 completed, average training loss is 0.07258583905640989\n",
            "Testing Acc.: 0.9757000076770782\n",
            "Epoch 268 completed, average training loss is 0.07316419116298979\n",
            "Testing Acc.: 0.9751000070571899\n",
            "Epoch 269 completed, average training loss is 0.07268430056593692\n",
            "Testing Acc.: 0.9753000074625016\n",
            "Epoch 270 completed, average training loss is 0.07222266204189509\n",
            "Testing Acc.: 0.9754000049829483\n",
            "Epoch 271 completed, average training loss is 0.07270086800679565\n",
            "Testing Acc.: 0.9744000047445297\n",
            "Epoch 272 completed, average training loss is 0.07263075322300816\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 273 completed, average training loss is 0.07174076069844887\n",
            "Testing Acc.: 0.9749000060558319\n",
            "Epoch 274 completed, average training loss is 0.07158202718167256\n",
            "Testing Acc.: 0.9739000064134598\n",
            "Epoch 275 completed, average training loss is 0.0725701194574746\n",
            "Testing Acc.: 0.9745000046491623\n",
            "Epoch 276 completed, average training loss is 0.07260741061065346\n",
            "Testing Acc.: 0.972500005364418\n",
            "Epoch 277 completed, average training loss is 0.07200744851802786\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 278 completed, average training loss is 0.07191733033396304\n",
            "Testing Acc.: 0.9723000031709671\n",
            "Epoch 279 completed, average training loss is 0.07227632631082087\n",
            "Testing Acc.: 0.9754000061750412\n",
            "Epoch 280 completed, average training loss is 0.07261228439087669\n",
            "Testing Acc.: 0.9742000061273575\n",
            "Epoch 281 completed, average training loss is 0.07172199856024236\n",
            "Testing Acc.: 0.9743000078201294\n",
            "Epoch 282 completed, average training loss is 0.07254843337694183\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 283 completed, average training loss is 0.07207710066344589\n",
            "Testing Acc.: 0.9741000068187714\n",
            "Epoch 284 completed, average training loss is 0.07164207303974157\n",
            "Testing Acc.: 0.9746000057458878\n",
            "Epoch 285 completed, average training loss is 0.0711613380148386\n",
            "Testing Acc.: 0.974200005531311\n",
            "Epoch 286 completed, average training loss is 0.07220723055225486\n",
            "Testing Acc.: 0.9748000037670136\n",
            "Epoch 287 completed, average training loss is 0.07163439652261634\n",
            "Testing Acc.: 0.9745000088214875\n",
            "Epoch 288 completed, average training loss is 0.07127480935926239\n",
            "Testing Acc.: 0.9739000058174133\n",
            "Epoch 289 completed, average training loss is 0.07175081571408858\n",
            "Testing Acc.: 0.9756000071763993\n",
            "Epoch 290 completed, average training loss is 0.07129847258251781\n",
            "Testing Acc.: 0.9752000063657761\n",
            "Epoch 291 completed, average training loss is 0.07062269501000022\n",
            "Testing Acc.: 0.9730000048875809\n",
            "Epoch 292 completed, average training loss is 0.071361588339981\n",
            "Testing Acc.: 0.9737000077962875\n",
            "Epoch 293 completed, average training loss is 0.07158583979743223\n",
            "Testing Acc.: 0.9747000050544739\n",
            "Epoch 294 completed, average training loss is 0.07051210137937838\n",
            "Testing Acc.: 0.9755000072717667\n",
            "Epoch 295 completed, average training loss is 0.07094576195037613\n",
            "Testing Acc.: 0.9742000025510787\n",
            "Epoch 296 completed, average training loss is 0.07146512433576087\n",
            "Testing Acc.: 0.9755000078678131\n",
            "Epoch 297 completed, average training loss is 0.07054233756692459\n",
            "Testing Acc.: 0.9763000053167343\n",
            "Epoch 298 completed, average training loss is 0.07102399754803627\n",
            "Testing Acc.: 0.9739000070095062\n",
            "Epoch 299 completed, average training loss is 0.07148852659932649\n",
            "Testing Acc.: 0.9725000065565109\n",
            "Epoch 300 completed, average training loss is 0.07088590770959854\n",
            "Testing Acc.: 0.9759000086784363\n",
            "Epoch 301 completed, average training loss is 0.07093496584023039\n",
            "Testing Acc.: 0.9769000059366226\n",
            "Epoch 302 completed, average training loss is 0.07084170213822896\n",
            "Testing Acc.: 0.9756000065803527\n",
            "Epoch 303 completed, average training loss is 0.07050454171917711\n",
            "Testing Acc.: 0.9750000035762787\n",
            "Epoch 304 completed, average training loss is 0.07075773572238783\n",
            "Testing Acc.: 0.9739000046253204\n",
            "Epoch 305 completed, average training loss is 0.07125409752518559\n",
            "Testing Acc.: 0.9748000055551529\n",
            "Epoch 306 completed, average training loss is 0.06940582197159528\n",
            "Testing Acc.: 0.974700003862381\n",
            "Epoch 307 completed, average training loss is 0.070358327444798\n",
            "Testing Acc.: 0.9745000046491623\n",
            "Epoch 308 completed, average training loss is 0.07064377843402327\n",
            "Testing Acc.: 0.9739000034332276\n",
            "Epoch 309 completed, average training loss is 0.07096911089417214\n",
            "Testing Acc.: 0.9736000061035156\n",
            "Epoch 310 completed, average training loss is 0.07076366546641415\n",
            "Testing Acc.: 0.9746000063419342\n",
            "Epoch 311 completed, average training loss is 0.06981673660222441\n",
            "Testing Acc.: 0.9749000060558319\n",
            "Epoch 312 completed, average training loss is 0.07047199725716685\n",
            "Testing Acc.: 0.9735000050067901\n",
            "Epoch 313 completed, average training loss is 0.07025511227858564\n",
            "Testing Acc.: 0.974200005531311\n",
            "Epoch 314 completed, average training loss is 0.07076749504931892\n",
            "Testing Acc.: 0.9747000044584274\n",
            "Epoch 315 completed, average training loss is 0.07035869609176491\n",
            "Testing Acc.: 0.9762000077962876\n",
            "Epoch 316 completed, average training loss is 0.07073796701656344\n",
            "Testing Acc.: 0.9743000072240829\n",
            "Epoch 317 completed, average training loss is 0.06977208075113595\n",
            "Testing Acc.: 0.9775000083446502\n",
            "Epoch 318 completed, average training loss is 0.07126878493387873\n",
            "Testing Acc.: 0.9754000055789948\n",
            "Epoch 319 completed, average training loss is 0.06991657007796069\n",
            "Testing Acc.: 0.9750000071525574\n",
            "Epoch 320 completed, average training loss is 0.0704465417843312\n",
            "Testing Acc.: 0.9749000072479248\n",
            "Epoch 321 completed, average training loss is 0.06993782500930441\n",
            "Testing Acc.: 0.9739000046253204\n",
            "Epoch 322 completed, average training loss is 0.07012799580814316\n",
            "Testing Acc.: 0.9751000076532363\n",
            "Epoch 323 completed, average training loss is 0.06981112323701381\n",
            "Testing Acc.: 0.9749000096321105\n",
            "Epoch 324 completed, average training loss is 0.06910845222029215\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 325 completed, average training loss is 0.07004766179559131\n",
            "Testing Acc.: 0.9755000048875808\n",
            "Epoch 326 completed, average training loss is 0.06976002499150734\n",
            "Testing Acc.: 0.975900005698204\n",
            "Epoch 327 completed, average training loss is 0.07006125037868817\n",
            "Testing Acc.: 0.9743000096082688\n",
            "Epoch 328 completed, average training loss is 0.06959312478778884\n",
            "Testing Acc.: 0.9737000066041946\n",
            "Epoch 329 completed, average training loss is 0.0695814796925212\n",
            "Testing Acc.: 0.9740000045299531\n",
            "Epoch 330 completed, average training loss is 0.06899929848344376\n",
            "Testing Acc.: 0.9721000057458877\n",
            "Epoch 331 completed, average training loss is 0.07037054843269289\n",
            "Testing Acc.: 0.976500004529953\n",
            "Epoch 332 completed, average training loss is 0.0692752790870145\n",
            "Testing Acc.: 0.9755000054836274\n",
            "Epoch 333 completed, average training loss is 0.069481935595007\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 334 completed, average training loss is 0.06935807486840834\n",
            "Testing Acc.: 0.975200007557869\n",
            "Epoch 335 completed, average training loss is 0.0701272616069764\n",
            "Testing Acc.: 0.9744000083208084\n",
            "Epoch 336 completed, average training loss is 0.0697679499303922\n",
            "Testing Acc.: 0.9754000067710876\n",
            "Epoch 337 completed, average training loss is 0.06880081488595655\n",
            "Testing Acc.: 0.975500009059906\n",
            "Epoch 338 completed, average training loss is 0.06902812159620225\n",
            "Testing Acc.: 0.9767000079154968\n",
            "Epoch 339 completed, average training loss is 0.06977834238049885\n",
            "Testing Acc.: 0.9768000066280365\n",
            "Epoch 340 completed, average training loss is 0.06965551446347187\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 341 completed, average training loss is 0.0695098863231639\n",
            "Testing Acc.: 0.97480000436306\n",
            "Epoch 342 completed, average training loss is 0.07015646206952321\n",
            "Testing Acc.: 0.9748000055551529\n",
            "Epoch 343 completed, average training loss is 0.07001619204878808\n",
            "Testing Acc.: 0.9759000051021576\n",
            "Epoch 344 completed, average training loss is 0.06869239715859293\n",
            "Testing Acc.: 0.9738000059127807\n",
            "Epoch 345 completed, average training loss is 0.06890000164198379\n",
            "Testing Acc.: 0.9737000066041946\n",
            "Epoch 346 completed, average training loss is 0.06915329688539108\n",
            "Testing Acc.: 0.9755000048875808\n",
            "Epoch 347 completed, average training loss is 0.06845975876552984\n",
            "Testing Acc.: 0.9750000047683716\n",
            "Epoch 348 completed, average training loss is 0.0697653488861397\n",
            "Testing Acc.: 0.9760000067949295\n",
            "Epoch 349 completed, average training loss is 0.06982504825883855\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 350 completed, average training loss is 0.0686358954043438\n",
            "Testing Acc.: 0.9757000082731246\n",
            "Epoch 351 completed, average training loss is 0.06866547603470584\n",
            "Testing Acc.: 0.9740000063180924\n",
            "Epoch 352 completed, average training loss is 0.06945420083589852\n",
            "Testing Acc.: 0.9758000046014785\n",
            "Epoch 353 completed, average training loss is 0.06889464251037376\n",
            "Testing Acc.: 0.9761000084877014\n",
            "Epoch 354 completed, average training loss is 0.06899564847078485\n",
            "Testing Acc.: 0.9769000083208084\n",
            "Epoch 355 completed, average training loss is 0.06891911280884717\n",
            "Testing Acc.: 0.9743000066280365\n",
            "Epoch 356 completed, average training loss is 0.06865377349662595\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 357 completed, average training loss is 0.0684635192140316\n",
            "Testing Acc.: 0.9743000036478042\n",
            "Epoch 358 completed, average training loss is 0.06904678038166215\n",
            "Testing Acc.: 0.9770000070333481\n",
            "Epoch 359 completed, average training loss is 0.06870673345867545\n",
            "Testing Acc.: 0.9752000063657761\n",
            "Epoch 360 completed, average training loss is 0.06801391422321709\n",
            "Testing Acc.: 0.9747000068426133\n",
            "Epoch 361 completed, average training loss is 0.06823503902802865\n",
            "Testing Acc.: 0.9752000063657761\n",
            "Epoch 362 completed, average training loss is 0.06901359291281552\n",
            "Testing Acc.: 0.9764000099897384\n",
            "Epoch 363 completed, average training loss is 0.06870714231859892\n",
            "Testing Acc.: 0.976500004529953\n",
            "Epoch 364 completed, average training loss is 0.0689298350053529\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 365 completed, average training loss is 0.06814012152996535\n",
            "Testing Acc.: 0.9735000056028366\n",
            "Epoch 366 completed, average training loss is 0.06844922489020973\n",
            "Testing Acc.: 0.976700006723404\n",
            "Epoch 367 completed, average training loss is 0.06796992278580244\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 368 completed, average training loss is 0.06885957358948265\n",
            "Testing Acc.: 0.9740000087022781\n",
            "Epoch 369 completed, average training loss is 0.06847233896221344\n",
            "Testing Acc.: 0.9760000079870224\n",
            "Epoch 370 completed, average training loss is 0.06898614520517489\n",
            "Testing Acc.: 0.9746000051498414\n",
            "Epoch 371 completed, average training loss is 0.06868631311692297\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 372 completed, average training loss is 0.06768467112521952\n",
            "Testing Acc.: 0.9749000066518784\n",
            "Epoch 373 completed, average training loss is 0.06926115169810752\n",
            "Testing Acc.: 0.9746000069379807\n",
            "Epoch 374 completed, average training loss is 0.06757818055804819\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 375 completed, average training loss is 0.06762555611281035\n",
            "Testing Acc.: 0.9737000072002411\n",
            "Epoch 376 completed, average training loss is 0.06826073680538684\n",
            "Testing Acc.: 0.9763000059127808\n",
            "Epoch 377 completed, average training loss is 0.06830969588675846\n",
            "Testing Acc.: 0.9725000065565109\n",
            "Epoch 378 completed, average training loss is 0.06798179638455622\n",
            "Testing Acc.: 0.9755000078678131\n",
            "Epoch 379 completed, average training loss is 0.06736987134286512\n",
            "Testing Acc.: 0.9756000083684921\n",
            "Epoch 380 completed, average training loss is 0.06815895723334203\n",
            "Testing Acc.: 0.9744000053405761\n",
            "Epoch 381 completed, average training loss is 0.06871630131965503\n",
            "Testing Acc.: 0.9736000061035156\n",
            "Epoch 382 completed, average training loss is 0.06803361482452601\n",
            "Testing Acc.: 0.975000006556511\n",
            "Epoch 383 completed, average training loss is 0.06778439625476797\n",
            "Testing Acc.: 0.9746000045537948\n",
            "Epoch 384 completed, average training loss is 0.0682006381258058\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 385 completed, average training loss is 0.06732982653969279\n",
            "Testing Acc.: 0.9760000050067902\n",
            "Epoch 386 completed, average training loss is 0.06841046084727471\n",
            "Testing Acc.: 0.9750000089406967\n",
            "Epoch 387 completed, average training loss is 0.06829767285380513\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 388 completed, average training loss is 0.06855520959555482\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 389 completed, average training loss is 0.06789405225620916\n",
            "Testing Acc.: 0.9757000076770782\n",
            "Epoch 390 completed, average training loss is 0.06759288923582062\n",
            "Testing Acc.: 0.9754000073671341\n",
            "Epoch 391 completed, average training loss is 0.06882715506013483\n",
            "Testing Acc.: 0.9742000043392182\n",
            "Epoch 392 completed, average training loss is 0.06773138604747753\n",
            "Testing Acc.: 0.9768000066280365\n",
            "Epoch 393 completed, average training loss is 0.06837530538594971\n",
            "Testing Acc.: 0.9753000056743621\n",
            "Epoch 394 completed, average training loss is 0.06773985066839183\n",
            "Testing Acc.: 0.9760000073909759\n",
            "Epoch 395 completed, average training loss is 0.06707936866752183\n",
            "Testing Acc.: 0.9738000059127807\n",
            "Epoch 396 completed, average training loss is 0.06783970636315644\n",
            "Testing Acc.: 0.975200007557869\n",
            "Epoch 397 completed, average training loss is 0.06726792333841634\n",
            "Testing Acc.: 0.9725000065565109\n",
            "Epoch 398 completed, average training loss is 0.0685756426184283\n",
            "Testing Acc.: 0.9749000072479248\n",
            "Epoch 399 completed, average training loss is 0.06770191126115\n",
            "Testing Acc.: 0.9742000079154969\n",
            "Epoch 400 completed, average training loss is 0.06855494523964202\n",
            "Testing Acc.: 0.9726000064611435\n",
            "Epoch 401 completed, average training loss is 0.07413323240354658\n",
            "Testing Acc.: 0.9744000047445297\n",
            "Epoch 402 completed, average training loss is 0.06693032406425724\n",
            "Testing Acc.: 0.9755000072717667\n",
            "Epoch 403 completed, average training loss is 0.06677679538571586\n",
            "Testing Acc.: 0.9753000074625016\n",
            "Epoch 404 completed, average training loss is 0.06708743422059342\n",
            "Testing Acc.: 0.9751000070571899\n",
            "Epoch 405 completed, average training loss is 0.06566483501344919\n",
            "Testing Acc.: 0.9731000065803528\n",
            "Epoch 406 completed, average training loss is 0.06586406658558795\n",
            "Testing Acc.: 0.9767000085115433\n",
            "Epoch 407 completed, average training loss is 0.06585312454650799\n",
            "Testing Acc.: 0.9745000064373016\n",
            "Epoch 408 completed, average training loss is 0.06493277324829251\n",
            "Testing Acc.: 0.9748000061511993\n",
            "Epoch 409 completed, average training loss is 0.06487619269794474\n",
            "Testing Acc.: 0.9748000061511993\n",
            "Epoch 410 completed, average training loss is 0.06498382199284\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 411 completed, average training loss is 0.06457645401164579\n",
            "Testing Acc.: 0.9770000064373017\n",
            "Epoch 412 completed, average training loss is 0.0650279131392017\n",
            "Testing Acc.: 0.9757000064849853\n",
            "Epoch 413 completed, average training loss is 0.06601644800122207\n",
            "Testing Acc.: 0.9742000073194503\n",
            "Epoch 414 completed, average training loss is 0.0652599247234563\n",
            "Testing Acc.: 0.9740000075101852\n",
            "Epoch 415 completed, average training loss is 0.06492921405627082\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 416 completed, average training loss is 0.06414939922668661\n",
            "Testing Acc.: 0.9745000052452087\n",
            "Epoch 417 completed, average training loss is 0.06448056639016916\n",
            "Testing Acc.: 0.9756000077724457\n",
            "Epoch 418 completed, average training loss is 0.06450293958652764\n",
            "Testing Acc.: 0.9757000064849853\n",
            "Epoch 419 completed, average training loss is 0.06497769034778078\n",
            "Testing Acc.: 0.9771000075340271\n",
            "Epoch 420 completed, average training loss is 0.06428181325551123\n",
            "Testing Acc.: 0.9757000076770782\n",
            "Epoch 421 completed, average training loss is 0.06490957209801612\n",
            "Testing Acc.: 0.9757000070810318\n",
            "Epoch 422 completed, average training loss is 0.065492136052344\n",
            "Testing Acc.: 0.9740000069141388\n",
            "Epoch 423 completed, average training loss is 0.06503773814377685\n",
            "Testing Acc.: 0.9753000074625016\n",
            "Epoch 424 completed, average training loss is 0.06451486543364202\n",
            "Testing Acc.: 0.9760000073909759\n",
            "Epoch 425 completed, average training loss is 0.06511056365910918\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 426 completed, average training loss is 0.06456801827026841\n",
            "Testing Acc.: 0.9746000057458878\n",
            "Epoch 427 completed, average training loss is 0.06454905532145251\n",
            "Testing Acc.: 0.9755000072717667\n",
            "Epoch 428 completed, average training loss is 0.06483528875435392\n",
            "Testing Acc.: 0.9759000062942504\n",
            "Epoch 429 completed, average training loss is 0.06428508392224709\n",
            "Testing Acc.: 0.9755000072717667\n",
            "Epoch 430 completed, average training loss is 0.06467132611044993\n",
            "Testing Acc.: 0.9747000050544739\n",
            "Epoch 431 completed, average training loss is 0.06513981348252855\n",
            "Testing Acc.: 0.9760000073909759\n",
            "Epoch 432 completed, average training loss is 0.06472115313789496\n",
            "Testing Acc.: 0.972700007557869\n",
            "Epoch 433 completed, average training loss is 0.06581561461789534\n",
            "Testing Acc.: 0.9739000070095062\n",
            "Epoch 434 completed, average training loss is 0.06438438618710886\n",
            "Testing Acc.: 0.9766000062227249\n",
            "Epoch 435 completed, average training loss is 0.06427647056446101\n",
            "Testing Acc.: 0.9748000073432922\n",
            "Epoch 436 completed, average training loss is 0.063763222922571\n",
            "Testing Acc.: 0.9771000081300736\n",
            "Epoch 437 completed, average training loss is 0.06535085229901597\n",
            "Testing Acc.: 0.9765000087022782\n",
            "Epoch 438 completed, average training loss is 0.06511358698053907\n",
            "Testing Acc.: 0.9759000086784363\n",
            "Epoch 439 completed, average training loss is 0.06348885079069684\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 440 completed, average training loss is 0.06392913250718266\n",
            "Testing Acc.: 0.9748000049591065\n",
            "Epoch 441 completed, average training loss is 0.06379609973713135\n",
            "Testing Acc.: 0.9754000067710876\n",
            "Epoch 442 completed, average training loss is 0.06339513612600664\n",
            "Testing Acc.: 0.9751000064611435\n",
            "Epoch 443 completed, average training loss is 0.06406401021173223\n",
            "Testing Acc.: 0.9737000077962875\n",
            "Epoch 444 completed, average training loss is 0.06382743561795602\n",
            "Testing Acc.: 0.9753000056743621\n",
            "Epoch 445 completed, average training loss is 0.06402730612084269\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 446 completed, average training loss is 0.06374413044890388\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 447 completed, average training loss is 0.06431160874858809\n",
            "Testing Acc.: 0.9758000057935715\n",
            "Epoch 448 completed, average training loss is 0.06422654283931478\n",
            "Testing Acc.: 0.9764000076055527\n",
            "Epoch 449 completed, average training loss is 0.06378671592567116\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 450 completed, average training loss is 0.06367323692655191\n",
            "Testing Acc.: 0.9763000077009201\n",
            "Epoch 451 completed, average training loss is 0.06417686515798171\n",
            "Testing Acc.: 0.9755000078678131\n",
            "Epoch 452 completed, average training loss is 0.06357790579786524\n",
            "Testing Acc.: 0.9763000065088272\n",
            "Epoch 453 completed, average training loss is 0.06321903220028617\n",
            "Testing Acc.: 0.9759000045061111\n",
            "Epoch 454 completed, average training loss is 0.06475632855280612\n",
            "Testing Acc.: 0.9781000077724457\n",
            "Epoch 455 completed, average training loss is 0.06343386825562145\n",
            "Testing Acc.: 0.9758000081777572\n",
            "Epoch 456 completed, average training loss is 0.06312106591106081\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 457 completed, average training loss is 0.06394569459448879\n",
            "Testing Acc.: 0.9752000051736832\n",
            "Epoch 458 completed, average training loss is 0.06380276653061931\n",
            "Testing Acc.: 0.975100005865097\n",
            "Epoch 459 completed, average training loss is 0.06409611114067956\n",
            "Testing Acc.: 0.9756000059843063\n",
            "Epoch 460 completed, average training loss is 0.06381927537498995\n",
            "Testing Acc.: 0.975900006890297\n",
            "Epoch 461 completed, average training loss is 0.06360098506789655\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 462 completed, average training loss is 0.06328087288343037\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 463 completed, average training loss is 0.06414660243317485\n",
            "Testing Acc.: 0.9768000078201294\n",
            "Epoch 464 completed, average training loss is 0.06333840871190963\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 465 completed, average training loss is 0.06365489694910745\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 466 completed, average training loss is 0.06354479798581451\n",
            "Testing Acc.: 0.9753000098466873\n",
            "Epoch 467 completed, average training loss is 0.06414295857927452\n",
            "Testing Acc.: 0.9756000077724457\n",
            "Epoch 468 completed, average training loss is 0.06350818492627393\n",
            "Testing Acc.: 0.9761000066995621\n",
            "Epoch 469 completed, average training loss is 0.06354641387161489\n",
            "Testing Acc.: 0.9762000072002411\n",
            "Epoch 470 completed, average training loss is 0.06276142006817584\n",
            "Testing Acc.: 0.975800006389618\n",
            "Epoch 471 completed, average training loss is 0.06453003836097196\n",
            "Testing Acc.: 0.9747000056505203\n",
            "Epoch 472 completed, average training loss is 0.063715031594038\n",
            "Testing Acc.: 0.9747000062465667\n",
            "Epoch 473 completed, average training loss is 0.06308621198792631\n",
            "Testing Acc.: 0.9757000064849853\n",
            "Epoch 474 completed, average training loss is 0.06306364564457909\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 475 completed, average training loss is 0.06394262867746875\n",
            "Testing Acc.: 0.9767000085115433\n",
            "Epoch 476 completed, average training loss is 0.06390627416083589\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 477 completed, average training loss is 0.0640586834276716\n",
            "Testing Acc.: 0.9777000069618225\n",
            "Epoch 478 completed, average training loss is 0.06307589389073352\n",
            "Testing Acc.: 0.9746000075340271\n",
            "Epoch 479 completed, average training loss is 0.06355578830776115\n",
            "Testing Acc.: 0.9748000049591065\n",
            "Epoch 480 completed, average training loss is 0.0637941775653356\n",
            "Testing Acc.: 0.9769000065326691\n",
            "Epoch 481 completed, average training loss is 0.06290821674861945\n",
            "Testing Acc.: 0.9767000073194504\n",
            "Epoch 482 completed, average training loss is 0.06279746741987764\n",
            "Testing Acc.: 0.9745000058412552\n",
            "Epoch 483 completed, average training loss is 0.06365690095350146\n",
            "Testing Acc.: 0.9748000055551529\n",
            "Epoch 484 completed, average training loss is 0.06341775275456409\n",
            "Testing Acc.: 0.9773000073432923\n",
            "Epoch 485 completed, average training loss is 0.06341626605329415\n",
            "Testing Acc.: 0.9766000086069107\n",
            "Epoch 486 completed, average training loss is 0.06298485956620425\n",
            "Testing Acc.: 0.9775000071525574\n",
            "Epoch 487 completed, average training loss is 0.06264423921549071\n",
            "Testing Acc.: 0.9741000092029571\n",
            "Epoch 488 completed, average training loss is 0.06293765559792519\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 489 completed, average training loss is 0.06424559288968643\n",
            "Testing Acc.: 0.9773000079393387\n",
            "Epoch 490 completed, average training loss is 0.06245395722333342\n",
            "Testing Acc.: 0.9753000086545944\n",
            "Epoch 491 completed, average training loss is 0.06242531479569152\n",
            "Testing Acc.: 0.9772000086307525\n",
            "Epoch 492 completed, average training loss is 0.06376753077609465\n",
            "Testing Acc.: 0.9758000081777572\n",
            "Epoch 493 completed, average training loss is 0.06285500147457546\n",
            "Testing Acc.: 0.9763000065088272\n",
            "Epoch 494 completed, average training loss is 0.06244801842607558\n",
            "Testing Acc.: 0.9769000089168549\n",
            "Epoch 495 completed, average training loss is 0.06252030187829707\n",
            "Testing Acc.: 0.9739000064134598\n",
            "Epoch 496 completed, average training loss is 0.06237312605294088\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 497 completed, average training loss is 0.06268603226480385\n",
            "Testing Acc.: 0.9756000071763993\n",
            "Epoch 498 completed, average training loss is 0.06277365091877679\n",
            "Testing Acc.: 0.9767000085115433\n",
            "Epoch 499 completed, average training loss is 0.06240104460467895\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 500 completed, average training loss is 0.06289415738002087\n",
            "Testing Acc.: 0.9756000059843063\n",
            "Epoch 501 completed, average training loss is 0.0628500923110793\n",
            "Testing Acc.: 0.9757000088691712\n",
            "Epoch 502 completed, average training loss is 0.06225584214630847\n",
            "Testing Acc.: 0.976700006723404\n",
            "Epoch 503 completed, average training loss is 0.06255440211078774\n",
            "Testing Acc.: 0.9768000084161759\n",
            "Epoch 504 completed, average training loss is 0.06238553004222922\n",
            "Testing Acc.: 0.9770000070333481\n",
            "Epoch 505 completed, average training loss is 0.06264974068850279\n",
            "Testing Acc.: 0.9769000101089478\n",
            "Epoch 506 completed, average training loss is 0.06263103556159573\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 507 completed, average training loss is 0.06309668328613043\n",
            "Testing Acc.: 0.9759000062942504\n",
            "Epoch 508 completed, average training loss is 0.06388995614135638\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 509 completed, average training loss is 0.06269427051922927\n",
            "Testing Acc.: 0.9754000061750412\n",
            "Epoch 510 completed, average training loss is 0.06277035455646304\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 511 completed, average training loss is 0.06255451745120809\n",
            "Testing Acc.: 0.976000006198883\n",
            "Epoch 512 completed, average training loss is 0.06291305370085562\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 513 completed, average training loss is 0.06272625540926431\n",
            "Testing Acc.: 0.9757000052928925\n",
            "Epoch 514 completed, average training loss is 0.0621312774977802\n",
            "Testing Acc.: 0.977000008225441\n",
            "Epoch 515 completed, average training loss is 0.0628115429286845\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 516 completed, average training loss is 0.062368043510553736\n",
            "Testing Acc.: 0.976200008392334\n",
            "Epoch 517 completed, average training loss is 0.062427152772822105\n",
            "Testing Acc.: 0.976100007891655\n",
            "Epoch 518 completed, average training loss is 0.06176946443039924\n",
            "Testing Acc.: 0.9766000080108642\n",
            "Epoch 519 completed, average training loss is 0.0625640991772525\n",
            "Testing Acc.: 0.975000006556511\n",
            "Epoch 520 completed, average training loss is 0.06232578576309607\n",
            "Testing Acc.: 0.9752000063657761\n",
            "Epoch 521 completed, average training loss is 0.06231361334095709\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 522 completed, average training loss is 0.06274208486313\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 523 completed, average training loss is 0.06188266593264416\n",
            "Testing Acc.: 0.9759000074863434\n",
            "Epoch 524 completed, average training loss is 0.06208270298394685\n",
            "Testing Acc.: 0.9769000089168549\n",
            "Epoch 525 completed, average training loss is 0.06243475064092005\n",
            "Testing Acc.: 0.9772000068426132\n",
            "Epoch 526 completed, average training loss is 0.06220206935501968\n",
            "Testing Acc.: 0.9757000088691712\n",
            "Epoch 527 completed, average training loss is 0.06366447486681864\n",
            "Testing Acc.: 0.975900005698204\n",
            "Epoch 528 completed, average training loss is 0.06243937650385002\n",
            "Testing Acc.: 0.9766000074148178\n",
            "Epoch 529 completed, average training loss is 0.06210838422877714\n",
            "Testing Acc.: 0.9766000086069107\n",
            "Epoch 530 completed, average training loss is 0.06270165951379264\n",
            "Testing Acc.: 0.9756000077724457\n",
            "Epoch 531 completed, average training loss is 0.06267455862602218\n",
            "Testing Acc.: 0.9746000069379807\n",
            "Epoch 532 completed, average training loss is 0.06209494603953014\n",
            "Testing Acc.: 0.9761000072956085\n",
            "Epoch 533 completed, average training loss is 0.06148759052235012\n",
            "Testing Acc.: 0.9760000067949295\n",
            "Epoch 534 completed, average training loss is 0.062450788574448475\n",
            "Testing Acc.: 0.9765000087022782\n",
            "Epoch 535 completed, average training loss is 0.062013044127185515\n",
            "Testing Acc.: 0.9750000059604644\n",
            "Epoch 536 completed, average training loss is 0.06323737451961885\n",
            "Testing Acc.: 0.9768000066280365\n",
            "Epoch 537 completed, average training loss is 0.06186678943457082\n",
            "Testing Acc.: 0.9753000056743621\n",
            "Epoch 538 completed, average training loss is 0.06201675078715198\n",
            "Testing Acc.: 0.9753000086545944\n",
            "Epoch 539 completed, average training loss is 0.06339415746973827\n",
            "Testing Acc.: 0.9762000072002411\n",
            "Epoch 540 completed, average training loss is 0.06137046209264857\n",
            "Testing Acc.: 0.9748000061511993\n",
            "Epoch 541 completed, average training loss is 0.06226596643527349\n",
            "Testing Acc.: 0.9749000060558319\n",
            "Epoch 542 completed, average training loss is 0.062123371795751156\n",
            "Testing Acc.: 0.975100005865097\n",
            "Epoch 543 completed, average training loss is 0.06145342194940895\n",
            "Testing Acc.: 0.9773000079393387\n",
            "Epoch 544 completed, average training loss is 0.06216488945491922\n",
            "Testing Acc.: 0.9761000084877014\n",
            "Epoch 545 completed, average training loss is 0.061758798501299075\n",
            "Testing Acc.: 0.9774000084400177\n",
            "Epoch 546 completed, average training loss is 0.061583254610110695\n",
            "Testing Acc.: 0.9768000066280365\n",
            "Epoch 547 completed, average training loss is 0.0623393439342423\n",
            "Testing Acc.: 0.9755000060796738\n",
            "Epoch 548 completed, average training loss is 0.061613228753364335\n",
            "Testing Acc.: 0.976200008392334\n",
            "Epoch 549 completed, average training loss is 0.06200011818281685\n",
            "Testing Acc.: 0.9750000083446503\n",
            "Epoch 550 completed, average training loss is 0.06186133193046165\n",
            "Testing Acc.: 0.9768000054359436\n",
            "Epoch 551 completed, average training loss is 0.06241462873217339\n",
            "Testing Acc.: 0.977700006365776\n",
            "Epoch 552 completed, average training loss is 0.06207186372252181\n",
            "Testing Acc.: 0.9752000063657761\n",
            "Epoch 553 completed, average training loss is 0.06136871588804449\n",
            "Testing Acc.: 0.9740000063180924\n",
            "Epoch 554 completed, average training loss is 0.06277621562592685\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 555 completed, average training loss is 0.06121008901624009\n",
            "Testing Acc.: 0.9759000092744827\n",
            "Epoch 556 completed, average training loss is 0.061533170787928006\n",
            "Testing Acc.: 0.9762000060081482\n",
            "Epoch 557 completed, average training loss is 0.06195757369200389\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 558 completed, average training loss is 0.06147648011916317\n",
            "Testing Acc.: 0.9757000076770782\n",
            "Epoch 559 completed, average training loss is 0.06097986337573578\n",
            "Testing Acc.: 0.9761000049114227\n",
            "Epoch 560 completed, average training loss is 0.06173780829257642\n",
            "Testing Acc.: 0.9774000072479248\n",
            "Epoch 561 completed, average training loss is 0.06113825854767735\n",
            "Testing Acc.: 0.977700007557869\n",
            "Epoch 562 completed, average training loss is 0.06141680184208478\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 563 completed, average training loss is 0.06269173694813314\n",
            "Testing Acc.: 0.975800006389618\n",
            "Epoch 564 completed, average training loss is 0.06164256077337389\n",
            "Testing Acc.: 0.9764000046253204\n",
            "Epoch 565 completed, average training loss is 0.06127386763536682\n",
            "Testing Acc.: 0.9768000090122223\n",
            "Epoch 566 completed, average training loss is 0.061717345011420545\n",
            "Testing Acc.: 0.9765000081062317\n",
            "Epoch 567 completed, average training loss is 0.06114728861333182\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 568 completed, average training loss is 0.06154894208807188\n",
            "Testing Acc.: 0.9767000085115433\n",
            "Epoch 569 completed, average training loss is 0.062221432358104116\n",
            "Testing Acc.: 0.975200007557869\n",
            "Epoch 570 completed, average training loss is 0.061652447684512786\n",
            "Testing Acc.: 0.976100007891655\n",
            "Epoch 571 completed, average training loss is 0.0614946927735582\n",
            "Testing Acc.: 0.9747000074386597\n",
            "Epoch 572 completed, average training loss is 0.0611009658073696\n",
            "Testing Acc.: 0.9763000053167343\n",
            "Epoch 573 completed, average training loss is 0.06196216106647626\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 574 completed, average training loss is 0.06174472493274758\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 575 completed, average training loss is 0.06264431006585558\n",
            "Testing Acc.: 0.9768000066280365\n",
            "Epoch 576 completed, average training loss is 0.06154016041662544\n",
            "Testing Acc.: 0.9769000083208084\n",
            "Epoch 577 completed, average training loss is 0.06126262629947936\n",
            "Testing Acc.: 0.9765000069141387\n",
            "Epoch 578 completed, average training loss is 0.06189781259046868\n",
            "Testing Acc.: 0.9755000066757202\n",
            "Epoch 579 completed, average training loss is 0.061936202639093\n",
            "Testing Acc.: 0.9765000069141387\n",
            "Epoch 580 completed, average training loss is 0.06099435568011055\n",
            "Testing Acc.: 0.9747000050544739\n",
            "Epoch 581 completed, average training loss is 0.06092055636535709\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 582 completed, average training loss is 0.06172867921181023\n",
            "Testing Acc.: 0.9753000056743621\n",
            "Epoch 583 completed, average training loss is 0.06155594480922446\n",
            "Testing Acc.: 0.9760000079870224\n",
            "Epoch 584 completed, average training loss is 0.06093173562704275\n",
            "Testing Acc.: 0.9756000065803527\n",
            "Epoch 585 completed, average training loss is 0.06264880522115467\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 586 completed, average training loss is 0.061863600655148426\n",
            "Testing Acc.: 0.9769000089168549\n",
            "Epoch 587 completed, average training loss is 0.061030047262708345\n",
            "Testing Acc.: 0.9766000080108642\n",
            "Epoch 588 completed, average training loss is 0.06202786419695864\n",
            "Testing Acc.: 0.9769000077247619\n",
            "Epoch 589 completed, average training loss is 0.06129652612338153\n",
            "Testing Acc.: 0.9758000081777572\n",
            "Epoch 590 completed, average training loss is 0.06028998715647807\n",
            "Testing Acc.: 0.9756000065803527\n",
            "Epoch 591 completed, average training loss is 0.061306250288616866\n",
            "Testing Acc.: 0.9766000068187713\n",
            "Epoch 592 completed, average training loss is 0.06027827575725193\n",
            "Testing Acc.: 0.9755000072717667\n",
            "Epoch 593 completed, average training loss is 0.061223347059761485\n",
            "Testing Acc.: 0.9761000066995621\n",
            "Epoch 594 completed, average training loss is 0.06263179930625483\n",
            "Testing Acc.: 0.9762000089883804\n",
            "Epoch 595 completed, average training loss is 0.061169983475313834\n",
            "Testing Acc.: 0.9766000086069107\n",
            "Epoch 596 completed, average training loss is 0.06085215887598072\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 597 completed, average training loss is 0.06130476886406541\n",
            "Testing Acc.: 0.9760000050067902\n",
            "Epoch 598 completed, average training loss is 0.061275806907409185\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 599 completed, average training loss is 0.06140502247183273\n",
            "Testing Acc.: 0.9771000075340271\n",
            "Epoch 600 completed, average training loss is 0.061477489174188424\n",
            "Testing Acc.: 0.9759000062942504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7Y5lj0USE4t",
        "outputId": "99dfcd0a-8ad2-43ef-b555-10404e0cba0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ^^^^ Above reads:\n",
        "### Epoch 600 completed, average training loss is 0.061477489174188424\n",
        "### Testing Acc.: 0.9759000062942504\n",
        "\n",
        "\n",
        "# Save model, for not having to fully retrain it each time.\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, 'unquantised/model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unquantised/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq-uQ3fPSE4x",
        "outputId": "a4103d80-f73b-4fcd-979c-7318bee6acd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9759000062942504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C3TzktgYSE41",
        "outputId": "089bce05-79db-4c81-c99b-9a5deaad40f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!zip -r unquantised.zip unquantised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: unquantised/ (stored 0%)\n",
            "  adding: unquantised/model.ckpt.data-00000-of-00001 (deflated 33%)\n",
            "  adding: unquantised/model.ckpt.index (deflated 43%)\n",
            "  adding: unquantised/checkpoint (deflated 42%)\n",
            "  adding: unquantised/model.ckpt.meta (deflated 88%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1B3LcU1fSE45"
      },
      "source": [
        "## 1.3 Weights rounding: 97.46%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "77c6e76d-604b-434b-f704-3278de688d23",
        "id": "pPCCsZY0SE46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!unzip unquantised.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  unquantised.zip\n",
            "replace unquantised/model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR8IboGzSE4-",
        "outputId": "1ee1df82-07c9-4484-f846-be18d1332d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)\n",
        "\n",
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool\n",
        "\n",
        "def network1(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  conv2 = slim.conv2d(conv, 3, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2')\n",
        "  \n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out\n",
        "\n",
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network1(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "ckpt = tf.train.get_checkpoint_state('unquantised')\n",
        "saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0831 16:21:09.277668 140007541606272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9759000062942504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b9IxLn_OSE5B",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2', reuse=True) as scope_conv:\n",
        "  w2 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhApSA71SE5H",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 4."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OVVIfSCISE5J",
        "outputId": "0fd2dec5-c8cf-4879-a98d-c5bda8f31f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Rounding operation\n",
        "rounding_weights1_op = tf.assign(w1, \n",
        "                          tf.round(w1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_bias1_op = tf.assign(b1,\n",
        "                          tf.round(b1/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "rounding_weights2_op = tf.assign(w2, \n",
        "                          tf.round(w2/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_thresh_op = tf.assign(thresh,\n",
        "                          tf.round(thresh/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "_ = sess.run([rounding_weights1_op, rounding_bias1_op])\n",
        "_ = sess.run([rounding_weights2_op, rounding_thresh_op])\n",
        "\n",
        "\n",
        "# Show final distribution of weights\n",
        "w1_values, b1_values = sess.run([w1, b1])\n",
        "w2_values, thresh_values = sess.run([w2, thresh])\n",
        "\n",
        "kernel_values = (list(w1_values.flatten()) + list(b1_values.flatten())\n",
        "                 + list(w2_values.flatten())\n",
        "                 + list(thresh_values.flatten()))\n",
        "fig = plt.figure()\n",
        "plt.scatter(kernel_values, [1]*len(kernel_values))\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9746000081300735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1IDcpJB6SE5R"
      },
      "source": [
        "## 1.4 Retrain FC: 97.61%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yHmVrXzuSE5S",
        "colab": {}
      },
      "source": [
        "LR = 0.001/4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sGHYUyi1SE5X",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('dense1', reuse=True) as scope_conv:\n",
        "  fc1_k = tf.get_variable('kernel')\n",
        "  fc1_b = tf.get_variable('bias')\n",
        "\n",
        "with tf.variable_scope('dense2', reuse=True) as scope_conv:\n",
        "  fc2_k = tf.get_variable('kernel')\n",
        "  fc2_b = tf.get_variable('bias')\n",
        "\n",
        "opt_fc = tf.train.AdamOptimizer(learning_rate=LR, name='Adam_reg')\n",
        "opt_fc_op = opt_fc.minimize(loss_op, var_list=[fc1_k, fc1_b,\n",
        "                                               fc2_k, fc2_b])\n",
        "\n",
        "sess.run(tf.variables_initializer(opt_fc.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mbrY1SOySE5d",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4SDogzb0SE5f",
        "outputId": "ed5789fa-20de-4a30-8f73-a1074cb5c765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bin_rate_feed = MAX_BIN_RATE + 1\n",
        "\n",
        "for epoch in range(EPOCHS*2):\n",
        "\n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "\n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "\n",
        "    _, current_loss = sess.run([opt_fc_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "\n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 0.06240302110789344\n",
            "Testing Acc.: 0.9754000061750412\n",
            "Epoch 2 completed, average training loss is 0.061438791300946226\n",
            "Testing Acc.: 0.9761000072956085\n",
            "Epoch 3 completed, average training loss is 0.06187951751402579\n",
            "Testing Acc.: 0.9757000064849853\n",
            "Epoch 4 completed, average training loss is 0.06145342868442337\n",
            "Testing Acc.: 0.9746000057458878\n",
            "Epoch 5 completed, average training loss is 0.061461475035951785\n",
            "Testing Acc.: 0.9767000061273575\n",
            "Epoch 6 completed, average training loss is 0.06135069111168074\n",
            "Testing Acc.: 0.9769000065326691\n",
            "Epoch 7 completed, average training loss is 0.061346774341848986\n",
            "Testing Acc.: 0.9776000076532364\n",
            "Epoch 8 completed, average training loss is 0.06107683159488564\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 9 completed, average training loss is 0.06124272273077319\n",
            "Testing Acc.: 0.975300008058548\n",
            "Epoch 10 completed, average training loss is 0.061452872081814956\n",
            "Testing Acc.: 0.9768000078201294\n",
            "Epoch 11 completed, average training loss is 0.061345732835276674\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 12 completed, average training loss is 0.06117536839485789\n",
            "Testing Acc.: 0.9761000061035157\n",
            "Epoch 13 completed, average training loss is 0.061001849497358006\n",
            "Testing Acc.: 0.9765000051259994\n",
            "Epoch 14 completed, average training loss is 0.0611400031116015\n",
            "Testing Acc.: 0.9764000070095062\n",
            "Epoch 15 completed, average training loss is 0.061439941937181476\n",
            "Testing Acc.: 0.9760000067949295\n",
            "Epoch 16 completed, average training loss is 0.06148761825092758\n",
            "Testing Acc.: 0.975300008058548\n",
            "Epoch 17 completed, average training loss is 0.06113352416005607\n",
            "Testing Acc.: 0.9764000046253204\n",
            "Epoch 18 completed, average training loss is 0.06101420585182495\n",
            "Testing Acc.: 0.9762000060081482\n",
            "Epoch 19 completed, average training loss is 0.06074771722313017\n",
            "Testing Acc.: 0.9760000056028366\n",
            "Epoch 20 completed, average training loss is 0.06088472998002544\n",
            "Testing Acc.: 0.9749000066518784\n",
            "Epoch 21 completed, average training loss is 0.060765435180316366\n",
            "Testing Acc.: 0.976800007224083\n",
            "Epoch 22 completed, average training loss is 0.060994601702938474\n",
            "Testing Acc.: 0.9756000065803527\n",
            "Epoch 23 completed, average training loss is 0.060991889318684114\n",
            "Testing Acc.: 0.9758000057935715\n",
            "Epoch 24 completed, average training loss is 0.060796806930253904\n",
            "Testing Acc.: 0.9766000080108642\n",
            "Epoch 25 completed, average training loss is 0.06099247767434766\n",
            "Testing Acc.: 0.9761000061035157\n",
            "Epoch 26 completed, average training loss is 0.06083791974834943\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 27 completed, average training loss is 0.061002582409419116\n",
            "Testing Acc.: 0.9762000054121017\n",
            "Epoch 28 completed, average training loss is 0.06082512840628624\n",
            "Testing Acc.: 0.9769000065326691\n",
            "Epoch 29 completed, average training loss is 0.06067314024160927\n",
            "Testing Acc.: 0.9765000057220459\n",
            "Epoch 30 completed, average training loss is 0.06062334103684407\n",
            "Testing Acc.: 0.9753000062704086\n",
            "Epoch 31 completed, average training loss is 0.06102464832443123\n",
            "Testing Acc.: 0.9761000061035157\n",
            "Epoch 32 completed, average training loss is 0.060529658130835745\n",
            "Testing Acc.: 0.9746000063419342\n",
            "Epoch 33 completed, average training loss is 0.06065570949809626\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 34 completed, average training loss is 0.06048853381536901\n",
            "Testing Acc.: 0.974500007033348\n",
            "Epoch 35 completed, average training loss is 0.06070337095297873\n",
            "Testing Acc.: 0.9765000063180923\n",
            "Epoch 36 completed, average training loss is 0.060684274224719656\n",
            "Testing Acc.: 0.9763000071048736\n",
            "Epoch 37 completed, average training loss is 0.060708453567543376\n",
            "Testing Acc.: 0.9751000070571899\n",
            "Epoch 38 completed, average training loss is 0.060848066778853536\n",
            "Testing Acc.: 0.9766000086069107\n",
            "Epoch 39 completed, average training loss is 0.060960701211200403\n",
            "Testing Acc.: 0.9754000061750412\n",
            "Epoch 40 completed, average training loss is 0.06048737220543747\n",
            "Testing Acc.: 0.9743000072240829\n",
            "Epoch 41 completed, average training loss is 0.06067172567825765\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 42 completed, average training loss is 0.060885673710145055\n",
            "Testing Acc.: 0.9761000055074692\n",
            "Epoch 43 completed, average training loss is 0.06036544785873654\n",
            "Testing Acc.: 0.9744000071287156\n",
            "Epoch 44 completed, average training loss is 0.060631587612830724\n",
            "Testing Acc.: 0.9766000056266785\n",
            "Epoch 45 completed, average training loss is 0.06071839504952853\n",
            "Testing Acc.: 0.9765000069141387\n",
            "Epoch 46 completed, average training loss is 0.06085179538621257\n",
            "Testing Acc.: 0.9765000063180923\n",
            "Epoch 47 completed, average training loss is 0.060376473038923\n",
            "Testing Acc.: 0.9765000069141387\n",
            "Epoch 48 completed, average training loss is 0.06076932770665735\n",
            "Testing Acc.: 0.9759000062942504\n",
            "Epoch 49 completed, average training loss is 0.060196667049701016\n",
            "Testing Acc.: 0.9758000069856644\n",
            "Epoch 50 completed, average training loss is 0.06061255946212138\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 51 completed, average training loss is 0.06061430317737783\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 52 completed, average training loss is 0.06018921764179443\n",
            "Testing Acc.: 0.9766000068187713\n",
            "Epoch 53 completed, average training loss is 0.060508088615800565\n",
            "Testing Acc.: 0.977000008225441\n",
            "Epoch 54 completed, average training loss is 0.060666874484159056\n",
            "Testing Acc.: 0.9762000060081482\n",
            "Epoch 55 completed, average training loss is 0.06042007602052763\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 56 completed, average training loss is 0.06040390790207312\n",
            "Testing Acc.: 0.9756000059843063\n",
            "Epoch 57 completed, average training loss is 0.0607282206757615\n",
            "Testing Acc.: 0.9765000075101853\n",
            "Epoch 58 completed, average training loss is 0.06010590907496711\n",
            "Testing Acc.: 0.9751000052690506\n",
            "Epoch 59 completed, average training loss is 0.06075405968081517\n",
            "Testing Acc.: 0.9747000050544739\n",
            "Epoch 60 completed, average training loss is 0.06024147076609855\n",
            "Testing Acc.: 0.9771000063419342\n",
            "Epoch 61 completed, average training loss is 0.060397891308336206\n",
            "Testing Acc.: 0.9769000065326691\n",
            "Epoch 62 completed, average training loss is 0.06003532148742428\n",
            "Testing Acc.: 0.9772000074386596\n",
            "Epoch 63 completed, average training loss is 0.06040312704630196\n",
            "Testing Acc.: 0.9762000066041946\n",
            "Epoch 64 completed, average training loss is 0.05990121559938416\n",
            "Testing Acc.: 0.9754000079631805\n",
            "Epoch 65 completed, average training loss is 0.05991130982603257\n",
            "Testing Acc.: 0.9766000074148178\n",
            "Epoch 66 completed, average training loss is 0.06065333532091851\n",
            "Testing Acc.: 0.9762000054121017\n",
            "Epoch 67 completed, average training loss is 0.06008660395784925\n",
            "Testing Acc.: 0.9748000073432922\n",
            "Epoch 68 completed, average training loss is 0.06012290165487987\n",
            "Testing Acc.: 0.9770000052452087\n",
            "Epoch 69 completed, average training loss is 0.06039856885171806\n",
            "Testing Acc.: 0.9758000075817108\n",
            "Epoch 70 completed, average training loss is 0.0603643068294817\n",
            "Testing Acc.: 0.975800006389618\n",
            "Epoch 71 completed, average training loss is 0.06005575146448488\n",
            "Testing Acc.: 0.9761000055074692\n",
            "Epoch 72 completed, average training loss is 0.0599991222564131\n",
            "Testing Acc.: 0.9768000078201294\n",
            "Epoch 73 completed, average training loss is 0.06041534710559063\n",
            "Testing Acc.: 0.9761000066995621\n",
            "Epoch 74 completed, average training loss is 0.05991198158900564\n",
            "Testing Acc.: 0.9760000056028366\n",
            "Epoch 75 completed, average training loss is 0.060271103565270705\n",
            "Testing Acc.: 0.9756000071763993\n",
            "Epoch 76 completed, average training loss is 0.06055080866596351\n",
            "Testing Acc.: 0.9752000069618225\n",
            "Epoch 77 completed, average training loss is 0.06054312146035954\n",
            "Testing Acc.: 0.9763000059127808\n",
            "Epoch 78 completed, average training loss is 0.060162755254811295\n",
            "Testing Acc.: 0.976000006198883\n",
            "Epoch 79 completed, average training loss is 0.06029799377390494\n",
            "Testing Acc.: 0.9761000049114227\n",
            "Epoch 80 completed, average training loss is 0.06010073966269071\n",
            "Testing Acc.: 0.9761000072956085\n",
            "Epoch 81 completed, average training loss is 0.060159035664206995\n",
            "Testing Acc.: 0.9761000049114227\n",
            "Epoch 82 completed, average training loss is 0.05976792436131897\n",
            "Testing Acc.: 0.9758000087738037\n",
            "Epoch 83 completed, average training loss is 0.06010246109527846\n",
            "Testing Acc.: 0.9763000065088272\n",
            "Epoch 84 completed, average training loss is 0.0598335824906826\n",
            "Testing Acc.: 0.9744000089168549\n",
            "Epoch 85 completed, average training loss is 0.06005499210984756\n",
            "Testing Acc.: 0.9764000064134598\n",
            "Epoch 86 completed, average training loss is 0.05994820114923641\n",
            "Testing Acc.: 0.9756000059843063\n",
            "Epoch 87 completed, average training loss is 0.060012180698880306\n",
            "Testing Acc.: 0.9772000056505203\n",
            "Epoch 88 completed, average training loss is 0.059955013497189305\n",
            "Testing Acc.: 0.9769000059366226\n",
            "Epoch 89 completed, average training loss is 0.059978537111310286\n",
            "Testing Acc.: 0.9757000058889389\n",
            "Epoch 90 completed, average training loss is 0.060172032622698074\n",
            "Testing Acc.: 0.9771000051498413\n",
            "Epoch 91 completed, average training loss is 0.06039778913914536\n",
            "Testing Acc.: 0.9751000076532363\n",
            "Epoch 92 completed, average training loss is 0.05952004844555631\n",
            "Testing Acc.: 0.9765000063180923\n",
            "Epoch 93 completed, average training loss is 0.05997349330534538\n",
            "Testing Acc.: 0.977000008225441\n",
            "Epoch 94 completed, average training loss is 0.059596850857293854\n",
            "Testing Acc.: 0.9761000061035157\n",
            "Epoch 95 completed, average training loss is 0.06004708299568544\n",
            "Testing Acc.: 0.9760000067949295\n",
            "Epoch 96 completed, average training loss is 0.0602354762214236\n",
            "Testing Acc.: 0.9741000044345856\n",
            "Epoch 97 completed, average training loss is 0.05981722836033441\n",
            "Testing Acc.: 0.9768000042438507\n",
            "Epoch 98 completed, average training loss is 0.059758789371699095\n",
            "Testing Acc.: 0.9757000064849853\n",
            "Epoch 99 completed, average training loss is 0.059794659099231165\n",
            "Testing Acc.: 0.9761000055074692\n",
            "Epoch 100 completed, average training loss is 0.060223024408333006\n",
            "Testing Acc.: 0.9761000061035157\n",
            "Testing Acc.: 0.9761000061035157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQExdNNnSE5k",
        "colab": {}
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE)\n",
        "test_accuracy_bin_rate(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cP44vuN7VVp7"
      },
      "source": [
        "# 2. 4 bits unsigned quantisation: 97.0%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a54c6vgKVVqA"
      },
      "source": [
        "## 2.1 Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4t3RWeIKVVqE",
        "colab": {}
      },
      "source": [
        "# Quantize a [0, +127] value on 4 bits, so that\n",
        "# it fits in 4 DREGs\n",
        "# 4 DREGs for the most significant bits\n",
        "def quantization_4bits_unsigned(tensor):\n",
        "  sign = tf.sign(tensor)\n",
        "  \n",
        "  cast = tf.cast(sign*tensor, tf.int32)\n",
        "  sign = tf.cast(sign, tf.int32)\n",
        "  \n",
        "  digitized = (64*tf.mod(tf.bitwise.right_shift(cast,6), 2) +\n",
        "               32*tf.mod(tf.bitwise.right_shift(cast,5), 2) +\n",
        "               16*tf.mod(tf.bitwise.right_shift(cast,4), 2) + \n",
        "               8*tf.mod(tf.bitwise.right_shift(cast,3), 2))\n",
        "  digitized = digitized * sign  \n",
        "  return tf.cast(digitized, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G9q4QKNRVVqW",
        "colab": {}
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTLdjPhJVVqe",
        "colab": {}
      },
      "source": [
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l19NoPeMVVqm",
        "colab": {}
      },
      "source": [
        "def network2(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  # C2.1, none of the input is quantized\n",
        "  conv2_1 = slim.conv2d(conv, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.1')\n",
        "  \n",
        "  # C2.2, two inputs (out of 3) are quantized\n",
        "  slice_A_2_2 = quantization_4bits_unsigned(conv[...,:2])\n",
        "  slice_B_2_2 = conv[...,2:]\n",
        "  conv2_2_input = tf.concat([slice_A_2_2, slice_B_2_2], axis=-1)\n",
        "  conv2_2 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.2')\n",
        "  \n",
        "  # C2.3, same inputs as for C2.2 (2 feature maps quantified out of 3)\n",
        "  conv2_3 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.3')\n",
        "  \n",
        "  \n",
        "  conv2 = tf.concat([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JJouJhZ-VVqs",
        "colab": {}
      },
      "source": [
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network2(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hHGuhjANVVqw",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qErGvB8_VVq1",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2.1', reuse=True) as scope_conv:\n",
        "  w2_1 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.2', reuse=True) as scope_conv:\n",
        "  w2_2 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.3', reuse=True) as scope_conv:\n",
        "  w2_3 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DrqF7r_wVVq6",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 25."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2m_ydjBpVVrA",
        "colab": {}
      },
      "source": [
        "def customRegularizerConv(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_CONV*np.pi*(x-ROUNDING_STEP_CONV/2))+1.\n",
        "\n",
        "def customRegularizerBias(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_BIAS*np.pi*(x-ROUNDING_STEP_BIAS/2))+1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nah-TASkVVrE",
        "colab": {}
      },
      "source": [
        "NUM_PARAM_REG = 30. + 27. + 27. + 27. + 3.\n",
        "reg_losses = 27./NUM_PARAM_REG *tf.reduce_mean(customRegularizerConv(w1))\n",
        "reg_losses += 3./NUM_PARAM_REG *tf.reduce_mean(customRegularizerBias(b1))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_1))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_2))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_3))\n",
        "reg_losses += 3./NUM_PARAM_REG*tf.reduce_mean(customRegularizerBias(thresh))\n",
        "\n",
        "reg_factor_ph = tf.placeholder(tf.float32)\n",
        "loss_with_reg_op = loss_op + reg_factor_ph * reg_losses\n",
        "\n",
        "lr_ph = tf.placeholder(tf.float32)\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr_ph, name='Adam_reg')\n",
        "opt_op = opt.minimize(loss_with_reg_op)\n",
        "sess.run(tf.variables_initializer(opt.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfR_HMiAVVrK"
      },
      "source": [
        "## 2.2 Training: 97.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LhTFJCToVVrL"
      },
      "source": [
        "Evolving params:\n",
        "- lr_ph : learning rate\n",
        "- reg_factor_ph : regularization factor -> this time, set directly to final value in the second learning stage\n",
        "- bin_rate_ph : binarization rate -> this time, set directly to final value in the third learning stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "esF3dbTRVVrN",
        "colab": {}
      },
      "source": [
        "INPUT_SCALING = 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aoH3RXM4VVrR",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kst02HVRVVrb",
        "outputId": "e3c30289-db82-4764-e298-b4f41336d33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "    \n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "    \n",
        "    _, current_loss = sess.run([opt_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  lr_ph: lr_feed,\n",
        "                                  reg_factor_ph: reg_factor_feed,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "  \n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 4.789047748247782\n",
            "Testing Acc.: 0.5875999987125397\n",
            "Epoch 2 completed, average training loss is 0.9418615859250228\n",
            "Testing Acc.: 0.790499997138977\n",
            "Epoch 3 completed, average training loss is 0.5781417363882064\n",
            "Testing Acc.: 0.8586000001430512\n",
            "Epoch 4 completed, average training loss is 0.42854555959502855\n",
            "Testing Acc.: 0.877299998998642\n",
            "Epoch 5 completed, average training loss is 0.3445007232328256\n",
            "Testing Acc.: 0.9051000010967255\n",
            "Epoch 6 completed, average training loss is 0.2994749060521523\n",
            "Testing Acc.: 0.9102000004053116\n",
            "Epoch 7 completed, average training loss is 0.2665181989967823\n",
            "Testing Acc.: 0.9136999982595444\n",
            "Epoch 8 completed, average training loss is 0.2467475297798713\n",
            "Testing Acc.: 0.9295000052452087\n",
            "Epoch 9 completed, average training loss is 0.2258262018300593\n",
            "Testing Acc.: 0.9327000039815903\n",
            "Epoch 10 completed, average training loss is 0.210019299890846\n",
            "Testing Acc.: 0.9399000042676926\n",
            "Epoch 11 completed, average training loss is 0.20710691755016644\n",
            "Testing Acc.: 0.9311000037193299\n",
            "Epoch 12 completed, average training loss is 0.1996662193723023\n",
            "Testing Acc.: 0.94430000603199\n",
            "Epoch 13 completed, average training loss is 0.1854267198033631\n",
            "Testing Acc.: 0.9382000035047531\n",
            "Epoch 14 completed, average training loss is 0.18868830566604933\n",
            "Testing Acc.: 0.948100003004074\n",
            "Epoch 15 completed, average training loss is 0.1809264217255016\n",
            "Testing Acc.: 0.9432000035047531\n",
            "Epoch 16 completed, average training loss is 0.1792292670533061\n",
            "Testing Acc.: 0.9470000040531158\n",
            "Epoch 17 completed, average training loss is 0.17226228814572095\n",
            "Testing Acc.: 0.948100003004074\n",
            "Epoch 18 completed, average training loss is 0.17068686578422784\n",
            "Testing Acc.: 0.9407000070810319\n",
            "Epoch 19 completed, average training loss is 0.16584835262348255\n",
            "Testing Acc.: 0.9531000047922135\n",
            "Epoch 20 completed, average training loss is 0.16536081698102256\n",
            "Testing Acc.: 0.9494000011682511\n",
            "Epoch 21 completed, average training loss is 0.16095721032470464\n",
            "Testing Acc.: 0.9466000044345856\n",
            "Epoch 22 completed, average training loss is 0.160023223956426\n",
            "Testing Acc.: 0.9527000015974045\n",
            "Epoch 23 completed, average training loss is 0.156005498940746\n",
            "Testing Acc.: 0.9567000031471252\n",
            "Epoch 24 completed, average training loss is 0.15455347225690882\n",
            "Testing Acc.: 0.9565000081062317\n",
            "Epoch 25 completed, average training loss is 0.15016548292400936\n",
            "Testing Acc.: 0.9450000053644181\n",
            "Epoch 26 completed, average training loss is 0.15180951833104095\n",
            "Testing Acc.: 0.9578000044822693\n",
            "Epoch 27 completed, average training loss is 0.14877487789529065\n",
            "Testing Acc.: 0.9586000019311904\n",
            "Epoch 28 completed, average training loss is 0.14572860304266214\n",
            "Testing Acc.: 0.9558000051975251\n",
            "Epoch 29 completed, average training loss is 0.1436862632414947\n",
            "Testing Acc.: 0.9589000076055527\n",
            "Epoch 30 completed, average training loss is 0.14198589258516828\n",
            "Testing Acc.: 0.9587000024318695\n",
            "Epoch 31 completed, average training loss is 0.13823803322700162\n",
            "Testing Acc.: 0.9578000044822693\n",
            "Epoch 32 completed, average training loss is 0.14051295579100648\n",
            "Testing Acc.: 0.9602000045776368\n",
            "Epoch 33 completed, average training loss is 0.1374011279332141\n",
            "Testing Acc.: 0.9583000028133393\n",
            "Epoch 34 completed, average training loss is 0.13456602193259945\n",
            "Testing Acc.: 0.9593000030517578\n",
            "Epoch 35 completed, average training loss is 0.1325336055799077\n",
            "Testing Acc.: 0.9541000026464462\n",
            "Epoch 36 completed, average training loss is 0.13380830066899457\n",
            "Testing Acc.: 0.9596000051498413\n",
            "Epoch 37 completed, average training loss is 0.1323178613667066\n",
            "Testing Acc.: 0.9571000045537948\n",
            "Epoch 38 completed, average training loss is 0.13176048075780272\n",
            "Testing Acc.: 0.9575000029802322\n",
            "Epoch 39 completed, average training loss is 0.13195387812020878\n",
            "Testing Acc.: 0.96030000269413\n",
            "Epoch 40 completed, average training loss is 0.13049439778085797\n",
            "Testing Acc.: 0.9597000032663345\n",
            "Epoch 41 completed, average training loss is 0.12931022486959895\n",
            "Testing Acc.: 0.9604000037908554\n",
            "Epoch 42 completed, average training loss is 0.13364504427028198\n",
            "Testing Acc.: 0.963900004029274\n",
            "Epoch 43 completed, average training loss is 0.12615813123683134\n",
            "Testing Acc.: 0.9617000019550324\n",
            "Epoch 44 completed, average training loss is 0.1241447834049662\n",
            "Testing Acc.: 0.9525000029802322\n",
            "Epoch 45 completed, average training loss is 0.12583226521809895\n",
            "Testing Acc.: 0.9527000033855438\n",
            "Epoch 46 completed, average training loss is 0.12389849729215105\n",
            "Testing Acc.: 0.9564000022411346\n",
            "Epoch 47 completed, average training loss is 0.12361328984300296\n",
            "Testing Acc.: 0.9609000033140183\n",
            "Epoch 48 completed, average training loss is 0.12318903326056897\n",
            "Testing Acc.: 0.9559000033140183\n",
            "Epoch 49 completed, average training loss is 0.12257926413789391\n",
            "Testing Acc.: 0.9610000044107437\n",
            "Epoch 50 completed, average training loss is 0.12448502907529474\n",
            "Testing Acc.: 0.9606000030040741\n",
            "Epoch 51 completed, average training loss is 0.12002948929245273\n",
            "Testing Acc.: 0.96530000269413\n",
            "Epoch 52 completed, average training loss is 0.12298109457672884\n",
            "Testing Acc.: 0.9603000038862228\n",
            "Epoch 53 completed, average training loss is 0.12456216849076251\n",
            "Testing Acc.: 0.9574000030755997\n",
            "Epoch 54 completed, average training loss is 0.12133425418908397\n",
            "Testing Acc.: 0.9632000029087067\n",
            "Epoch 55 completed, average training loss is 0.1208090143216153\n",
            "Testing Acc.: 0.9594000053405761\n",
            "Epoch 56 completed, average training loss is 0.12753591122105717\n",
            "Testing Acc.: 0.9589000052213669\n",
            "Epoch 57 completed, average training loss is 0.12353921618002156\n",
            "Testing Acc.: 0.963400005698204\n",
            "Epoch 58 completed, average training loss is 0.11925257981133958\n",
            "Testing Acc.: 0.9594000035524368\n",
            "Epoch 59 completed, average training loss is 0.11997660707371931\n",
            "Testing Acc.: 0.945600004196167\n",
            "Epoch 60 completed, average training loss is 0.11473317690193653\n",
            "Testing Acc.: 0.9621000039577484\n",
            "Epoch 61 completed, average training loss is 0.11823763781227171\n",
            "Testing Acc.: 0.9631000024080276\n",
            "Epoch 62 completed, average training loss is 0.11677897141315043\n",
            "Testing Acc.: 0.9657000058889389\n",
            "Epoch 63 completed, average training loss is 0.11958962396718562\n",
            "Testing Acc.: 0.9663000047206879\n",
            "Epoch 64 completed, average training loss is 0.11682923428714276\n",
            "Testing Acc.: 0.967700006365776\n",
            "Epoch 65 completed, average training loss is 0.1248573680749784\n",
            "Testing Acc.: 0.9588000053167343\n",
            "Epoch 66 completed, average training loss is 0.11846766457892954\n",
            "Testing Acc.: 0.9611000049114228\n",
            "Epoch 67 completed, average training loss is 0.11474243157853682\n",
            "Testing Acc.: 0.9634000033140182\n",
            "Epoch 68 completed, average training loss is 0.11808820588203768\n",
            "Testing Acc.: 0.9646000057458878\n",
            "Epoch 69 completed, average training loss is 0.11534734079614281\n",
            "Testing Acc.: 0.9596000033617019\n",
            "Epoch 70 completed, average training loss is 0.11745463476516306\n",
            "Testing Acc.: 0.9616000044345856\n",
            "Epoch 71 completed, average training loss is 0.11486186017747968\n",
            "Testing Acc.: 0.961300003528595\n",
            "Epoch 72 completed, average training loss is 0.11712480427542081\n",
            "Testing Acc.: 0.9614000028371811\n",
            "Epoch 73 completed, average training loss is 0.12597824287911255\n",
            "Testing Acc.: 0.9634000045061112\n",
            "Epoch 74 completed, average training loss is 0.134770109259213\n",
            "Testing Acc.: 0.9556000059843064\n",
            "Epoch 75 completed, average training loss is 0.14037474416506787\n",
            "Testing Acc.: 0.9546000015735626\n",
            "Epoch 76 completed, average training loss is 0.1302815600267301\n",
            "Testing Acc.: 0.9532000029087067\n",
            "Epoch 77 completed, average training loss is 0.12463790426030755\n",
            "Testing Acc.: 0.9637000042200089\n",
            "Epoch 78 completed, average training loss is 0.12288070702614884\n",
            "Testing Acc.: 0.9622000044584275\n",
            "Epoch 79 completed, average training loss is 0.12351592311635613\n",
            "Testing Acc.: 0.9622000044584275\n",
            "Epoch 80 completed, average training loss is 0.12290130722336472\n",
            "Testing Acc.: 0.9608000087738037\n",
            "Epoch 81 completed, average training loss is 0.11905535868679484\n",
            "Testing Acc.: 0.9618000036478043\n",
            "Epoch 82 completed, average training loss is 0.11656449363256494\n",
            "Testing Acc.: 0.9627000033855438\n",
            "Epoch 83 completed, average training loss is 0.11838178109688063\n",
            "Testing Acc.: 0.9657000035047532\n",
            "Epoch 84 completed, average training loss is 0.11602083254295091\n",
            "Testing Acc.: 0.9674000060558319\n",
            "Epoch 85 completed, average training loss is 0.11673821519128978\n",
            "Testing Acc.: 0.9569000059366226\n",
            "Epoch 86 completed, average training loss is 0.11602844754699618\n",
            "Testing Acc.: 0.9597000008821488\n",
            "Epoch 87 completed, average training loss is 0.11221584717432657\n",
            "Testing Acc.: 0.9678000062704086\n",
            "Epoch 88 completed, average training loss is 0.11187682942021639\n",
            "Testing Acc.: 0.9660000026226043\n",
            "Epoch 89 completed, average training loss is 0.11548878484405577\n",
            "Testing Acc.: 0.9651000028848649\n",
            "Epoch 90 completed, average training loss is 0.11156764608031759\n",
            "Testing Acc.: 0.9655000019073486\n",
            "Epoch 91 completed, average training loss is 0.11366132836168011\n",
            "Testing Acc.: 0.9650000023841858\n",
            "Epoch 92 completed, average training loss is 0.11080220366517703\n",
            "Testing Acc.: 0.9629000049829483\n",
            "Epoch 93 completed, average training loss is 0.10856445063215991\n",
            "Testing Acc.: 0.967900003194809\n",
            "Epoch 94 completed, average training loss is 0.1085148393859466\n",
            "Testing Acc.: 0.9691000056266784\n",
            "Epoch 95 completed, average training loss is 0.10893217627890409\n",
            "Testing Acc.: 0.9644000065326691\n",
            "Epoch 96 completed, average training loss is 0.10804339642636478\n",
            "Testing Acc.: 0.9651000040769577\n",
            "Epoch 97 completed, average training loss is 0.10549327753018588\n",
            "Testing Acc.: 0.9682000035047531\n",
            "Epoch 98 completed, average training loss is 0.10904870735326161\n",
            "Testing Acc.: 0.9675000059604645\n",
            "Epoch 99 completed, average training loss is 0.1079195813431094\n",
            "Testing Acc.: 0.9681000030040741\n",
            "Epoch 100 completed, average training loss is 0.10569634039575855\n",
            "Testing Acc.: 0.9629000055789948\n",
            "Epoch 101 completed, average training loss is 0.10576902680099011\n",
            "Testing Acc.: 0.9693000048398972\n",
            "Epoch 102 completed, average training loss is 0.10465284841290365\n",
            "Testing Acc.: 0.9694000047445297\n",
            "Epoch 103 completed, average training loss is 0.10109176090142379\n",
            "Testing Acc.: 0.9651000052690506\n",
            "Epoch 104 completed, average training loss is 0.10435557649936528\n",
            "Testing Acc.: 0.9664000046253204\n",
            "Epoch 105 completed, average training loss is 0.10329215564454595\n",
            "Testing Acc.: 0.9676000034809112\n",
            "Epoch 106 completed, average training loss is 0.1023224877907584\n",
            "Testing Acc.: 0.9694000035524368\n",
            "Epoch 107 completed, average training loss is 0.10311699181019018\n",
            "Testing Acc.: 0.9691000080108643\n",
            "Epoch 108 completed, average training loss is 0.10243918071811398\n",
            "Testing Acc.: 0.9698000055551529\n",
            "Epoch 109 completed, average training loss is 0.10133985915531715\n",
            "Testing Acc.: 0.9696000075340271\n",
            "Epoch 110 completed, average training loss is 0.10294767805375159\n",
            "Testing Acc.: 0.9648000055551529\n",
            "Epoch 111 completed, average training loss is 0.1003523110008488\n",
            "Testing Acc.: 0.9651000034809113\n",
            "Epoch 112 completed, average training loss is 0.10075964441833397\n",
            "Testing Acc.: 0.9699000036716461\n",
            "Epoch 113 completed, average training loss is 0.09775529745655755\n",
            "Testing Acc.: 0.9648000055551529\n",
            "Epoch 114 completed, average training loss is 0.1007380261225626\n",
            "Testing Acc.: 0.9659000033140183\n",
            "Epoch 115 completed, average training loss is 0.10009842751702915\n",
            "Testing Acc.: 0.9698000055551529\n",
            "Epoch 116 completed, average training loss is 0.10137105541458974\n",
            "Testing Acc.: 0.9668000048398971\n",
            "Epoch 117 completed, average training loss is 0.10032920429017395\n",
            "Testing Acc.: 0.9648000055551529\n",
            "Epoch 118 completed, average training loss is 0.10185752879828215\n",
            "Testing Acc.: 0.9661000025272369\n",
            "Epoch 119 completed, average training loss is 0.10193035898885379\n",
            "Testing Acc.: 0.9704000055789948\n",
            "Epoch 120 completed, average training loss is 0.10008211501253148\n",
            "Testing Acc.: 0.9651000046730042\n",
            "Epoch 121 completed, average training loss is 0.10174119657526413\n",
            "Testing Acc.: 0.9682000076770783\n",
            "Epoch 122 completed, average training loss is 0.09958253330706308\n",
            "Testing Acc.: 0.9673000037670135\n",
            "Epoch 123 completed, average training loss is 0.1007972907119741\n",
            "Testing Acc.: 0.9645000040531159\n",
            "Epoch 124 completed, average training loss is 0.10235131090506912\n",
            "Testing Acc.: 0.9680000030994416\n",
            "Epoch 125 completed, average training loss is 0.10037398329470307\n",
            "Testing Acc.: 0.962700002193451\n",
            "Epoch 126 completed, average training loss is 0.09940468286940207\n",
            "Testing Acc.: 0.9706000065803528\n",
            "Epoch 127 completed, average training loss is 0.10044650169089436\n",
            "Testing Acc.: 0.9701000052690506\n",
            "Epoch 128 completed, average training loss is 0.09746647907731433\n",
            "Testing Acc.: 0.9704000049829483\n",
            "Epoch 129 completed, average training loss is 0.09886117408828189\n",
            "Testing Acc.: 0.9697000050544738\n",
            "Epoch 130 completed, average training loss is 0.09898651597710947\n",
            "Testing Acc.: 0.9668000030517578\n",
            "Epoch 131 completed, average training loss is 0.09929908922562997\n",
            "Testing Acc.: 0.9701000028848648\n",
            "Epoch 132 completed, average training loss is 0.10551607209413003\n",
            "Testing Acc.: 0.9702000057697296\n",
            "Epoch 133 completed, average training loss is 0.10483280919026583\n",
            "Testing Acc.: 0.966400004029274\n",
            "Epoch 134 completed, average training loss is 0.10157281639830519\n",
            "Testing Acc.: 0.9676000040769577\n",
            "Epoch 135 completed, average training loss is 0.1018599289888516\n",
            "Testing Acc.: 0.969500007033348\n",
            "Epoch 136 completed, average training loss is 0.10099992794450373\n",
            "Testing Acc.: 0.9687000036239624\n",
            "Epoch 137 completed, average training loss is 0.10068904686098297\n",
            "Testing Acc.: 0.9727000081539154\n",
            "Epoch 138 completed, average training loss is 0.10215387947236498\n",
            "Testing Acc.: 0.9659000033140183\n",
            "Epoch 139 completed, average training loss is 0.09860344017079721\n",
            "Testing Acc.: 0.965400002002716\n",
            "Epoch 140 completed, average training loss is 0.09881423612280438\n",
            "Testing Acc.: 0.9624000054597854\n",
            "Epoch 141 completed, average training loss is 0.09985144222698485\n",
            "Testing Acc.: 0.965600004196167\n",
            "Epoch 142 completed, average training loss is 0.10027257137155782\n",
            "Testing Acc.: 0.9698000049591065\n",
            "Epoch 143 completed, average training loss is 0.09713461784025033\n",
            "Testing Acc.: 0.9626000022888184\n",
            "Epoch 144 completed, average training loss is 0.0987355167263498\n",
            "Testing Acc.: 0.97260000705719\n",
            "Epoch 145 completed, average training loss is 0.09750552274907628\n",
            "Testing Acc.: 0.9720000058412552\n",
            "Epoch 146 completed, average training loss is 0.09681263417781641\n",
            "Testing Acc.: 0.9689000046253204\n",
            "Epoch 147 completed, average training loss is 0.09678273142160226\n",
            "Testing Acc.: 0.9699000054597855\n",
            "Epoch 148 completed, average training loss is 0.09499499705309669\n",
            "Testing Acc.: 0.970700004696846\n",
            "Epoch 149 completed, average training loss is 0.0965439411966751\n",
            "Testing Acc.: 0.9643000054359436\n",
            "Epoch 150 completed, average training loss is 0.09397186994707833\n",
            "Testing Acc.: 0.9711000066995621\n",
            "Epoch 151 completed, average training loss is 0.09344789840591451\n",
            "Testing Acc.: 0.9723000037670135\n",
            "Epoch 152 completed, average training loss is 0.09326203435193747\n",
            "Testing Acc.: 0.9720000058412552\n",
            "Epoch 153 completed, average training loss is 0.09540634956055631\n",
            "Testing Acc.: 0.9671000039577484\n",
            "Epoch 154 completed, average training loss is 0.09635368027413885\n",
            "Testing Acc.: 0.96760000705719\n",
            "Epoch 155 completed, average training loss is 0.09493645197401444\n",
            "Testing Acc.: 0.9685000056028366\n",
            "Epoch 156 completed, average training loss is 0.09596966963727027\n",
            "Testing Acc.: 0.9617000007629395\n",
            "Epoch 157 completed, average training loss is 0.09657407712346563\n",
            "Testing Acc.: 0.9694000065326691\n",
            "Epoch 158 completed, average training loss is 0.09437725058911989\n",
            "Testing Acc.: 0.9611000019311905\n",
            "Epoch 159 completed, average training loss is 0.09447269657781969\n",
            "Testing Acc.: 0.9678000038862229\n",
            "Epoch 160 completed, average training loss is 0.09541360011013846\n",
            "Testing Acc.: 0.9681000059843063\n",
            "Epoch 161 completed, average training loss is 0.09313791600987315\n",
            "Testing Acc.: 0.9715000081062317\n",
            "Epoch 162 completed, average training loss is 0.09426523718206833\n",
            "Testing Acc.: 0.9699000036716461\n",
            "Epoch 163 completed, average training loss is 0.09340942380018533\n",
            "Testing Acc.: 0.9693000072240829\n",
            "Epoch 164 completed, average training loss is 0.09312253115931526\n",
            "Testing Acc.: 0.9689000070095062\n",
            "Epoch 165 completed, average training loss is 0.0934190449056526\n",
            "Testing Acc.: 0.9732000035047531\n",
            "Epoch 166 completed, average training loss is 0.09361163266158352\n",
            "Testing Acc.: 0.9678000044822693\n",
            "Epoch 167 completed, average training loss is 0.0937522763805464\n",
            "Testing Acc.: 0.9662000060081481\n",
            "Epoch 168 completed, average training loss is 0.09315179968407998\n",
            "Testing Acc.: 0.9715000075101853\n",
            "Epoch 169 completed, average training loss is 0.0931972892768681\n",
            "Testing Acc.: 0.9720000046491623\n",
            "Epoch 170 completed, average training loss is 0.09430231626300763\n",
            "Testing Acc.: 0.9693000060319901\n",
            "Epoch 171 completed, average training loss is 0.09248476359061897\n",
            "Testing Acc.: 0.9697000074386597\n",
            "Epoch 172 completed, average training loss is 0.0939572075707838\n",
            "Testing Acc.: 0.9733000040054322\n",
            "Epoch 173 completed, average training loss is 0.09556994147598744\n",
            "Testing Acc.: 0.9684000045061112\n",
            "Epoch 174 completed, average training loss is 0.09568432029491912\n",
            "Testing Acc.: 0.9679000049829483\n",
            "Epoch 175 completed, average training loss is 0.09182064407194654\n",
            "Testing Acc.: 0.9721000051498413\n",
            "Epoch 176 completed, average training loss is 0.09318535362059871\n",
            "Testing Acc.: 0.9726000046730041\n",
            "Epoch 177 completed, average training loss is 0.09138818766766538\n",
            "Testing Acc.: 0.9742000073194503\n",
            "Epoch 178 completed, average training loss is 0.09193795203541716\n",
            "Testing Acc.: 0.97180000603199\n",
            "Epoch 179 completed, average training loss is 0.09260120228320981\n",
            "Testing Acc.: 0.9697000026702881\n",
            "Epoch 180 completed, average training loss is 0.09536998070466021\n",
            "Testing Acc.: 0.9672000032663345\n",
            "Epoch 181 completed, average training loss is 0.09334741448517889\n",
            "Testing Acc.: 0.9674000054597854\n",
            "Epoch 182 completed, average training loss is 0.09600665588242312\n",
            "Testing Acc.: 0.9681000030040741\n",
            "Epoch 183 completed, average training loss is 0.09355066242472579\n",
            "Testing Acc.: 0.9699000030755996\n",
            "Epoch 184 completed, average training loss is 0.09592637873254717\n",
            "Testing Acc.: 0.9690000051259995\n",
            "Epoch 185 completed, average training loss is 0.09380814538802952\n",
            "Testing Acc.: 0.9697000050544738\n",
            "Epoch 186 completed, average training loss is 0.09510447844086836\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 187 completed, average training loss is 0.09505054693824301\n",
            "Testing Acc.: 0.9712000060081482\n",
            "Epoch 188 completed, average training loss is 0.09216938843484968\n",
            "Testing Acc.: 0.9709000051021576\n",
            "Epoch 189 completed, average training loss is 0.09177457634359598\n",
            "Testing Acc.: 0.9675000035762786\n",
            "Epoch 190 completed, average training loss is 0.0929538658928747\n",
            "Testing Acc.: 0.9681000047922135\n",
            "Epoch 191 completed, average training loss is 0.09096207692287862\n",
            "Testing Acc.: 0.9706000036001206\n",
            "Epoch 192 completed, average training loss is 0.09301708650076762\n",
            "Testing Acc.: 0.9686000031232834\n",
            "Epoch 193 completed, average training loss is 0.09193475922259192\n",
            "Testing Acc.: 0.9698000055551529\n",
            "Epoch 194 completed, average training loss is 0.09173466446809471\n",
            "Testing Acc.: 0.9678000020980835\n",
            "Epoch 195 completed, average training loss is 0.08981066586294521\n",
            "Testing Acc.: 0.9706000053882599\n",
            "Epoch 196 completed, average training loss is 0.09086563964995245\n",
            "Testing Acc.: 0.9643000012636185\n",
            "Epoch 197 completed, average training loss is 0.09235125651583076\n",
            "Testing Acc.: 0.9700000047683716\n",
            "Epoch 198 completed, average training loss is 0.0921641507682701\n",
            "Testing Acc.: 0.968200004696846\n",
            "Epoch 199 completed, average training loss is 0.09226413608063012\n",
            "Testing Acc.: 0.9671000051498413\n",
            "Epoch 200 completed, average training loss is 0.09256981416915853\n",
            "Testing Acc.: 0.9703000038862228\n",
            "Epoch 201 completed, average training loss is 0.12861774436819057\n",
            "Testing Acc.: 0.9600000035762787\n",
            "Epoch 202 completed, average training loss is 0.12316782891439895\n",
            "Testing Acc.: 0.9622000032663345\n",
            "Epoch 203 completed, average training loss is 0.12125131150086721\n",
            "Testing Acc.: 0.961300003528595\n",
            "Epoch 204 completed, average training loss is 0.12038896090040604\n",
            "Testing Acc.: 0.9620000052452088\n",
            "Epoch 205 completed, average training loss is 0.11701689640060067\n",
            "Testing Acc.: 0.9585000020265579\n",
            "Epoch 206 completed, average training loss is 0.11703737725193301\n",
            "Testing Acc.: 0.9641000056266784\n",
            "Epoch 207 completed, average training loss is 0.1167160227863739\n",
            "Testing Acc.: 0.9615000027418137\n",
            "Epoch 208 completed, average training loss is 0.11520618284897258\n",
            "Testing Acc.: 0.9651000064611435\n",
            "Epoch 209 completed, average training loss is 0.11490116387605667\n",
            "Testing Acc.: 0.961200003027916\n",
            "Epoch 210 completed, average training loss is 0.11465753856115043\n",
            "Testing Acc.: 0.9636000055074692\n",
            "Epoch 211 completed, average training loss is 0.11429792111118635\n",
            "Testing Acc.: 0.9608000034093857\n",
            "Epoch 212 completed, average training loss is 0.11263236436682443\n",
            "Testing Acc.: 0.9644000035524368\n",
            "Epoch 213 completed, average training loss is 0.11306790387878815\n",
            "Testing Acc.: 0.9638000029325485\n",
            "Epoch 214 completed, average training loss is 0.11193251534054677\n",
            "Testing Acc.: 0.9634000033140182\n",
            "Epoch 215 completed, average training loss is 0.11121150936620931\n",
            "Testing Acc.: 0.960200001001358\n",
            "Epoch 216 completed, average training loss is 0.11078325004316866\n",
            "Testing Acc.: 0.9621000045537949\n",
            "Epoch 217 completed, average training loss is 0.11227579230442643\n",
            "Testing Acc.: 0.963900004029274\n",
            "Epoch 218 completed, average training loss is 0.11177640218753368\n",
            "Testing Acc.: 0.9652000057697296\n",
            "Epoch 219 completed, average training loss is 0.1100036422132204\n",
            "Testing Acc.: 0.9660000020265579\n",
            "Epoch 220 completed, average training loss is 0.10996411204338073\n",
            "Testing Acc.: 0.9639000046253204\n",
            "Epoch 221 completed, average training loss is 0.10976920504899075\n",
            "Testing Acc.: 0.966400004029274\n",
            "Epoch 222 completed, average training loss is 0.10923116920826335\n",
            "Testing Acc.: 0.963600002527237\n",
            "Epoch 223 completed, average training loss is 0.10914714811524998\n",
            "Testing Acc.: 0.9639000028371811\n",
            "Epoch 224 completed, average training loss is 0.11034998034437497\n",
            "Testing Acc.: 0.9637000012397766\n",
            "Epoch 225 completed, average training loss is 0.10877924667671322\n",
            "Testing Acc.: 0.9639000022411346\n",
            "Epoch 226 completed, average training loss is 0.10811094474357863\n",
            "Testing Acc.: 0.9656000030040741\n",
            "Epoch 227 completed, average training loss is 0.10860496695153415\n",
            "Testing Acc.: 0.9655000042915344\n",
            "Epoch 228 completed, average training loss is 0.10677575269093116\n",
            "Testing Acc.: 0.9644000041484833\n",
            "Epoch 229 completed, average training loss is 0.10909224513297279\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 230 completed, average training loss is 0.10807608858371774\n",
            "Testing Acc.: 0.9652000033855438\n",
            "Epoch 231 completed, average training loss is 0.10730502953131994\n",
            "Testing Acc.: 0.9660000056028366\n",
            "Epoch 232 completed, average training loss is 0.10819512414125104\n",
            "Testing Acc.: 0.9622000044584275\n",
            "Epoch 233 completed, average training loss is 0.10667690878268331\n",
            "Testing Acc.: 0.9671000063419342\n",
            "Epoch 234 completed, average training loss is 0.1071793176031982\n",
            "Testing Acc.: 0.9656000047922134\n",
            "Epoch 235 completed, average training loss is 0.10785287640367945\n",
            "Testing Acc.: 0.9624000024795533\n",
            "Epoch 236 completed, average training loss is 0.10742853212015083\n",
            "Testing Acc.: 0.9647000044584274\n",
            "Epoch 237 completed, average training loss is 0.10697407332248986\n",
            "Testing Acc.: 0.9671000069379807\n",
            "Epoch 238 completed, average training loss is 0.1078646036113302\n",
            "Testing Acc.: 0.9634000045061112\n",
            "Epoch 239 completed, average training loss is 0.10620788105142613\n",
            "Testing Acc.: 0.9637000012397766\n",
            "Epoch 240 completed, average training loss is 0.10583682700215528\n",
            "Testing Acc.: 0.9638000041246414\n",
            "Epoch 241 completed, average training loss is 0.10478348186705261\n",
            "Testing Acc.: 0.9644000011682511\n",
            "Epoch 242 completed, average training loss is 0.10698306417713563\n",
            "Testing Acc.: 0.9668000048398971\n",
            "Epoch 243 completed, average training loss is 0.10615948094365497\n",
            "Testing Acc.: 0.9653000044822693\n",
            "Epoch 244 completed, average training loss is 0.10595002232740323\n",
            "Testing Acc.: 0.9639000028371811\n",
            "Epoch 245 completed, average training loss is 0.10704366373829544\n",
            "Testing Acc.: 0.9656000036001205\n",
            "Epoch 246 completed, average training loss is 0.10668378345357875\n",
            "Testing Acc.: 0.9642000043392182\n",
            "Epoch 247 completed, average training loss is 0.10560205201928814\n",
            "Testing Acc.: 0.9640000039339065\n",
            "Epoch 248 completed, average training loss is 0.10530620199317733\n",
            "Testing Acc.: 0.9643000060319901\n",
            "Epoch 249 completed, average training loss is 0.10674124748290827\n",
            "Testing Acc.: 0.9670000016689301\n",
            "Epoch 250 completed, average training loss is 0.10537549930935104\n",
            "Testing Acc.: 0.965700004696846\n",
            "Epoch 251 completed, average training loss is 0.10578258731557677\n",
            "Testing Acc.: 0.9658000063896179\n",
            "Epoch 252 completed, average training loss is 0.1048395066537584\n",
            "Testing Acc.: 0.9658000028133392\n",
            "Epoch 253 completed, average training loss is 0.10613396418901781\n",
            "Testing Acc.: 0.9658000028133392\n",
            "Epoch 254 completed, average training loss is 0.10431963850278407\n",
            "Testing Acc.: 0.9663000029325485\n",
            "Epoch 255 completed, average training loss is 0.10399466382029156\n",
            "Testing Acc.: 0.9651000028848649\n",
            "Epoch 256 completed, average training loss is 0.1051892725021268\n",
            "Testing Acc.: 0.9675000047683716\n",
            "Epoch 257 completed, average training loss is 0.1041038968662421\n",
            "Testing Acc.: 0.9651000076532363\n",
            "Epoch 258 completed, average training loss is 0.10426514021276186\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 259 completed, average training loss is 0.10617790165667733\n",
            "Testing Acc.: 0.9671000039577484\n",
            "Epoch 260 completed, average training loss is 0.10524043333251029\n",
            "Testing Acc.: 0.9657000017166137\n",
            "Epoch 261 completed, average training loss is 0.10412184057136377\n",
            "Testing Acc.: 0.9650000029802323\n",
            "Epoch 262 completed, average training loss is 0.10390756019468729\n",
            "Testing Acc.: 0.966000006198883\n",
            "Epoch 263 completed, average training loss is 0.10513051442646731\n",
            "Testing Acc.: 0.965600004196167\n",
            "Epoch 264 completed, average training loss is 0.1035881024816384\n",
            "Testing Acc.: 0.967200003862381\n",
            "Epoch 265 completed, average training loss is 0.1038849916215986\n",
            "Testing Acc.: 0.9655000066757202\n",
            "Epoch 266 completed, average training loss is 0.10303913027824213\n",
            "Testing Acc.: 0.9646000039577484\n",
            "Epoch 267 completed, average training loss is 0.10349060758327444\n",
            "Testing Acc.: 0.9683000034093857\n",
            "Epoch 268 completed, average training loss is 0.10427022390067578\n",
            "Testing Acc.: 0.9666000032424926\n",
            "Epoch 269 completed, average training loss is 0.10356344335402051\n",
            "Testing Acc.: 0.9663000041246415\n",
            "Epoch 270 completed, average training loss is 0.10409228390548378\n",
            "Testing Acc.: 0.9660000050067902\n",
            "Epoch 271 completed, average training loss is 0.10337365834818532\n",
            "Testing Acc.: 0.9617000049352646\n",
            "Epoch 272 completed, average training loss is 0.10412675911560655\n",
            "Testing Acc.: 0.9650000023841858\n",
            "Epoch 273 completed, average training loss is 0.10314469905259709\n",
            "Testing Acc.: 0.9663000047206879\n",
            "Epoch 274 completed, average training loss is 0.10236680003193517\n",
            "Testing Acc.: 0.9660000044107437\n",
            "Epoch 275 completed, average training loss is 0.10373169594754775\n",
            "Testing Acc.: 0.9661000031232834\n",
            "Epoch 276 completed, average training loss is 0.10388408415485173\n",
            "Testing Acc.: 0.9639000052213669\n",
            "Epoch 277 completed, average training loss is 0.10287574202753604\n",
            "Testing Acc.: 0.9669000059366226\n",
            "Epoch 278 completed, average training loss is 0.1027994738938287\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 279 completed, average training loss is 0.10268111251294613\n",
            "Testing Acc.: 0.9648000037670136\n",
            "Epoch 280 completed, average training loss is 0.10271133373336246\n",
            "Testing Acc.: 0.9659000045061111\n",
            "Epoch 281 completed, average training loss is 0.10274982615529249\n",
            "Testing Acc.: 0.9661000007390976\n",
            "Epoch 282 completed, average training loss is 0.10246923234934609\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 283 completed, average training loss is 0.10184044777881354\n",
            "Testing Acc.: 0.967100003361702\n",
            "Epoch 284 completed, average training loss is 0.10208312992937862\n",
            "Testing Acc.: 0.9672000044584275\n",
            "Epoch 285 completed, average training loss is 0.10271236831787973\n",
            "Testing Acc.: 0.9669000059366226\n",
            "Epoch 286 completed, average training loss is 0.10068854650016874\n",
            "Testing Acc.: 0.9669000047445298\n",
            "Epoch 287 completed, average training loss is 0.10260801588029911\n",
            "Testing Acc.: 0.9661000049114228\n",
            "Epoch 288 completed, average training loss is 0.10256563040272643\n",
            "Testing Acc.: 0.9671000045537949\n",
            "Epoch 289 completed, average training loss is 0.10205183883508047\n",
            "Testing Acc.: 0.9650000029802323\n",
            "Epoch 290 completed, average training loss is 0.10097759892077496\n",
            "Testing Acc.: 0.9655000066757202\n",
            "Epoch 291 completed, average training loss is 0.10193949218684187\n",
            "Testing Acc.: 0.9688000047206878\n",
            "Epoch 292 completed, average training loss is 0.102158539534236\n",
            "Testing Acc.: 0.9654000061750412\n",
            "Epoch 293 completed, average training loss is 0.10230041085121533\n",
            "Testing Acc.: 0.9673000073432922\n",
            "Epoch 294 completed, average training loss is 0.10181431038926045\n",
            "Testing Acc.: 0.9670000058412552\n",
            "Epoch 295 completed, average training loss is 0.1011783902036647\n",
            "Testing Acc.: 0.9660000032186509\n",
            "Epoch 296 completed, average training loss is 0.10163973613952597\n",
            "Testing Acc.: 0.9651000046730042\n",
            "Epoch 297 completed, average training loss is 0.10225846698197226\n",
            "Testing Acc.: 0.9641000038385391\n",
            "Epoch 298 completed, average training loss is 0.1013171883346513\n",
            "Testing Acc.: 0.9666000032424926\n",
            "Epoch 299 completed, average training loss is 0.10082550443553676\n",
            "Testing Acc.: 0.9664000022411346\n",
            "Epoch 300 completed, average training loss is 0.10118300475335369\n",
            "Testing Acc.: 0.9693000054359436\n",
            "Epoch 301 completed, average training loss is 0.1004941642144695\n",
            "Testing Acc.: 0.9662000036239624\n",
            "Epoch 302 completed, average training loss is 0.10105273488598565\n",
            "Testing Acc.: 0.9675000077486038\n",
            "Epoch 303 completed, average training loss is 0.10100186426037301\n",
            "Testing Acc.: 0.9686000072956085\n",
            "Epoch 304 completed, average training loss is 0.10295296067216744\n",
            "Testing Acc.: 0.9657000035047532\n",
            "Epoch 305 completed, average training loss is 0.10033542157926907\n",
            "Testing Acc.: 0.9670000046491622\n",
            "Epoch 306 completed, average training loss is 0.10192687092969815\n",
            "Testing Acc.: 0.9624000030755997\n",
            "Epoch 307 completed, average training loss is 0.10162749630554269\n",
            "Testing Acc.: 0.9676000064611435\n",
            "Epoch 308 completed, average training loss is 0.10018382883630693\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 309 completed, average training loss is 0.10234666499154021\n",
            "Testing Acc.: 0.965700004696846\n",
            "Epoch 310 completed, average training loss is 0.10069123324472457\n",
            "Testing Acc.: 0.965600004196167\n",
            "Epoch 311 completed, average training loss is 0.10028575690463186\n",
            "Testing Acc.: 0.9675000065565109\n",
            "Epoch 312 completed, average training loss is 0.10099659103124092\n",
            "Testing Acc.: 0.9641000056266784\n",
            "Epoch 313 completed, average training loss is 0.10113646799698472\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 314 completed, average training loss is 0.10166841995747139\n",
            "Testing Acc.: 0.96680000603199\n",
            "Epoch 315 completed, average training loss is 0.10114906544641902\n",
            "Testing Acc.: 0.9647000062465668\n",
            "Epoch 316 completed, average training loss is 0.1000547044724226\n",
            "Testing Acc.: 0.9689000064134597\n",
            "Epoch 317 completed, average training loss is 0.10097340597771108\n",
            "Testing Acc.: 0.9664000028371811\n",
            "Epoch 318 completed, average training loss is 0.0997349986853078\n",
            "Testing Acc.: 0.9641000026464462\n",
            "Epoch 319 completed, average training loss is 0.10045442345086485\n",
            "Testing Acc.: 0.9667000055313111\n",
            "Epoch 320 completed, average training loss is 0.09999450456040601\n",
            "Testing Acc.: 0.9674000066518783\n",
            "Epoch 321 completed, average training loss is 0.10107720281463116\n",
            "Testing Acc.: 0.967000002861023\n",
            "Epoch 322 completed, average training loss is 0.10086655299489697\n",
            "Testing Acc.: 0.9671000045537949\n",
            "Epoch 323 completed, average training loss is 0.09891676858688395\n",
            "Testing Acc.: 0.965000005364418\n",
            "Epoch 324 completed, average training loss is 0.10166040561472377\n",
            "Testing Acc.: 0.9646000063419342\n",
            "Epoch 325 completed, average training loss is 0.09971123010696223\n",
            "Testing Acc.: 0.9654000061750412\n",
            "Epoch 326 completed, average training loss is 0.10067099276153992\n",
            "Testing Acc.: 0.9664000046253204\n",
            "Epoch 327 completed, average training loss is 0.10007709922579427\n",
            "Testing Acc.: 0.9637000060081482\n",
            "Epoch 328 completed, average training loss is 0.10056257111020386\n",
            "Testing Acc.: 0.9670000016689301\n",
            "Epoch 329 completed, average training loss is 0.09884078258648515\n",
            "Testing Acc.: 0.9647000050544738\n",
            "Epoch 330 completed, average training loss is 0.10001484920736402\n",
            "Testing Acc.: 0.9665000033378601\n",
            "Epoch 331 completed, average training loss is 0.09981254760486384\n",
            "Testing Acc.: 0.9659000045061111\n",
            "Epoch 332 completed, average training loss is 0.09883889337846388\n",
            "Testing Acc.: 0.96980000436306\n",
            "Epoch 333 completed, average training loss is 0.09990428233984858\n",
            "Testing Acc.: 0.9651000052690506\n",
            "Epoch 334 completed, average training loss is 0.09881969373828421\n",
            "Testing Acc.: 0.9686000031232834\n",
            "Epoch 335 completed, average training loss is 0.09929850609662633\n",
            "Testing Acc.: 0.9677000033855438\n",
            "Epoch 336 completed, average training loss is 0.09944547503410528\n",
            "Testing Acc.: 0.9689000058174133\n",
            "Epoch 337 completed, average training loss is 0.0990726982584844\n",
            "Testing Acc.: 0.9680000048875809\n",
            "Epoch 338 completed, average training loss is 0.10092077698403348\n",
            "Testing Acc.: 0.9663000047206879\n",
            "Epoch 339 completed, average training loss is 0.09965972791425884\n",
            "Testing Acc.: 0.9664000052213669\n",
            "Epoch 340 completed, average training loss is 0.09869735476405671\n",
            "Testing Acc.: 0.9647000044584274\n",
            "Epoch 341 completed, average training loss is 0.09861273383566489\n",
            "Testing Acc.: 0.9690000063180924\n",
            "Epoch 342 completed, average training loss is 0.09839068678828577\n",
            "Testing Acc.: 0.9696000057458878\n",
            "Epoch 343 completed, average training loss is 0.09965613412437961\n",
            "Testing Acc.: 0.9672000056505203\n",
            "Epoch 344 completed, average training loss is 0.09889432757394388\n",
            "Testing Acc.: 0.9660000073909759\n",
            "Epoch 345 completed, average training loss is 0.09985781095921993\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 346 completed, average training loss is 0.09881567311628411\n",
            "Testing Acc.: 0.9652000057697296\n",
            "Epoch 347 completed, average training loss is 0.0983731272816658\n",
            "Testing Acc.: 0.9690000021457672\n",
            "Epoch 348 completed, average training loss is 0.1003337137013053\n",
            "Testing Acc.: 0.9686000061035156\n",
            "Epoch 349 completed, average training loss is 0.09857310167048126\n",
            "Testing Acc.: 0.9674000024795533\n",
            "Epoch 350 completed, average training loss is 0.09894664462888614\n",
            "Testing Acc.: 0.9693000048398972\n",
            "Epoch 351 completed, average training loss is 0.09955169381108135\n",
            "Testing Acc.: 0.9681000047922135\n",
            "Epoch 352 completed, average training loss is 0.09883065026719123\n",
            "Testing Acc.: 0.9689000058174133\n",
            "Epoch 353 completed, average training loss is 0.09931797898684938\n",
            "Testing Acc.: 0.9679000061750412\n",
            "Epoch 354 completed, average training loss is 0.09888092755029598\n",
            "Testing Acc.: 0.9684000080823898\n",
            "Epoch 355 completed, average training loss is 0.09933511077426374\n",
            "Testing Acc.: 0.9650000059604644\n",
            "Epoch 356 completed, average training loss is 0.09897798841353506\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 357 completed, average training loss is 0.0987481026025489\n",
            "Testing Acc.: 0.9657000052928925\n",
            "Epoch 358 completed, average training loss is 0.09833191312849522\n",
            "Testing Acc.: 0.9681000053882599\n",
            "Epoch 359 completed, average training loss is 0.09784388758707792\n",
            "Testing Acc.: 0.9688000065088272\n",
            "Epoch 360 completed, average training loss is 0.10016450250676523\n",
            "Testing Acc.: 0.9663000053167343\n",
            "Epoch 361 completed, average training loss is 0.09918155630740026\n",
            "Testing Acc.: 0.9655000036954879\n",
            "Epoch 362 completed, average training loss is 0.0983053775007526\n",
            "Testing Acc.: 0.9654000049829483\n",
            "Epoch 363 completed, average training loss is 0.09806315823458135\n",
            "Testing Acc.: 0.969100006222725\n",
            "Epoch 364 completed, average training loss is 0.09787020491125684\n",
            "Testing Acc.: 0.9653000032901764\n",
            "Epoch 365 completed, average training loss is 0.09874500961508602\n",
            "Testing Acc.: 0.9662000060081481\n",
            "Epoch 366 completed, average training loss is 0.09883606028122206\n",
            "Testing Acc.: 0.9670000046491622\n",
            "Epoch 367 completed, average training loss is 0.09922358693710218\n",
            "Testing Acc.: 0.9681000053882599\n",
            "Epoch 368 completed, average training loss is 0.0984868910924221\n",
            "Testing Acc.: 0.9679000002145767\n",
            "Epoch 369 completed, average training loss is 0.09627913217215488\n",
            "Testing Acc.: 0.9678000050783158\n",
            "Epoch 370 completed, average training loss is 0.09808688403883328\n",
            "Testing Acc.: 0.9675000035762786\n",
            "Epoch 371 completed, average training loss is 0.09826610789323846\n",
            "Testing Acc.: 0.9656000047922134\n",
            "Epoch 372 completed, average training loss is 0.09754581269963335\n",
            "Testing Acc.: 0.9666000038385392\n",
            "Epoch 373 completed, average training loss is 0.0980434960188965\n",
            "Testing Acc.: 0.9670000046491622\n",
            "Epoch 374 completed, average training loss is 0.09825825848306219\n",
            "Testing Acc.: 0.9694000065326691\n",
            "Epoch 375 completed, average training loss is 0.09839936923235655\n",
            "Testing Acc.: 0.968300005197525\n",
            "Epoch 376 completed, average training loss is 0.09808148672028134\n",
            "Testing Acc.: 0.9693000042438507\n",
            "Epoch 377 completed, average training loss is 0.09873500360175967\n",
            "Testing Acc.: 0.9687000054121018\n",
            "Epoch 378 completed, average training loss is 0.09813184554145361\n",
            "Testing Acc.: 0.9681000030040741\n",
            "Epoch 379 completed, average training loss is 0.09792162821938594\n",
            "Testing Acc.: 0.9695000046491623\n",
            "Epoch 380 completed, average training loss is 0.09891981445563336\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 381 completed, average training loss is 0.09761444639026498\n",
            "Testing Acc.: 0.9694000053405761\n",
            "Epoch 382 completed, average training loss is 0.0986723562888801\n",
            "Testing Acc.: 0.9668000042438507\n",
            "Epoch 383 completed, average training loss is 0.09807869302729766\n",
            "Testing Acc.: 0.9626000028848648\n",
            "Epoch 384 completed, average training loss is 0.09711545129617055\n",
            "Testing Acc.: 0.9673000055551529\n",
            "Epoch 385 completed, average training loss is 0.09717290531067799\n",
            "Testing Acc.: 0.9694000059366226\n",
            "Epoch 386 completed, average training loss is 0.09708322258200497\n",
            "Testing Acc.: 0.9678000050783158\n",
            "Epoch 387 completed, average training loss is 0.09746533251833171\n",
            "Testing Acc.: 0.9659000045061111\n",
            "Epoch 388 completed, average training loss is 0.09763551330504318\n",
            "Testing Acc.: 0.9694000059366226\n",
            "Epoch 389 completed, average training loss is 0.09735793656669557\n",
            "Testing Acc.: 0.967400004863739\n",
            "Epoch 390 completed, average training loss is 0.0972986434570824\n",
            "Testing Acc.: 0.9709000074863434\n",
            "Epoch 391 completed, average training loss is 0.09765633747757722\n",
            "Testing Acc.: 0.9643000054359436\n",
            "Epoch 392 completed, average training loss is 0.09699917391718675\n",
            "Testing Acc.: 0.9661000049114228\n",
            "Epoch 393 completed, average training loss is 0.0967607001028955\n",
            "Testing Acc.: 0.9696000039577484\n",
            "Epoch 394 completed, average training loss is 0.09682147100102156\n",
            "Testing Acc.: 0.9669000041484833\n",
            "Epoch 395 completed, average training loss is 0.09804012769212325\n",
            "Testing Acc.: 0.966200003027916\n",
            "Epoch 396 completed, average training loss is 0.09744566955293218\n",
            "Testing Acc.: 0.9672000044584275\n",
            "Epoch 397 completed, average training loss is 0.09741262210843464\n",
            "Testing Acc.: 0.9667000073194504\n",
            "Epoch 398 completed, average training loss is 0.09700480872609964\n",
            "Testing Acc.: 0.9663000071048736\n",
            "Epoch 399 completed, average training loss is 0.09707817921259751\n",
            "Testing Acc.: 0.9686000055074692\n",
            "Epoch 400 completed, average training loss is 0.09704228053024659\n",
            "Testing Acc.: 0.9665000051259994\n",
            "Epoch 401 completed, average training loss is 0.1181517377216369\n",
            "Testing Acc.: 0.965000006556511\n",
            "Epoch 402 completed, average training loss is 0.10448292635536442\n",
            "Testing Acc.: 0.9657000041007996\n",
            "Epoch 403 completed, average training loss is 0.10234158738826712\n",
            "Testing Acc.: 0.9651000052690506\n",
            "Epoch 404 completed, average training loss is 0.10123255276897301\n",
            "Testing Acc.: 0.9626000005006791\n",
            "Epoch 405 completed, average training loss is 0.09951814567515006\n",
            "Testing Acc.: 0.9670000046491622\n",
            "Epoch 406 completed, average training loss is 0.0994963080373903\n",
            "Testing Acc.: 0.9669000047445298\n",
            "Epoch 407 completed, average training loss is 0.09725040592253208\n",
            "Testing Acc.: 0.9664000082015991\n",
            "Epoch 408 completed, average training loss is 0.09793392403051257\n",
            "Testing Acc.: 0.9645000052452087\n",
            "Epoch 409 completed, average training loss is 0.096551981032826\n",
            "Testing Acc.: 0.9647000068426133\n",
            "Epoch 410 completed, average training loss is 0.09785000527044758\n",
            "Testing Acc.: 0.9682000052928924\n",
            "Epoch 411 completed, average training loss is 0.097286009072947\n",
            "Testing Acc.: 0.9665000033378601\n",
            "Epoch 412 completed, average training loss is 0.09741666188308348\n",
            "Testing Acc.: 0.9658000028133392\n",
            "Epoch 413 completed, average training loss is 0.09659724689243983\n",
            "Testing Acc.: 0.9670000052452088\n",
            "Epoch 414 completed, average training loss is 0.09792889195028692\n",
            "Testing Acc.: 0.9656000036001205\n",
            "Epoch 415 completed, average training loss is 0.09765033911758413\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 416 completed, average training loss is 0.095143755467919\n",
            "Testing Acc.: 0.963300005197525\n",
            "Epoch 417 completed, average training loss is 0.09566736752167344\n",
            "Testing Acc.: 0.9678000074625015\n",
            "Epoch 418 completed, average training loss is 0.0958498768073817\n",
            "Testing Acc.: 0.9663000041246415\n",
            "Epoch 419 completed, average training loss is 0.09524371214055767\n",
            "Testing Acc.: 0.9643000048398972\n",
            "Epoch 420 completed, average training loss is 0.09595385787077249\n",
            "Testing Acc.: 0.9668000048398971\n",
            "Epoch 421 completed, average training loss is 0.09549590740508089\n",
            "Testing Acc.: 0.9666000044345856\n",
            "Epoch 422 completed, average training loss is 0.0957565333476911\n",
            "Testing Acc.: 0.9680000030994416\n",
            "Epoch 423 completed, average training loss is 0.09498998322058469\n",
            "Testing Acc.: 0.9665000057220459\n",
            "Epoch 424 completed, average training loss is 0.09447442662203684\n",
            "Testing Acc.: 0.96760000705719\n",
            "Epoch 425 completed, average training loss is 0.09391161126705508\n",
            "Testing Acc.: 0.9671000075340271\n",
            "Epoch 426 completed, average training loss is 0.09478915013528119\n",
            "Testing Acc.: 0.9680000078678132\n",
            "Epoch 427 completed, average training loss is 0.0933925774274394\n",
            "Testing Acc.: 0.9687000072002411\n",
            "Epoch 428 completed, average training loss is 0.09447377525269986\n",
            "Testing Acc.: 0.964100005030632\n",
            "Epoch 429 completed, average training loss is 0.09513642068797101\n",
            "Testing Acc.: 0.9664000076055527\n",
            "Epoch 430 completed, average training loss is 0.09496522458425413\n",
            "Testing Acc.: 0.9678000080585479\n",
            "Epoch 431 completed, average training loss is 0.09477553664706648\n",
            "Testing Acc.: 0.970000006556511\n",
            "Epoch 432 completed, average training loss is 0.09500169797800481\n",
            "Testing Acc.: 0.9676000058650971\n",
            "Epoch 433 completed, average training loss is 0.09345202327085038\n",
            "Testing Acc.: 0.9676000064611435\n",
            "Epoch 434 completed, average training loss is 0.09486661707361539\n",
            "Testing Acc.: 0.9655000054836274\n",
            "Epoch 435 completed, average training loss is 0.09454976435595502\n",
            "Testing Acc.: 0.968000003695488\n",
            "Epoch 436 completed, average training loss is 0.09402822701260448\n",
            "Testing Acc.: 0.9695000058412552\n",
            "Epoch 437 completed, average training loss is 0.09458904270393154\n",
            "Testing Acc.: 0.9654000067710876\n",
            "Epoch 438 completed, average training loss is 0.09354035984181489\n",
            "Testing Acc.: 0.9676000052690505\n",
            "Epoch 439 completed, average training loss is 0.0949647641981331\n",
            "Testing Acc.: 0.9672000068426132\n",
            "Epoch 440 completed, average training loss is 0.09300349107012153\n",
            "Testing Acc.: 0.9664000064134598\n",
            "Epoch 441 completed, average training loss is 0.09379146348840246\n",
            "Testing Acc.: 0.9685000067949295\n",
            "Epoch 442 completed, average training loss is 0.09274366408586503\n",
            "Testing Acc.: 0.9676000064611435\n",
            "Epoch 443 completed, average training loss is 0.09428614643557619\n",
            "Testing Acc.: 0.9661000072956085\n",
            "Epoch 444 completed, average training loss is 0.09375601886150738\n",
            "Testing Acc.: 0.9678000044822693\n",
            "Epoch 445 completed, average training loss is 0.09221087938485047\n",
            "Testing Acc.: 0.9673000061511994\n",
            "Epoch 446 completed, average training loss is 0.09395898393044869\n",
            "Testing Acc.: 0.9657000058889389\n",
            "Epoch 447 completed, average training loss is 0.0922193397488445\n",
            "Testing Acc.: 0.9673000049591064\n",
            "Epoch 448 completed, average training loss is 0.094175617864045\n",
            "Testing Acc.: 0.9683000087738037\n",
            "Epoch 449 completed, average training loss is 0.09244157002152255\n",
            "Testing Acc.: 0.9676000034809112\n",
            "Epoch 450 completed, average training loss is 0.09435071638474862\n",
            "Testing Acc.: 0.9686000049114227\n",
            "Epoch 451 completed, average training loss is 0.09200053178239614\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 452 completed, average training loss is 0.09464180494192988\n",
            "Testing Acc.: 0.9668000066280364\n",
            "Epoch 453 completed, average training loss is 0.09291648165788502\n",
            "Testing Acc.: 0.9693000054359436\n",
            "Epoch 454 completed, average training loss is 0.09328840753374\n",
            "Testing Acc.: 0.9665000081062317\n",
            "Epoch 455 completed, average training loss is 0.093304147318316\n",
            "Testing Acc.: 0.9692000079154969\n",
            "Epoch 456 completed, average training loss is 0.09203074982545029\n",
            "Testing Acc.: 0.9682000035047531\n",
            "Epoch 457 completed, average training loss is 0.09258366522689661\n",
            "Testing Acc.: 0.9658000046014785\n",
            "Epoch 458 completed, average training loss is 0.09372901714717348\n",
            "Testing Acc.: 0.9695000076293945\n",
            "Epoch 459 completed, average training loss is 0.09272168525572246\n",
            "Testing Acc.: 0.9681000059843063\n",
            "Epoch 460 completed, average training loss is 0.092612232704026\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 461 completed, average training loss is 0.09301629365887493\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 462 completed, average training loss is 0.09209169378193716\n",
            "Testing Acc.: 0.9693000066280365\n",
            "Epoch 463 completed, average training loss is 0.09323935641596715\n",
            "Testing Acc.: 0.9689000076055527\n",
            "Epoch 464 completed, average training loss is 0.09341614572020869\n",
            "Testing Acc.: 0.9681000077724456\n",
            "Epoch 465 completed, average training loss is 0.09284435405395924\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 466 completed, average training loss is 0.09137109652472039\n",
            "Testing Acc.: 0.9679000067710877\n",
            "Epoch 467 completed, average training loss is 0.09277582205909615\n",
            "Testing Acc.: 0.9675000047683716\n",
            "Epoch 468 completed, average training loss is 0.09290733084393045\n",
            "Testing Acc.: 0.9657000035047532\n",
            "Epoch 469 completed, average training loss is 0.09215003470579783\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 470 completed, average training loss is 0.09348890601967771\n",
            "Testing Acc.: 0.968600006699562\n",
            "Epoch 471 completed, average training loss is 0.09239348477528742\n",
            "Testing Acc.: 0.9679000043869018\n",
            "Epoch 472 completed, average training loss is 0.09268276086542755\n",
            "Testing Acc.: 0.9665000033378601\n",
            "Epoch 473 completed, average training loss is 0.09143086310941725\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 474 completed, average training loss is 0.09208615494659171\n",
            "Testing Acc.: 0.967000008225441\n",
            "Epoch 475 completed, average training loss is 0.09238040182584276\n",
            "Testing Acc.: 0.9703000056743621\n",
            "Epoch 476 completed, average training loss is 0.09227789693512023\n",
            "Testing Acc.: 0.968500007390976\n",
            "Epoch 477 completed, average training loss is 0.09194254665945967\n",
            "Testing Acc.: 0.9672000062465668\n",
            "Epoch 478 completed, average training loss is 0.0924638796520109\n",
            "Testing Acc.: 0.9687000072002411\n",
            "Epoch 479 completed, average training loss is 0.0916080247083058\n",
            "Testing Acc.: 0.9689000076055527\n",
            "Epoch 480 completed, average training loss is 0.09153159199437748\n",
            "Testing Acc.: 0.9693000066280365\n",
            "Epoch 481 completed, average training loss is 0.09246124572896709\n",
            "Testing Acc.: 0.9686000072956085\n",
            "Epoch 482 completed, average training loss is 0.0908734167025735\n",
            "Testing Acc.: 0.9679000091552734\n",
            "Epoch 483 completed, average training loss is 0.0917961569596082\n",
            "Testing Acc.: 0.9685000079870224\n",
            "Epoch 484 completed, average training loss is 0.09193053843298306\n",
            "Testing Acc.: 0.9699000078439712\n",
            "Epoch 485 completed, average training loss is 0.093000627765432\n",
            "Testing Acc.: 0.9670000076293945\n",
            "Epoch 486 completed, average training loss is 0.09100148532927657\n",
            "Testing Acc.: 0.9694000065326691\n",
            "Epoch 487 completed, average training loss is 0.09261111378204077\n",
            "Testing Acc.: 0.970500009059906\n",
            "Epoch 488 completed, average training loss is 0.0916195536761855\n",
            "Testing Acc.: 0.9688000071048737\n",
            "Epoch 489 completed, average training loss is 0.09247499238078793\n",
            "Testing Acc.: 0.9692000037431717\n",
            "Epoch 490 completed, average training loss is 0.09190784581005573\n",
            "Testing Acc.: 0.9674000078439713\n",
            "Epoch 491 completed, average training loss is 0.09313585474931946\n",
            "Testing Acc.: 0.9687000054121018\n",
            "Epoch 492 completed, average training loss is 0.09084171967270474\n",
            "Testing Acc.: 0.9685000085830688\n",
            "Epoch 493 completed, average training loss is 0.09409806997670482\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 494 completed, average training loss is 0.09279428517911584\n",
            "Testing Acc.: 0.9686000055074692\n",
            "Epoch 495 completed, average training loss is 0.09261749534867704\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 496 completed, average training loss is 0.09043639611918479\n",
            "Testing Acc.: 0.9680000060796737\n",
            "Epoch 497 completed, average training loss is 0.09130651979862402\n",
            "Testing Acc.: 0.9683000046014786\n",
            "Epoch 498 completed, average training loss is 0.09208168604876846\n",
            "Testing Acc.: 0.9689000064134597\n",
            "Epoch 499 completed, average training loss is 0.09315164057149862\n",
            "Testing Acc.: 0.9697000074386597\n",
            "Epoch 500 completed, average training loss is 0.09207944116555154\n",
            "Testing Acc.: 0.9696000057458878\n",
            "Epoch 501 completed, average training loss is 0.09178716462726394\n",
            "Testing Acc.: 0.969200005531311\n",
            "Epoch 502 completed, average training loss is 0.09072312500017385\n",
            "Testing Acc.: 0.9683000087738037\n",
            "Epoch 503 completed, average training loss is 0.09357402769383043\n",
            "Testing Acc.: 0.9683000057935714\n",
            "Epoch 504 completed, average training loss is 0.09249747038663675\n",
            "Testing Acc.: 0.9683000063896179\n",
            "Epoch 505 completed, average training loss is 0.09190213990552971\n",
            "Testing Acc.: 0.9687000060081482\n",
            "Epoch 506 completed, average training loss is 0.0916779655435433\n",
            "Testing Acc.: 0.9680000054836273\n",
            "Epoch 507 completed, average training loss is 0.09140269259223714\n",
            "Testing Acc.: 0.9692000043392182\n",
            "Epoch 508 completed, average training loss is 0.09189142891361067\n",
            "Testing Acc.: 0.9678000068664551\n",
            "Epoch 509 completed, average training loss is 0.093112791160432\n",
            "Testing Acc.: 0.9681000053882599\n",
            "Epoch 510 completed, average training loss is 0.09181945021885136\n",
            "Testing Acc.: 0.9677000081539154\n",
            "Epoch 511 completed, average training loss is 0.09019769112424304\n",
            "Testing Acc.: 0.9661000049114228\n",
            "Epoch 512 completed, average training loss is 0.09208797584054991\n",
            "Testing Acc.: 0.9685000085830688\n",
            "Epoch 513 completed, average training loss is 0.09210357128254448\n",
            "Testing Acc.: 0.9684000068902969\n",
            "Epoch 514 completed, average training loss is 0.0918051045279329\n",
            "Testing Acc.: 0.9701000064611435\n",
            "Epoch 515 completed, average training loss is 0.09220538561077167\n",
            "Testing Acc.: 0.9698000061511993\n",
            "Epoch 516 completed, average training loss is 0.09202489250494789\n",
            "Testing Acc.: 0.9693000078201294\n",
            "Epoch 517 completed, average training loss is 0.09029310100246221\n",
            "Testing Acc.: 0.9675000047683716\n",
            "Epoch 518 completed, average training loss is 0.09204267104621977\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 519 completed, average training loss is 0.09038103874151905\n",
            "Testing Acc.: 0.9684000051021576\n",
            "Epoch 520 completed, average training loss is 0.09079939412573973\n",
            "Testing Acc.: 0.9676000052690505\n",
            "Epoch 521 completed, average training loss is 0.09093208793240289\n",
            "Testing Acc.: 0.9678000062704086\n",
            "Epoch 522 completed, average training loss is 0.09171182565546285\n",
            "Testing Acc.: 0.9665000027418137\n",
            "Epoch 523 completed, average training loss is 0.09111303067300469\n",
            "Testing Acc.: 0.9683000069856643\n",
            "Epoch 524 completed, average training loss is 0.09120527865209928\n",
            "Testing Acc.: 0.96710000872612\n",
            "Epoch 525 completed, average training loss is 0.09149921224297335\n",
            "Testing Acc.: 0.9686000055074692\n",
            "Epoch 526 completed, average training loss is 0.09050769610485683\n",
            "Testing Acc.: 0.9697000074386597\n",
            "Epoch 527 completed, average training loss is 0.09072520980145782\n",
            "Testing Acc.: 0.9656000053882599\n",
            "Epoch 528 completed, average training loss is 0.09149803698683778\n",
            "Testing Acc.: 0.9674000072479249\n",
            "Epoch 529 completed, average training loss is 0.09142027457865576\n",
            "Testing Acc.: 0.9656000065803528\n",
            "Epoch 530 completed, average training loss is 0.08997347495363404\n",
            "Testing Acc.: 0.969500007033348\n",
            "Epoch 531 completed, average training loss is 0.09202395516602943\n",
            "Testing Acc.: 0.9704000067710876\n",
            "Epoch 532 completed, average training loss is 0.09161087205788741\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 533 completed, average training loss is 0.091790054673329\n",
            "Testing Acc.: 0.9666000044345856\n",
            "Epoch 534 completed, average training loss is 0.0911361147214969\n",
            "Testing Acc.: 0.9693000072240829\n",
            "Epoch 535 completed, average training loss is 0.09125994296899687\n",
            "Testing Acc.: 0.9695000040531159\n",
            "Epoch 536 completed, average training loss is 0.0905311381816864\n",
            "Testing Acc.: 0.9702000057697296\n",
            "Epoch 537 completed, average training loss is 0.09050301049370318\n",
            "Testing Acc.: 0.9666000074148178\n",
            "Epoch 538 completed, average training loss is 0.09053493571778139\n",
            "Testing Acc.: 0.9656000047922134\n",
            "Epoch 539 completed, average training loss is 0.09099624697118998\n",
            "Testing Acc.: 0.9697000068426133\n",
            "Epoch 540 completed, average training loss is 0.09054741570415596\n",
            "Testing Acc.: 0.9687000083923339\n",
            "Epoch 541 completed, average training loss is 0.08953706143734355\n",
            "Testing Acc.: 0.969500007033348\n",
            "Epoch 542 completed, average training loss is 0.09088783291478952\n",
            "Testing Acc.: 0.9688000071048737\n",
            "Epoch 543 completed, average training loss is 0.089112467081286\n",
            "Testing Acc.: 0.9702000069618225\n",
            "Epoch 544 completed, average training loss is 0.09074641510145738\n",
            "Testing Acc.: 0.96730000436306\n",
            "Epoch 545 completed, average training loss is 0.09190881797422966\n",
            "Testing Acc.: 0.9691000086069107\n",
            "Epoch 546 completed, average training loss is 0.09087606608867645\n",
            "Testing Acc.: 0.9692000073194503\n",
            "Epoch 547 completed, average training loss is 0.09210316758447637\n",
            "Testing Acc.: 0.9684000074863434\n",
            "Epoch 548 completed, average training loss is 0.09000358154376348\n",
            "Testing Acc.: 0.968500007390976\n",
            "Epoch 549 completed, average training loss is 0.08939875642458597\n",
            "Testing Acc.: 0.9684000056982041\n",
            "Epoch 550 completed, average training loss is 0.09127057890873402\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 551 completed, average training loss is 0.09055245093458021\n",
            "Testing Acc.: 0.9710000079870224\n",
            "Epoch 552 completed, average training loss is 0.09111103486424933\n",
            "Testing Acc.: 0.9680000084638596\n",
            "Epoch 553 completed, average training loss is 0.0893738978394928\n",
            "Testing Acc.: 0.9699000060558319\n",
            "Epoch 554 completed, average training loss is 0.0894642767853414\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 555 completed, average training loss is 0.09005787634678807\n",
            "Testing Acc.: 0.970400008559227\n",
            "Epoch 556 completed, average training loss is 0.08988920256650697\n",
            "Testing Acc.: 0.9682000052928924\n",
            "Epoch 557 completed, average training loss is 0.09073597891877094\n",
            "Testing Acc.: 0.9661000049114228\n",
            "Epoch 558 completed, average training loss is 0.09012904320688297\n",
            "Testing Acc.: 0.9697000074386597\n",
            "Epoch 559 completed, average training loss is 0.09192576567642391\n",
            "Testing Acc.: 0.9684000045061112\n",
            "Epoch 560 completed, average training loss is 0.09150083738534401\n",
            "Testing Acc.: 0.9681000077724456\n",
            "Epoch 561 completed, average training loss is 0.09003770011477172\n",
            "Testing Acc.: 0.9664000058174134\n",
            "Epoch 562 completed, average training loss is 0.09051942431212713\n",
            "Testing Acc.: 0.9642000079154969\n",
            "Epoch 563 completed, average training loss is 0.09031460711422067\n",
            "Testing Acc.: 0.9681000059843063\n",
            "Epoch 564 completed, average training loss is 0.09005357998268058\n",
            "Testing Acc.: 0.9685000038146973\n",
            "Epoch 565 completed, average training loss is 0.0913584107067436\n",
            "Testing Acc.: 0.9687000083923339\n",
            "Epoch 566 completed, average training loss is 0.09023590200114995\n",
            "Testing Acc.: 0.9680000072717667\n",
            "Epoch 567 completed, average training loss is 0.09015030737655859\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 568 completed, average training loss is 0.09081449319453289\n",
            "Testing Acc.: 0.9694000083208084\n",
            "Epoch 569 completed, average training loss is 0.09030713793355971\n",
            "Testing Acc.: 0.9690000057220459\n",
            "Epoch 570 completed, average training loss is 0.09071605072822422\n",
            "Testing Acc.: 0.9661000072956085\n",
            "Epoch 571 completed, average training loss is 0.08912103033003707\n",
            "Testing Acc.: 0.9694000041484833\n",
            "Epoch 572 completed, average training loss is 0.09044369507115334\n",
            "Testing Acc.: 0.9688000077009201\n",
            "Epoch 573 completed, average training loss is 0.0896264330468451\n",
            "Testing Acc.: 0.9696000057458878\n",
            "Epoch 574 completed, average training loss is 0.09073703921865672\n",
            "Testing Acc.: 0.9667000055313111\n",
            "Epoch 575 completed, average training loss is 0.09139539524757613\n",
            "Testing Acc.: 0.9704000049829483\n",
            "Epoch 576 completed, average training loss is 0.09092981057862441\n",
            "Testing Acc.: 0.9676000052690505\n",
            "Epoch 577 completed, average training loss is 0.09020577611401677\n",
            "Testing Acc.: 0.9687000054121018\n",
            "Epoch 578 completed, average training loss is 0.09053693623437235\n",
            "Testing Acc.: 0.9667000055313111\n",
            "Epoch 579 completed, average training loss is 0.08960610959989329\n",
            "Testing Acc.: 0.9662000048160553\n",
            "Epoch 580 completed, average training loss is 0.09186781342606992\n",
            "Testing Acc.: 0.9689000058174133\n",
            "Epoch 581 completed, average training loss is 0.09212640751929332\n",
            "Testing Acc.: 0.9687000060081482\n",
            "Epoch 582 completed, average training loss is 0.09016968047711998\n",
            "Testing Acc.: 0.9676000076532364\n",
            "Epoch 583 completed, average training loss is 0.08982576865974504\n",
            "Testing Acc.: 0.9688000071048737\n",
            "Epoch 584 completed, average training loss is 0.09164416897731523\n",
            "Testing Acc.: 0.9683000081777573\n",
            "Epoch 585 completed, average training loss is 0.09046478096550951\n",
            "Testing Acc.: 0.9665000051259994\n",
            "Epoch 586 completed, average training loss is 0.08944740290287882\n",
            "Testing Acc.: 0.9694000095129013\n",
            "Epoch 587 completed, average training loss is 0.09178466065010676\n",
            "Testing Acc.: 0.9680000048875809\n",
            "Epoch 588 completed, average training loss is 0.08890974813761811\n",
            "Testing Acc.: 0.9682000064849854\n",
            "Epoch 589 completed, average training loss is 0.09041948852594942\n",
            "Testing Acc.: 0.9674000072479249\n",
            "Epoch 590 completed, average training loss is 0.0895141816449662\n",
            "Testing Acc.: 0.9701000046730042\n",
            "Epoch 591 completed, average training loss is 0.09014949367071191\n",
            "Testing Acc.: 0.9665000069141388\n",
            "Epoch 592 completed, average training loss is 0.09049169313550616\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 593 completed, average training loss is 0.09042171510091672\n",
            "Testing Acc.: 0.9702000057697296\n",
            "Epoch 594 completed, average training loss is 0.09103733029371748\n",
            "Testing Acc.: 0.9698000091314316\n",
            "Epoch 595 completed, average training loss is 0.08998791784824182\n",
            "Testing Acc.: 0.9685000067949295\n",
            "Epoch 596 completed, average training loss is 0.08951800592709333\n",
            "Testing Acc.: 0.9698000055551529\n",
            "Epoch 597 completed, average training loss is 0.0904851919459179\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 598 completed, average training loss is 0.08970825831716259\n",
            "Testing Acc.: 0.9695000064373016\n",
            "Epoch 599 completed, average training loss is 0.09060016197152436\n",
            "Testing Acc.: 0.9679000055789948\n",
            "Epoch 600 completed, average training loss is 0.09010642134274045\n",
            "Testing Acc.: 0.9716000092029572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M01RmWi3VVrd",
        "outputId": "131f08e8-4b7f-431b-f981-7e4782850a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ^^^^ Above reads:\n",
        "### Epoch 600 completed, average training loss is 0.09010642134274045\n",
        "### Testing Acc.: 0.9716000092029572\n",
        "\n",
        "\n",
        "# Save model, for not having to fully retrain it each time.\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, '4bits/model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4bits/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bF_rNlqMVVrh",
        "outputId": "ff63c440-1187-48d3-a6fe-86e2fbeba4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9716000092029572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PgVpP42fVVrl",
        "outputId": "32088db0-2452-49eb-c576-0e5618f22989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!zip -r 4bits.zip 4bits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: 4bits/ (stored 0%)\n",
            "  adding: 4bits/model.ckpt.data-00000-of-00001 (deflated 37%)\n",
            "  adding: 4bits/model.ckpt.index (deflated 49%)\n",
            "  adding: 4bits/checkpoint (deflated 42%)\n",
            "  adding: 4bits/model.ckpt.meta (deflated 89%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xX_5Sr18VVrr"
      },
      "source": [
        "## 2.3 Weights rounding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8PeWWkHVVrs",
        "colab": {}
      },
      "source": [
        "!unzip 4bits.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L495L-x9VVrx",
        "outputId": "4f017e24-0609-4a3e-c83e-ece71ccfcedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Quantize a [0, +127] value on 4 bits, so that\n",
        "# it fits in 4 DREGs\n",
        "# 4 DREGs for the most significant bits\n",
        "def quantization_4bits_unsigned(tensor):\n",
        "  sign = tf.sign(tensor)\n",
        "  \n",
        "  cast = tf.cast(sign*tensor, tf.int32)\n",
        "  sign = tf.cast(sign, tf.int32)\n",
        "  \n",
        "  digitized = (64*tf.mod(tf.bitwise.right_shift(cast,6), 2) +\n",
        "               32*tf.mod(tf.bitwise.right_shift(cast,5), 2) +\n",
        "               16*tf.mod(tf.bitwise.right_shift(cast,4), 2) + \n",
        "               8*tf.mod(tf.bitwise.right_shift(cast,3), 2))\n",
        "  digitized = digitized * sign  \n",
        "  return tf.cast(digitized, tf.float32)\n",
        "\n",
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)\n",
        "\n",
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool\n",
        "\n",
        "def network2(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  # C2.1, none of the input is quantized\n",
        "  conv2_1 = slim.conv2d(conv, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.1')\n",
        "  \n",
        "  # C2.2, two inputs (out of 3) are quantized\n",
        "  slice_A_2_2 = quantization_4bits_unsigned(conv[...,:2])\n",
        "  slice_B_2_2 = conv[...,2:]\n",
        "  conv2_2_input = tf.concat([slice_A_2_2, slice_B_2_2], axis=-1)\n",
        "  conv2_2 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.2')\n",
        "  \n",
        "  # C2.3, same inputs as for C2.2 (2 feature maps quantified out of 3)\n",
        "  conv2_3 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.3')\n",
        "  \n",
        "  \n",
        "  conv2 = tf.concat([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out\n",
        "\n",
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network2(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "ckpt = tf.train.get_checkpoint_state('4bits')\n",
        "saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0902 09:02:06.496021 140268826339200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.971100007891655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fc3nLSzoVVr1",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2.1', reuse=True) as scope_conv:\n",
        "  w2_1 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.2', reuse=True) as scope_conv:\n",
        "  w2_2 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.3', reuse=True) as scope_conv:\n",
        "  w2_3 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u9p5EF91VVr4",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 4."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PSJ5nHPAVVr8",
        "outputId": "be7c89f9-86c7-40d3-e180-9a06184ebfe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Rounding operation\n",
        "rounding_weights1_op = tf.assign(w1, \n",
        "                          tf.round(w1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_bias1_op = tf.assign(b1,\n",
        "                          tf.round(b1/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "rounding_weights2_1_op = tf.assign(w2_1, \n",
        "                          tf.round(w2_1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_weights2_2_op = tf.assign(w2_2, \n",
        "                          tf.round(w2_2/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_weights2_3_op = tf.assign(w2_3, \n",
        "                          tf.round(w2_3/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_thresh_op = tf.assign(thresh,\n",
        "                          tf.round(thresh/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "_ = sess.run([rounding_weights1_op, rounding_bias1_op])\n",
        "_ = sess.run([rounding_weights2_1_op, rounding_weights2_2_op])\n",
        "_ = sess.run([rounding_weights2_3_op, rounding_thresh_op])\n",
        "\n",
        "\n",
        "# Show final distribution of weights\n",
        "w1_values, b1_values = sess.run([w1, b1])\n",
        "w2_1_values, w2_2_values = sess.run([w2_1, w2_2])\n",
        "w2_3_values, thresh_values = sess.run([w2_3, thresh])\n",
        "\n",
        "kernel_values = (list(w1_values.flatten()) + list(b1_values.flatten())\n",
        "                 + list(w2_1_values.flatten())\n",
        "                 + list(w2_2_values.flatten())\n",
        "                 + list(w2_3_values.flatten())\n",
        "                 + list(thresh_values.flatten()))\n",
        "fig = plt.figure()\n",
        "plt.scatter(kernel_values, [1]*len(kernel_values))\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.814500002861023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9C1G-FUKVVsA",
        "colab": {}
      },
      "source": [
        "sess.run(w2_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "piuWiN0hVVsE"
      },
      "source": [
        "## 2.4 Retrain FC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xxnoTsdJVVsE",
        "colab": {}
      },
      "source": [
        "LR = 0.001/4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbkTorQvVVsJ",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('dense1', reuse=True) as scope_conv:\n",
        "  fc1_k = tf.get_variable('kernel')\n",
        "  fc1_b = tf.get_variable('bias')\n",
        "\n",
        "with tf.variable_scope('dense2', reuse=True) as scope_conv:\n",
        "  fc2_k = tf.get_variable('kernel')\n",
        "  fc2_b = tf.get_variable('bias')\n",
        "\n",
        "opt_fc = tf.train.AdamOptimizer(learning_rate=LR, name='Adam_reg')\n",
        "opt_fc_op = opt_fc.minimize(loss_op, var_list=[fc1_k, fc1_b,\n",
        "                                               fc2_k, fc2_b])\n",
        "\n",
        "sess.run(tf.variables_initializer(opt_fc.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gewiTntBVVsM",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GJcS20CbVVsO",
        "outputId": "1e0483fe-673b-4529-d604-e4dddf94ff11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bin_rate_feed = MAX_BIN_RATE + 1\n",
        "\n",
        "for epoch in range(EPOCHS*2):\n",
        "\n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "\n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "\n",
        "    _, current_loss = sess.run([opt_fc_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "\n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 0.12474793142639101\n",
            "Testing Acc.: 0.9641000056266784\n",
            "Epoch 2 completed, average training loss is 0.10504382239188999\n",
            "Testing Acc.: 0.9675000071525574\n",
            "Epoch 3 completed, average training loss is 0.10201754812461634\n",
            "Testing Acc.: 0.9663000059127808\n",
            "Epoch 4 completed, average training loss is 0.09987455175879101\n",
            "Testing Acc.: 0.9661000025272369\n",
            "Epoch 5 completed, average training loss is 0.09955287608628471\n",
            "Testing Acc.: 0.9667000061273575\n",
            "Epoch 6 completed, average training loss is 0.09854853264987469\n",
            "Testing Acc.: 0.9670000070333481\n",
            "Epoch 7 completed, average training loss is 0.09818603516245882\n",
            "Testing Acc.: 0.9681000047922135\n",
            "Epoch 8 completed, average training loss is 0.09763067949408044\n",
            "Testing Acc.: 0.967700006365776\n",
            "Epoch 9 completed, average training loss is 0.09713823487982154\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 10 completed, average training loss is 0.0965991374043127\n",
            "Testing Acc.: 0.9687000060081482\n",
            "Epoch 11 completed, average training loss is 0.09662687571719289\n",
            "Testing Acc.: 0.9675000071525574\n",
            "Epoch 12 completed, average training loss is 0.09711578602126489\n",
            "Testing Acc.: 0.9685000061988831\n",
            "Epoch 13 completed, average training loss is 0.09623019053445508\n",
            "Testing Acc.: 0.9669000047445298\n",
            "Epoch 14 completed, average training loss is 0.09551892770764728\n",
            "Testing Acc.: 0.9682000076770783\n",
            "Epoch 15 completed, average training loss is 0.09534910457674414\n",
            "Testing Acc.: 0.9680000060796737\n",
            "Epoch 16 completed, average training loss is 0.09517669301169614\n",
            "Testing Acc.: 0.9671000081300736\n",
            "Epoch 17 completed, average training loss is 0.09546613585514327\n",
            "Testing Acc.: 0.9680000102519989\n",
            "Epoch 18 completed, average training loss is 0.09550793372560293\n",
            "Testing Acc.: 0.9681000071763992\n",
            "Epoch 19 completed, average training loss is 0.09494361829323074\n",
            "Testing Acc.: 0.9688000071048737\n",
            "Epoch 20 completed, average training loss is 0.09492916746375461\n",
            "Testing Acc.: 0.968200005888939\n",
            "Epoch 21 completed, average training loss is 0.09448537363205105\n",
            "Testing Acc.: 0.9661000031232834\n",
            "Epoch 22 completed, average training loss is 0.09425629248376936\n",
            "Testing Acc.: 0.9673000079393387\n",
            "Epoch 23 completed, average training loss is 0.09426563142954061\n",
            "Testing Acc.: 0.9700000071525574\n",
            "Epoch 24 completed, average training loss is 0.09422645508001248\n",
            "Testing Acc.: 0.966300003528595\n",
            "Epoch 25 completed, average training loss is 0.09441796189484497\n",
            "Testing Acc.: 0.9694000065326691\n",
            "Epoch 26 completed, average training loss is 0.09404737955890596\n",
            "Testing Acc.: 0.9674000054597854\n",
            "Epoch 27 completed, average training loss is 0.09461036053486169\n",
            "Testing Acc.: 0.9675000065565109\n",
            "Epoch 28 completed, average training loss is 0.09444970842450857\n",
            "Testing Acc.: 0.9687000089883804\n",
            "Epoch 29 completed, average training loss is 0.09388910994088898\n",
            "Testing Acc.: 0.9685000056028366\n",
            "Epoch 30 completed, average training loss is 0.09430643587838858\n",
            "Testing Acc.: 0.9694000083208084\n",
            "Epoch 31 completed, average training loss is 0.09389575843854497\n",
            "Testing Acc.: 0.9666000062227249\n",
            "Epoch 32 completed, average training loss is 0.09367861263764402\n",
            "Testing Acc.: 0.9692000061273575\n",
            "Epoch 33 completed, average training loss is 0.09318495848526558\n",
            "Testing Acc.: 0.9679000061750412\n",
            "Epoch 34 completed, average training loss is 0.09381851190623516\n",
            "Testing Acc.: 0.9694000089168548\n",
            "Epoch 35 completed, average training loss is 0.09370921411551535\n",
            "Testing Acc.: 0.9676000088453293\n",
            "Epoch 36 completed, average training loss is 0.09353630095099409\n",
            "Testing Acc.: 0.967700006365776\n",
            "Epoch 37 completed, average training loss is 0.0932402261874328\n",
            "Testing Acc.: 0.9679000073671341\n",
            "Epoch 38 completed, average training loss is 0.09465090516178558\n",
            "Testing Acc.: 0.966300003528595\n",
            "Epoch 39 completed, average training loss is 0.09343315326608717\n",
            "Testing Acc.: 0.969100006222725\n",
            "Epoch 40 completed, average training loss is 0.0937039596028626\n",
            "Testing Acc.: 0.9681000053882599\n",
            "Epoch 41 completed, average training loss is 0.09358234344205509\n",
            "Testing Acc.: 0.9693000054359436\n",
            "Epoch 42 completed, average training loss is 0.09312392573182782\n",
            "Testing Acc.: 0.9695000058412552\n",
            "Epoch 43 completed, average training loss is 0.09379949969549974\n",
            "Testing Acc.: 0.9679000061750412\n",
            "Epoch 44 completed, average training loss is 0.09324854177733262\n",
            "Testing Acc.: 0.9692000061273575\n",
            "Epoch 45 completed, average training loss is 0.09269496049887191\n",
            "Testing Acc.: 0.9696000081300735\n",
            "Epoch 46 completed, average training loss is 0.09303545662667602\n",
            "Testing Acc.: 0.9672000062465668\n",
            "Epoch 47 completed, average training loss is 0.09315082625796398\n",
            "Testing Acc.: 0.9700000095367431\n",
            "Epoch 48 completed, average training loss is 0.09272274706202249\n",
            "Testing Acc.: 0.969100006222725\n",
            "Epoch 49 completed, average training loss is 0.09303947755523646\n",
            "Testing Acc.: 0.9707000082731247\n",
            "Epoch 50 completed, average training loss is 0.09290900471620261\n",
            "Testing Acc.: 0.9683000075817109\n",
            "Epoch 51 completed, average training loss is 0.09303894280766448\n",
            "Testing Acc.: 0.9672000050544739\n",
            "Epoch 52 completed, average training loss is 0.09314977414285143\n",
            "Testing Acc.: 0.9695000064373016\n",
            "Epoch 53 completed, average training loss is 0.09270169582838814\n",
            "Testing Acc.: 0.9689000076055527\n",
            "Epoch 54 completed, average training loss is 0.09254447881287585\n",
            "Testing Acc.: 0.9692000067234039\n",
            "Epoch 55 completed, average training loss is 0.09277066287274162\n",
            "Testing Acc.: 0.9685000044107437\n",
            "Epoch 56 completed, average training loss is 0.09293258057286342\n",
            "Testing Acc.: 0.9665000057220459\n",
            "Epoch 57 completed, average training loss is 0.09306638868991285\n",
            "Testing Acc.: 0.9689000058174133\n",
            "Epoch 58 completed, average training loss is 0.09282570037059486\n",
            "Testing Acc.: 0.9686000049114227\n",
            "Epoch 59 completed, average training loss is 0.09265845130973806\n",
            "Testing Acc.: 0.9678000038862229\n",
            "Epoch 60 completed, average training loss is 0.09320853152001897\n",
            "Testing Acc.: 0.9679000067710877\n",
            "Epoch 61 completed, average training loss is 0.09308473754208535\n",
            "Testing Acc.: 0.969100006222725\n",
            "Epoch 62 completed, average training loss is 0.09239980215206742\n",
            "Testing Acc.: 0.9683000075817109\n",
            "Epoch 63 completed, average training loss is 0.09283896104587863\n",
            "Testing Acc.: 0.9688000047206878\n",
            "Epoch 64 completed, average training loss is 0.09212421901058405\n",
            "Testing Acc.: 0.9692000067234039\n",
            "Epoch 65 completed, average training loss is 0.09333149688784033\n",
            "Testing Acc.: 0.9693000054359436\n",
            "Epoch 66 completed, average training loss is 0.09224258173101892\n",
            "Testing Acc.: 0.968300005197525\n",
            "Epoch 67 completed, average training loss is 0.09219645034677039\n",
            "Testing Acc.: 0.9705000072717667\n",
            "Epoch 68 completed, average training loss is 0.09265287340463449\n",
            "Testing Acc.: 0.9681000030040741\n",
            "Epoch 69 completed, average training loss is 0.09223804692582538\n",
            "Testing Acc.: 0.9706000083684921\n",
            "Epoch 70 completed, average training loss is 0.09284052512142807\n",
            "Testing Acc.: 0.9694000053405761\n",
            "Epoch 71 completed, average training loss is 0.09271640519766758\n",
            "Testing Acc.: 0.9685000056028366\n",
            "Epoch 72 completed, average training loss is 0.0924054156957815\n",
            "Testing Acc.: 0.9698000091314316\n",
            "Epoch 73 completed, average training loss is 0.09210269291109095\n",
            "Testing Acc.: 0.9687000077962875\n",
            "Epoch 74 completed, average training loss is 0.09255408624497553\n",
            "Testing Acc.: 0.966600005030632\n",
            "Epoch 75 completed, average training loss is 0.09264042087985823\n",
            "Testing Acc.: 0.9686000084877014\n",
            "Epoch 76 completed, average training loss is 0.09277635369760295\n",
            "Testing Acc.: 0.9694000083208084\n",
            "Epoch 77 completed, average training loss is 0.09211281792571148\n",
            "Testing Acc.: 0.970400007367134\n",
            "Epoch 78 completed, average training loss is 0.09213209283848603\n",
            "Testing Acc.: 0.9683000046014786\n",
            "Epoch 79 completed, average training loss is 0.09173636358386526\n",
            "Testing Acc.: 0.9700000077486038\n",
            "Epoch 80 completed, average training loss is 0.09249309479569395\n",
            "Testing Acc.: 0.9699000084400177\n",
            "Epoch 81 completed, average training loss is 0.09209926228970289\n",
            "Testing Acc.: 0.9680000072717667\n",
            "Epoch 82 completed, average training loss is 0.09177715966788431\n",
            "Testing Acc.: 0.9691000080108643\n",
            "Epoch 83 completed, average training loss is 0.0920950526650995\n",
            "Testing Acc.: 0.9682000052928924\n",
            "Epoch 84 completed, average training loss is 0.09242487056491276\n",
            "Testing Acc.: 0.96760000705719\n",
            "Epoch 85 completed, average training loss is 0.09223291193755964\n",
            "Testing Acc.: 0.9703000068664551\n",
            "Epoch 86 completed, average training loss is 0.09239037355485683\n",
            "Testing Acc.: 0.9674000066518783\n",
            "Epoch 87 completed, average training loss is 0.09180414540382723\n",
            "Testing Acc.: 0.9662000054121017\n",
            "Epoch 88 completed, average training loss is 0.09225953534555932\n",
            "Testing Acc.: 0.9693000060319901\n",
            "Epoch 89 completed, average training loss is 0.09175993535667658\n",
            "Testing Acc.: 0.9690000069141388\n",
            "Epoch 90 completed, average training loss is 0.09192002572119236\n",
            "Testing Acc.: 0.968600007891655\n",
            "Epoch 91 completed, average training loss is 0.09180954899949333\n",
            "Testing Acc.: 0.9691000086069107\n",
            "Epoch 92 completed, average training loss is 0.09243685805083562\n",
            "Testing Acc.: 0.9704000067710876\n",
            "Epoch 93 completed, average training loss is 0.09239893574034795\n",
            "Testing Acc.: 0.9695000052452087\n",
            "Epoch 94 completed, average training loss is 0.091850979795369\n",
            "Testing Acc.: 0.9691000074148178\n",
            "Epoch 95 completed, average training loss is 0.09162964397110045\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 96 completed, average training loss is 0.09142562029883265\n",
            "Testing Acc.: 0.9700000089406967\n",
            "Epoch 97 completed, average training loss is 0.09221911824463556\n",
            "Testing Acc.: 0.9700000077486038\n",
            "Epoch 98 completed, average training loss is 0.09165639456051092\n",
            "Testing Acc.: 0.9691000056266784\n",
            "Epoch 99 completed, average training loss is 0.09142542997685571\n",
            "Testing Acc.: 0.9683000081777573\n",
            "Epoch 100 completed, average training loss is 0.09150250263822575\n",
            "Testing Acc.: 0.9700000059604644\n",
            "Testing Acc.: 0.9700000059604644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aly4pDyOVVsS",
        "colab": {}
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE)\n",
        "test_accuracy_bin_rate(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dOoV3cZ8VVsU",
        "colab": {}
      },
      "source": [
        "# Get final distribution of weights\n",
        "w1_values, b1_values = sess.run([w1, b1])\n",
        "w2_1_values, w2_2_values = sess.run([w2_1, w2_2])\n",
        "w2_3_values, thresh_values = sess.run([w2_3, thresh])\n",
        "\n",
        "with open('NP_WEIGHTS_4bits.pck', 'wb') as f:\n",
        "  pickle.dump((w1_values, b1_values, w2_1_values, w2_2_values, w2_3_values, thresh_values), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9lj-aI4UTeV"
      },
      "source": [
        "# 3. 3 bits unsigned quantisation: 94.9% with fully independent training, 96.9% when re-using convolutions from the above network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j_LMAykNUTeZ"
      },
      "source": [
        "## 3.1 Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5yE8M2wUTeb",
        "colab": {}
      },
      "source": [
        "# Quantize a [0, +127] value on 3 bits, so that\n",
        "# it fits in 3 DREGs\n",
        "# 3 DREGs for the most significant bits\n",
        "def quantization_3bits_unsigned(tensor):\n",
        "  sign = tf.sign(tensor)\n",
        "  \n",
        "  cast = tf.cast(sign*tensor, tf.int32)\n",
        "  sign = tf.cast(sign, tf.int32)\n",
        "  \n",
        "  digitized = (64*tf.mod(tf.bitwise.right_shift(cast,6), 2) +\n",
        "               32*tf.mod(tf.bitwise.right_shift(cast,5), 2) +\n",
        "               16*tf.mod(tf.bitwise.right_shift(cast,4), 2))\n",
        "  digitized = digitized * sign  \n",
        "  return tf.cast(digitized, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TblxWAQ4UTeo",
        "colab": {}
      },
      "source": [
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX07_I9SU6ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VUrCy9ScUTeu",
        "colab": {}
      },
      "source": [
        "def network3(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  # C2.1, none of the input is quantized\n",
        "  conv2_1 = slim.conv2d(conv, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.1')\n",
        "  \n",
        "  # C2.2, two inputs (out of 3) are quantized\n",
        "  slice_A_2_2 = quantization_3bits_unsigned(conv[...,:2])\n",
        "  slice_B_2_2 = conv[...,2:]\n",
        "  conv2_2_input = tf.concat([slice_A_2_2, slice_B_2_2], axis=-1)\n",
        "  conv2_2 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.2')\n",
        "  \n",
        "  # C2.3, same inputs as for C2.2 (2 feature maps quantified out of 3)\n",
        "  conv2_3 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.3')\n",
        "  \n",
        "  \n",
        "  conv2 = tf.concat([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE1NdaAAUTe0",
        "colab": {}
      },
      "source": [
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network3(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmnPN6J6UTe4",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "45i1Q28zUTe8",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2.1', reuse=True) as scope_conv:\n",
        "  w2_1 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.2', reuse=True) as scope_conv:\n",
        "  w2_2 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.3', reuse=True) as scope_conv:\n",
        "  w2_3 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgUVYoAPUTfB",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 25."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_pkZjIrkUTfF",
        "colab": {}
      },
      "source": [
        "def customRegularizerConv(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_CONV*np.pi*(x-ROUNDING_STEP_CONV/2))+1.\n",
        "\n",
        "def customRegularizerBias(x):\n",
        "  return tf.math.cos(2/ROUNDING_STEP_BIAS*np.pi*(x-ROUNDING_STEP_BIAS/2))+1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvsXOGMcUTfP",
        "colab": {}
      },
      "source": [
        "NUM_PARAM_REG = 30. + 27. + 27. + 27. + 3.\n",
        "reg_losses = 27./NUM_PARAM_REG *tf.reduce_mean(customRegularizerConv(w1))\n",
        "reg_losses += 3./NUM_PARAM_REG *tf.reduce_mean(customRegularizerBias(b1))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_1))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_2))\n",
        "reg_losses += 27./NUM_PARAM_REG*tf.reduce_mean(customRegularizerConv(w2_3))\n",
        "reg_losses += 3./NUM_PARAM_REG*tf.reduce_mean(customRegularizerBias(thresh))\n",
        "\n",
        "reg_factor_ph = tf.placeholder(tf.float32)\n",
        "loss_with_reg_op = loss_op + reg_factor_ph * reg_losses\n",
        "\n",
        "lr_ph = tf.placeholder(tf.float32)\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr_ph, name='Adam_reg')\n",
        "opt_op = opt.minimize(loss_with_reg_op)\n",
        "sess.run(tf.variables_initializer(opt.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYpNBHwzUTfV"
      },
      "source": [
        "## 3.2 Training: 95.3%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLwz7U5OUTfX"
      },
      "source": [
        "Evolving params:\n",
        "- lr_ph : learning rate\n",
        "- reg_factor_ph : regularization factor -> this time, set directly to final value in the second learning stage\n",
        "- bin_rate_ph : binarization rate -> this time, set directly to final value in the third learning stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_GBT7JY7UTfY",
        "colab": {}
      },
      "source": [
        "INPUT_SCALING = 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FyyKv9zeUTfc",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tBRPQpJrUTfk",
        "outputId": "b6e65d77-c300-4939-b640-4790c3161c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initial version, reg then bin\n",
        "for epoch in range(EPOCHS*12):\n",
        "  if epoch < EPOCHS * 4:\n",
        "    lr_feed = LR\n",
        "    reg_factor_feed = 0.\n",
        "    bin_rate_feed = 1.\n",
        "  elif epoch < EPOCHS * 8:\n",
        "    lr_feed = LR / 2.\n",
        "    #reg_factor_feed = adaptative_factor(epoch - EPOCHS * 4, EPOCHS * 4)\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    bin_rate_feed = 1.\n",
        "  else:\n",
        "    lr_feed = LR / 4.\n",
        "    reg_factor_feed = REG_CONSTANT\n",
        "    #bin_rate_feed = adaptative_factor(epoch - EPOCHS * 8, EPOCHS * 4)\n",
        "    bin_rate_feed = MAX_BIN_RATE + 1\n",
        "    \n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "    \n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "    \n",
        "    _, current_loss = sess.run([opt_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  lr_ph: lr_feed,\n",
        "                                  reg_factor_ph: reg_factor_feed,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "  \n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 9.161420022249223\n",
            "Testing Acc.: 0.23610000044107438\n",
            "Epoch 2 completed, average training loss is 2.4112015384435654\n",
            "Testing Acc.: 0.45229999542236327\n",
            "Epoch 3 completed, average training loss is 1.576145546833674\n",
            "Testing Acc.: 0.5922999998927116\n",
            "Epoch 4 completed, average training loss is 1.1791085133949915\n",
            "Testing Acc.: 0.6662000003457069\n",
            "Epoch 5 completed, average training loss is 0.964980432887872\n",
            "Testing Acc.: 0.7158999997377395\n",
            "Epoch 6 completed, average training loss is 0.8250529302656651\n",
            "Testing Acc.: 0.7603999996185302\n",
            "Epoch 7 completed, average training loss is 0.7164055945972602\n",
            "Testing Acc.: 0.7984000009298324\n",
            "Epoch 8 completed, average training loss is 0.6421147687236468\n",
            "Testing Acc.: 0.8165000003576278\n",
            "Epoch 9 completed, average training loss is 0.5740993201732636\n",
            "Testing Acc.: 0.8352999949455261\n",
            "Epoch 10 completed, average training loss is 0.5257400588442882\n",
            "Testing Acc.: 0.8485999989509583\n",
            "Epoch 11 completed, average training loss is 0.48573200665414334\n",
            "Testing Acc.: 0.8657999992370605\n",
            "Epoch 12 completed, average training loss is 0.44874197845657665\n",
            "Testing Acc.: 0.8685000044107437\n",
            "Epoch 13 completed, average training loss is 0.4141039082407951\n",
            "Testing Acc.: 0.8859000015258789\n",
            "Epoch 14 completed, average training loss is 0.39311905307074385\n",
            "Testing Acc.: 0.8800000023841857\n",
            "Epoch 15 completed, average training loss is 0.3647571221739054\n",
            "Testing Acc.: 0.9011000019311904\n",
            "Epoch 16 completed, average training loss is 0.34339856762439014\n",
            "Testing Acc.: 0.9016000020503998\n",
            "Epoch 17 completed, average training loss is 0.32776281422624987\n",
            "Testing Acc.: 0.8943000030517578\n",
            "Epoch 18 completed, average training loss is 0.31040669867148\n",
            "Testing Acc.: 0.9116999995708466\n",
            "Epoch 19 completed, average training loss is 0.2962268633271257\n",
            "Testing Acc.: 0.9179999989271164\n",
            "Epoch 20 completed, average training loss is 0.28436392726997534\n",
            "Testing Acc.: 0.9213999998569489\n",
            "Epoch 21 completed, average training loss is 0.2751694824794928\n",
            "Testing Acc.: 0.9212000030279159\n",
            "Epoch 22 completed, average training loss is 0.2632154382765293\n",
            "Testing Acc.: 0.9255000019073486\n",
            "Epoch 23 completed, average training loss is 0.2604853674893578\n",
            "Testing Acc.: 0.9278000009059906\n",
            "Epoch 24 completed, average training loss is 0.24875355646014213\n",
            "Testing Acc.: 0.932800001502037\n",
            "Epoch 25 completed, average training loss is 0.24338894680142403\n",
            "Testing Acc.: 0.9290000039339066\n",
            "Epoch 26 completed, average training loss is 0.23449369178464016\n",
            "Testing Acc.: 0.9258000040054322\n",
            "Epoch 27 completed, average training loss is 0.22849840673307578\n",
            "Testing Acc.: 0.9353000038862228\n",
            "Epoch 28 completed, average training loss is 0.22333174178376794\n",
            "Testing Acc.: 0.9431000036001206\n",
            "Epoch 29 completed, average training loss is 0.21637813752517104\n",
            "Testing Acc.: 0.9409000039100647\n",
            "Epoch 30 completed, average training loss is 0.21308192195991674\n",
            "Testing Acc.: 0.9405000019073486\n",
            "Epoch 31 completed, average training loss is 0.2090846991725266\n",
            "Testing Acc.: 0.9361000019311905\n",
            "Epoch 32 completed, average training loss is 0.2054098172672093\n",
            "Testing Acc.: 0.9398000019788743\n",
            "Epoch 33 completed, average training loss is 0.20160753705228368\n",
            "Testing Acc.: 0.934600002169609\n",
            "Epoch 34 completed, average training loss is 0.19831994141141573\n",
            "Testing Acc.: 0.9459000039100647\n",
            "Epoch 35 completed, average training loss is 0.19292966881766915\n",
            "Testing Acc.: 0.9445000022649765\n",
            "Epoch 36 completed, average training loss is 0.19322070232282082\n",
            "Testing Acc.: 0.9442000025510788\n",
            "Epoch 37 completed, average training loss is 0.18881493277226885\n",
            "Testing Acc.: 0.9431000036001206\n",
            "Epoch 38 completed, average training loss is 0.1874058728540937\n",
            "Testing Acc.: 0.9461000025272369\n",
            "Epoch 39 completed, average training loss is 0.18479426662127177\n",
            "Testing Acc.: 0.9431000012159347\n",
            "Epoch 40 completed, average training loss is 0.18112351781999073\n",
            "Testing Acc.: 0.9489000022411347\n",
            "Epoch 41 completed, average training loss is 0.1800402609569331\n",
            "Testing Acc.: 0.9476000016927719\n",
            "Epoch 42 completed, average training loss is 0.1766846698212127\n",
            "Testing Acc.: 0.9496000003814697\n",
            "Epoch 43 completed, average training loss is 0.17633052500585714\n",
            "Testing Acc.: 0.9515000021457672\n",
            "Epoch 44 completed, average training loss is 0.1770356587258478\n",
            "Testing Acc.: 0.9492000049352646\n",
            "Epoch 45 completed, average training loss is 0.1717022865700225\n",
            "Testing Acc.: 0.9507000035047531\n",
            "Epoch 46 completed, average training loss is 0.1731256698134045\n",
            "Testing Acc.: 0.9473000019788742\n",
            "Epoch 47 completed, average training loss is 0.17179628017668921\n",
            "Testing Acc.: 0.9494000023603439\n",
            "Epoch 48 completed, average training loss is 0.16828785926724474\n",
            "Testing Acc.: 0.9493000042438507\n",
            "Epoch 49 completed, average training loss is 0.16213498377085975\n",
            "Testing Acc.: 0.9521000009775161\n",
            "Epoch 50 completed, average training loss is 0.1613888905538867\n",
            "Testing Acc.: 0.9543000024557113\n",
            "Epoch 51 completed, average training loss is 0.1591904133061568\n",
            "Testing Acc.: 0.9517000019550323\n",
            "Epoch 52 completed, average training loss is 0.1591037542403986\n",
            "Testing Acc.: 0.9535000026226044\n",
            "Epoch 53 completed, average training loss is 0.15872854409739376\n",
            "Testing Acc.: 0.9541000032424927\n",
            "Epoch 54 completed, average training loss is 0.15448179507628082\n",
            "Testing Acc.: 0.9569000035524369\n",
            "Epoch 55 completed, average training loss is 0.1551569982431829\n",
            "Testing Acc.: 0.9542000043392181\n",
            "Epoch 56 completed, average training loss is 0.15131392525509\n",
            "Testing Acc.: 0.9561000031232834\n",
            "Epoch 57 completed, average training loss is 0.1489734594958524\n",
            "Testing Acc.: 0.9568000024557114\n",
            "Epoch 58 completed, average training loss is 0.15033299246492485\n",
            "Testing Acc.: 0.957200003862381\n",
            "Epoch 59 completed, average training loss is 0.14695656244953473\n",
            "Testing Acc.: 0.9579000037908554\n",
            "Epoch 60 completed, average training loss is 0.1489303816513469\n",
            "Testing Acc.: 0.9534000045061112\n",
            "Epoch 61 completed, average training loss is 0.14411339359047512\n",
            "Testing Acc.: 0.9566000032424927\n",
            "Epoch 62 completed, average training loss is 0.1448568355788787\n",
            "Testing Acc.: 0.9597000032663345\n",
            "Epoch 63 completed, average training loss is 0.14661838698511323\n",
            "Testing Acc.: 0.9571000021696091\n",
            "Epoch 64 completed, average training loss is 0.14586012322455644\n",
            "Testing Acc.: 0.958800003528595\n",
            "Epoch 65 completed, average training loss is 0.14177790470110874\n",
            "Testing Acc.: 0.9558000040054321\n",
            "Epoch 66 completed, average training loss is 0.14262541750445962\n",
            "Testing Acc.: 0.9570000058412552\n",
            "Epoch 67 completed, average training loss is 0.14151083596982061\n",
            "Testing Acc.: 0.956300003528595\n",
            "Epoch 68 completed, average training loss is 0.13887134168917933\n",
            "Testing Acc.: 0.9525000023841858\n",
            "Epoch 69 completed, average training loss is 0.14097851855990787\n",
            "Testing Acc.: 0.9601000034809113\n",
            "Epoch 70 completed, average training loss is 0.13777032282513876\n",
            "Testing Acc.: 0.9584000045061112\n",
            "Epoch 71 completed, average training loss is 0.13718622664920985\n",
            "Testing Acc.: 0.9556000030040741\n",
            "Epoch 72 completed, average training loss is 0.13569738724889854\n",
            "Testing Acc.: 0.9620000052452088\n",
            "Epoch 73 completed, average training loss is 0.13590448275208472\n",
            "Testing Acc.: 0.9578000032901763\n",
            "Epoch 74 completed, average training loss is 0.13493785607007644\n",
            "Testing Acc.: 0.957800001502037\n",
            "Epoch 75 completed, average training loss is 0.13335298020082215\n",
            "Testing Acc.: 0.9626000028848648\n",
            "Epoch 76 completed, average training loss is 0.13287909821917612\n",
            "Testing Acc.: 0.9576000058650971\n",
            "Epoch 77 completed, average training loss is 0.13371475561211507\n",
            "Testing Acc.: 0.9620000028610229\n",
            "Epoch 78 completed, average training loss is 0.13065955714012187\n",
            "Testing Acc.: 0.9633000057935714\n",
            "Epoch 79 completed, average training loss is 0.12881784295352797\n",
            "Testing Acc.: 0.9627000045776367\n",
            "Epoch 80 completed, average training loss is 0.1298664002912119\n",
            "Testing Acc.: 0.9617000061273575\n",
            "Epoch 81 completed, average training loss is 0.12888332074818512\n",
            "Testing Acc.: 0.9596000051498413\n",
            "Epoch 82 completed, average training loss is 0.12714510545444985\n",
            "Testing Acc.: 0.9599000060558319\n",
            "Epoch 83 completed, average training loss is 0.12804784626389543\n",
            "Testing Acc.: 0.9596000045537949\n",
            "Epoch 84 completed, average training loss is 0.12728260014516612\n",
            "Testing Acc.: 0.9595000046491623\n",
            "Epoch 85 completed, average training loss is 0.12560026943994065\n",
            "Testing Acc.: 0.9608000057935715\n",
            "Epoch 86 completed, average training loss is 0.12663635411299765\n",
            "Testing Acc.: 0.9634000045061112\n",
            "Epoch 87 completed, average training loss is 0.12453373883695652\n",
            "Testing Acc.: 0.9633000034093857\n",
            "Epoch 88 completed, average training loss is 0.12458821914469202\n",
            "Testing Acc.: 0.9641000086069107\n",
            "Epoch 89 completed, average training loss is 0.12407852455973625\n",
            "Testing Acc.: 0.9630000048875809\n",
            "Epoch 90 completed, average training loss is 0.12268701103205482\n",
            "Testing Acc.: 0.9625000077486038\n",
            "Epoch 91 completed, average training loss is 0.12329387597739697\n",
            "Testing Acc.: 0.9617000061273575\n",
            "Epoch 92 completed, average training loss is 0.12284459630648295\n",
            "Testing Acc.: 0.9641000044345855\n",
            "Epoch 93 completed, average training loss is 0.12270711658367266\n",
            "Testing Acc.: 0.9633000040054321\n",
            "Epoch 94 completed, average training loss is 0.12206257955792049\n",
            "Testing Acc.: 0.9638000059127808\n",
            "Epoch 95 completed, average training loss is 0.12330357614904643\n",
            "Testing Acc.: 0.9618000048398971\n",
            "Epoch 96 completed, average training loss is 0.12180246875000497\n",
            "Testing Acc.: 0.9628000044822693\n",
            "Epoch 97 completed, average training loss is 0.12004662577994168\n",
            "Testing Acc.: 0.9658000063896179\n",
            "Epoch 98 completed, average training loss is 0.12185140666551889\n",
            "Testing Acc.: 0.9614000064134598\n",
            "Epoch 99 completed, average training loss is 0.11881505159350733\n",
            "Testing Acc.: 0.9640000021457672\n",
            "Epoch 100 completed, average training loss is 0.1210590037703514\n",
            "Testing Acc.: 0.9629000037908554\n",
            "Epoch 101 completed, average training loss is 0.12012251914478839\n",
            "Testing Acc.: 0.9635000044107437\n",
            "Epoch 102 completed, average training loss is 0.1207585595594719\n",
            "Testing Acc.: 0.9617000031471252\n",
            "Epoch 103 completed, average training loss is 0.12304824205425878\n",
            "Testing Acc.: 0.9630000054836273\n",
            "Epoch 104 completed, average training loss is 0.12040031439314286\n",
            "Testing Acc.: 0.9635000032186508\n",
            "Epoch 105 completed, average training loss is 0.1225741599003474\n",
            "Testing Acc.: 0.9611000043153762\n",
            "Epoch 106 completed, average training loss is 0.12059716798675557\n",
            "Testing Acc.: 0.9633000028133393\n",
            "Epoch 107 completed, average training loss is 0.12164966693148017\n",
            "Testing Acc.: 0.964100005030632\n",
            "Epoch 108 completed, average training loss is 0.12001656772879263\n",
            "Testing Acc.: 0.9619000047445297\n",
            "Epoch 109 completed, average training loss is 0.11957110815060636\n",
            "Testing Acc.: 0.9573000007867813\n",
            "Epoch 110 completed, average training loss is 0.11910150636918843\n",
            "Testing Acc.: 0.9640000039339065\n",
            "Epoch 111 completed, average training loss is 0.11903306717673938\n",
            "Testing Acc.: 0.9642000037431717\n",
            "Epoch 112 completed, average training loss is 0.11910173357464374\n",
            "Testing Acc.: 0.9653000050783157\n",
            "Epoch 113 completed, average training loss is 0.11912677107689282\n",
            "Testing Acc.: 0.9583000046014786\n",
            "Epoch 114 completed, average training loss is 0.11930273963448902\n",
            "Testing Acc.: 0.960900000333786\n",
            "Epoch 115 completed, average training loss is 0.11933239214743177\n",
            "Testing Acc.: 0.9633000063896179\n",
            "Epoch 116 completed, average training loss is 0.11722492203582079\n",
            "Testing Acc.: 0.9627000051736831\n",
            "Epoch 117 completed, average training loss is 0.11712553395579259\n",
            "Testing Acc.: 0.962800001502037\n",
            "Epoch 118 completed, average training loss is 0.11733486603945494\n",
            "Testing Acc.: 0.9619000059366226\n",
            "Epoch 119 completed, average training loss is 0.11870644404242436\n",
            "Testing Acc.: 0.9640000033378601\n",
            "Epoch 120 completed, average training loss is 0.11657139900916566\n",
            "Testing Acc.: 0.9617000025510788\n",
            "Epoch 121 completed, average training loss is 0.11736421120663484\n",
            "Testing Acc.: 0.9638000059127808\n",
            "Epoch 122 completed, average training loss is 0.11658370654409131\n",
            "Testing Acc.: 0.9630000048875809\n",
            "Epoch 123 completed, average training loss is 0.11598598811154565\n",
            "Testing Acc.: 0.9661000043153762\n",
            "Epoch 124 completed, average training loss is 0.11567642637373259\n",
            "Testing Acc.: 0.9635000038146972\n",
            "Epoch 125 completed, average training loss is 0.11520161145521948\n",
            "Testing Acc.: 0.9642000043392182\n",
            "Epoch 126 completed, average training loss is 0.11501353568397463\n",
            "Testing Acc.: 0.9629000049829483\n",
            "Epoch 127 completed, average training loss is 0.11404069548938424\n",
            "Testing Acc.: 0.9662000048160553\n",
            "Epoch 128 completed, average training loss is 0.11493857812291632\n",
            "Testing Acc.: 0.9646000033617019\n",
            "Epoch 129 completed, average training loss is 0.11527685614302755\n",
            "Testing Acc.: 0.9646000027656555\n",
            "Epoch 130 completed, average training loss is 0.11499713017605245\n",
            "Testing Acc.: 0.9625000059604645\n",
            "Epoch 131 completed, average training loss is 0.11447743251609306\n",
            "Testing Acc.: 0.9642000037431717\n",
            "Epoch 132 completed, average training loss is 0.11562259131887306\n",
            "Testing Acc.: 0.96480000436306\n",
            "Epoch 133 completed, average training loss is 0.11446343997959048\n",
            "Testing Acc.: 0.9643000048398972\n",
            "Epoch 134 completed, average training loss is 0.11387829065322876\n",
            "Testing Acc.: 0.9593000048398972\n",
            "Epoch 135 completed, average training loss is 0.11235916066604355\n",
            "Testing Acc.: 0.9610000050067902\n",
            "Epoch 136 completed, average training loss is 0.11260106131744882\n",
            "Testing Acc.: 0.966000006198883\n",
            "Epoch 137 completed, average training loss is 0.11211513955456515\n",
            "Testing Acc.: 0.9662000048160553\n",
            "Epoch 138 completed, average training loss is 0.11232599533473452\n",
            "Testing Acc.: 0.9628000044822693\n",
            "Epoch 139 completed, average training loss is 0.1131389601714909\n",
            "Testing Acc.: 0.9652000063657761\n",
            "Epoch 140 completed, average training loss is 0.11008412120863795\n",
            "Testing Acc.: 0.9622000020742416\n",
            "Epoch 141 completed, average training loss is 0.1126869151679178\n",
            "Testing Acc.: 0.9641000038385391\n",
            "Epoch 142 completed, average training loss is 0.11149868379347026\n",
            "Testing Acc.: 0.9651000034809113\n",
            "Epoch 143 completed, average training loss is 0.11136511576827615\n",
            "Testing Acc.: 0.9646000057458878\n",
            "Epoch 144 completed, average training loss is 0.11116645318455994\n",
            "Testing Acc.: 0.9663000071048736\n",
            "Epoch 145 completed, average training loss is 0.1120472507737577\n",
            "Testing Acc.: 0.9648000055551529\n",
            "Epoch 146 completed, average training loss is 0.11274097529550393\n",
            "Testing Acc.: 0.9652000021934509\n",
            "Epoch 147 completed, average training loss is 0.10989124823672076\n",
            "Testing Acc.: 0.964100006222725\n",
            "Epoch 148 completed, average training loss is 0.1107830688295265\n",
            "Testing Acc.: 0.9639000052213669\n",
            "Epoch 149 completed, average training loss is 0.10890688094620904\n",
            "Testing Acc.: 0.9649000036716461\n",
            "Epoch 150 completed, average training loss is 0.11078478291009863\n",
            "Testing Acc.: 0.965600004196167\n",
            "Epoch 151 completed, average training loss is 0.11107611844781787\n",
            "Testing Acc.: 0.9654000061750412\n",
            "Epoch 152 completed, average training loss is 0.11032046669473251\n",
            "Testing Acc.: 0.965000005364418\n",
            "Epoch 153 completed, average training loss is 0.11082771852612495\n",
            "Testing Acc.: 0.9665000063180923\n",
            "Epoch 154 completed, average training loss is 0.10898932707651209\n",
            "Testing Acc.: 0.9661000043153762\n",
            "Epoch 155 completed, average training loss is 0.11040900052369883\n",
            "Testing Acc.: 0.9644000059366227\n",
            "Epoch 156 completed, average training loss is 0.1106377097281317\n",
            "Testing Acc.: 0.9657000052928925\n",
            "Epoch 157 completed, average training loss is 0.10890406028367579\n",
            "Testing Acc.: 0.9651000010967254\n",
            "Epoch 158 completed, average training loss is 0.10886761321686209\n",
            "Testing Acc.: 0.9623000037670135\n",
            "Epoch 159 completed, average training loss is 0.10928005377606799\n",
            "Testing Acc.: 0.9650000035762787\n",
            "Epoch 160 completed, average training loss is 0.10858569735040267\n",
            "Testing Acc.: 0.9652000063657761\n",
            "Epoch 161 completed, average training loss is 0.10889620604148756\n",
            "Testing Acc.: 0.9675000047683716\n",
            "Epoch 162 completed, average training loss is 0.1086414262590309\n",
            "Testing Acc.: 0.9632000029087067\n",
            "Epoch 163 completed, average training loss is 0.10886963596877952\n",
            "Testing Acc.: 0.9660000067949295\n",
            "Epoch 164 completed, average training loss is 0.10899537128551552\n",
            "Testing Acc.: 0.9661000043153762\n",
            "Epoch 165 completed, average training loss is 0.10804526411773016\n",
            "Testing Acc.: 0.9669000053405762\n",
            "Epoch 166 completed, average training loss is 0.10688628868820767\n",
            "Testing Acc.: 0.9678000068664551\n",
            "Epoch 167 completed, average training loss is 0.10761946280486882\n",
            "Testing Acc.: 0.963300005197525\n",
            "Epoch 168 completed, average training loss is 0.10748559017976125\n",
            "Testing Acc.: 0.9657000029087066\n",
            "Epoch 169 completed, average training loss is 0.10929439413361251\n",
            "Testing Acc.: 0.9670000058412552\n",
            "Epoch 170 completed, average training loss is 0.10684253897828361\n",
            "Testing Acc.: 0.961800001859665\n",
            "Epoch 171 completed, average training loss is 0.10658934390327583\n",
            "Testing Acc.: 0.9664000064134598\n",
            "Epoch 172 completed, average training loss is 0.10682662104411672\n",
            "Testing Acc.: 0.964000004529953\n",
            "Epoch 173 completed, average training loss is 0.10540626167474935\n",
            "Testing Acc.: 0.9676000046730041\n",
            "Epoch 174 completed, average training loss is 0.10647957015782594\n",
            "Testing Acc.: 0.9657000052928925\n",
            "Epoch 175 completed, average training loss is 0.10574906522718569\n",
            "Testing Acc.: 0.9685000038146973\n",
            "Epoch 176 completed, average training loss is 0.10428579677827657\n",
            "Testing Acc.: 0.9665000033378601\n",
            "Epoch 177 completed, average training loss is 0.1053696674418946\n",
            "Testing Acc.: 0.9679000049829483\n",
            "Epoch 178 completed, average training loss is 0.10545854314696043\n",
            "Testing Acc.: 0.9678000062704086\n",
            "Epoch 179 completed, average training loss is 0.10624439666513354\n",
            "Testing Acc.: 0.965400002002716\n",
            "Epoch 180 completed, average training loss is 0.10433027501373242\n",
            "Testing Acc.: 0.9644000059366227\n",
            "Epoch 181 completed, average training loss is 0.10527408411726355\n",
            "Testing Acc.: 0.9660000050067902\n",
            "Epoch 182 completed, average training loss is 0.10433114554577817\n",
            "Testing Acc.: 0.9684000015258789\n",
            "Epoch 183 completed, average training loss is 0.10345618107821793\n",
            "Testing Acc.: 0.9646000069379806\n",
            "Epoch 184 completed, average training loss is 0.10464784154823671\n",
            "Testing Acc.: 0.9654000049829483\n",
            "Epoch 185 completed, average training loss is 0.10391138630298277\n",
            "Testing Acc.: 0.9638000029325485\n",
            "Epoch 186 completed, average training loss is 0.10332146793759117\n",
            "Testing Acc.: 0.963900004029274\n",
            "Epoch 187 completed, average training loss is 0.10306707140679161\n",
            "Testing Acc.: 0.9610000038146973\n",
            "Epoch 188 completed, average training loss is 0.10232695711776614\n",
            "Testing Acc.: 0.9683000046014786\n",
            "Epoch 189 completed, average training loss is 0.10216735654820998\n",
            "Testing Acc.: 0.9662000042200088\n",
            "Epoch 190 completed, average training loss is 0.10128078383238365\n",
            "Testing Acc.: 0.9652000045776368\n",
            "Epoch 191 completed, average training loss is 0.10231676761526615\n",
            "Testing Acc.: 0.967200003862381\n",
            "Epoch 192 completed, average training loss is 0.10350558491113285\n",
            "Testing Acc.: 0.9664000052213669\n",
            "Epoch 193 completed, average training loss is 0.10151064566802233\n",
            "Testing Acc.: 0.9647000062465668\n",
            "Epoch 194 completed, average training loss is 0.10279108002005766\n",
            "Testing Acc.: 0.965700004696846\n",
            "Epoch 195 completed, average training loss is 0.10082526949079086\n",
            "Testing Acc.: 0.9665000033378601\n",
            "Epoch 196 completed, average training loss is 0.10134536822636922\n",
            "Testing Acc.: 0.9661000043153762\n",
            "Epoch 197 completed, average training loss is 0.10177113784011453\n",
            "Testing Acc.: 0.9672000032663345\n",
            "Epoch 198 completed, average training loss is 0.10215295128446693\n",
            "Testing Acc.: 0.9681000030040741\n",
            "Epoch 199 completed, average training loss is 0.10179261011847605\n",
            "Testing Acc.: 0.9653000038862228\n",
            "Epoch 200 completed, average training loss is 0.10205951154232025\n",
            "Testing Acc.: 0.9676000046730041\n",
            "Epoch 201 completed, average training loss is 0.26597531049512324\n",
            "Testing Acc.: 0.9055000019073486\n",
            "Epoch 202 completed, average training loss is 0.2794210023060441\n",
            "Testing Acc.: 0.9078000015020371\n",
            "Epoch 203 completed, average training loss is 0.26670389136920375\n",
            "Testing Acc.: 0.9182000011205673\n",
            "Epoch 204 completed, average training loss is 0.2588757167508205\n",
            "Testing Acc.: 0.9212000000476838\n",
            "Epoch 205 completed, average training loss is 0.2523585783566038\n",
            "Testing Acc.: 0.9214000004529953\n",
            "Epoch 206 completed, average training loss is 0.24607643225540718\n",
            "Testing Acc.: 0.9225000029802323\n",
            "Epoch 207 completed, average training loss is 0.24224042620509864\n",
            "Testing Acc.: 0.9263000053167343\n",
            "Epoch 208 completed, average training loss is 0.23718624432881674\n",
            "Testing Acc.: 0.9283000022172928\n",
            "Epoch 209 completed, average training loss is 0.23183586897949376\n",
            "Testing Acc.: 0.9296000015735626\n",
            "Epoch 210 completed, average training loss is 0.22979173014561335\n",
            "Testing Acc.: 0.9267000013589859\n",
            "Epoch 211 completed, average training loss is 0.22741855169336\n",
            "Testing Acc.: 0.9323000019788742\n",
            "Epoch 212 completed, average training loss is 0.22312217464049658\n",
            "Testing Acc.: 0.9303000026941299\n",
            "Epoch 213 completed, average training loss is 0.22172589854647715\n",
            "Testing Acc.: 0.9350000029802322\n",
            "Epoch 214 completed, average training loss is 0.2191008472815156\n",
            "Testing Acc.: 0.930400003194809\n",
            "Epoch 215 completed, average training loss is 0.21886953666806222\n",
            "Testing Acc.: 0.9302000027894973\n",
            "Epoch 216 completed, average training loss is 0.2157109787563483\n",
            "Testing Acc.: 0.9344000023603439\n",
            "Epoch 217 completed, average training loss is 0.21449228952328364\n",
            "Testing Acc.: 0.9359000021219254\n",
            "Epoch 218 completed, average training loss is 0.21243416146685679\n",
            "Testing Acc.: 0.9323000025749206\n",
            "Epoch 219 completed, average training loss is 0.21058828418453535\n",
            "Testing Acc.: 0.9362000012397766\n",
            "Epoch 220 completed, average training loss is 0.21049037906030812\n",
            "Testing Acc.: 0.9376000028848648\n",
            "Epoch 221 completed, average training loss is 0.20920980121319493\n",
            "Testing Acc.: 0.9364000046253205\n",
            "Epoch 222 completed, average training loss is 0.20784411958108345\n",
            "Testing Acc.: 0.9312000048160552\n",
            "Epoch 223 completed, average training loss is 0.20639677964150904\n",
            "Testing Acc.: 0.9364000052213669\n",
            "Epoch 224 completed, average training loss is 0.2051230697457989\n",
            "Testing Acc.: 0.935500003695488\n",
            "Epoch 225 completed, average training loss is 0.20347350005060436\n",
            "Testing Acc.: 0.9359000039100647\n",
            "Epoch 226 completed, average training loss is 0.2040983288983504\n",
            "Testing Acc.: 0.9357000029087067\n",
            "Epoch 227 completed, average training loss is 0.20301060729349654\n",
            "Testing Acc.: 0.9372000044584274\n",
            "Epoch 228 completed, average training loss is 0.2009286362056931\n",
            "Testing Acc.: 0.9373000013828278\n",
            "Epoch 229 completed, average training loss is 0.2013181127483646\n",
            "Testing Acc.: 0.9383000046014786\n",
            "Epoch 230 completed, average training loss is 0.19995503469059864\n",
            "Testing Acc.: 0.9389000082015991\n",
            "Epoch 231 completed, average training loss is 0.19963931368663906\n",
            "Testing Acc.: 0.9378000020980835\n",
            "Epoch 232 completed, average training loss is 0.19788306607554357\n",
            "Testing Acc.: 0.9364000046253205\n",
            "Epoch 233 completed, average training loss is 0.19907793575897814\n",
            "Testing Acc.: 0.9394000023603439\n",
            "Epoch 234 completed, average training loss is 0.19618252102285624\n",
            "Testing Acc.: 0.93700000166893\n",
            "Epoch 235 completed, average training loss is 0.1973410397209227\n",
            "Testing Acc.: 0.9363000023365021\n",
            "Epoch 236 completed, average training loss is 0.1971307966299355\n",
            "Testing Acc.: 0.9393000024557113\n",
            "Epoch 237 completed, average training loss is 0.1956941412513455\n",
            "Testing Acc.: 0.9374000030755997\n",
            "Epoch 238 completed, average training loss is 0.19448814521854121\n",
            "Testing Acc.: 0.9361000031232833\n",
            "Epoch 239 completed, average training loss is 0.19512448380390804\n",
            "Testing Acc.: 0.9407000035047531\n",
            "Epoch 240 completed, average training loss is 0.1950136109814048\n",
            "Testing Acc.: 0.9377000027894974\n",
            "Epoch 241 completed, average training loss is 0.1944300932995975\n",
            "Testing Acc.: 0.9422000002861023\n",
            "Epoch 242 completed, average training loss is 0.19280525832126538\n",
            "Testing Acc.: 0.938700003027916\n",
            "Epoch 243 completed, average training loss is 0.1933584771864116\n",
            "Testing Acc.: 0.9380000013113022\n",
            "Epoch 244 completed, average training loss is 0.1921834290958941\n",
            "Testing Acc.: 0.940200001001358\n",
            "Epoch 245 completed, average training loss is 0.19259700181583564\n",
            "Testing Acc.: 0.9386000037193298\n",
            "Epoch 246 completed, average training loss is 0.1921203470105926\n",
            "Testing Acc.: 0.9385000044107437\n",
            "Epoch 247 completed, average training loss is 0.19108380070577066\n",
            "Testing Acc.: 0.9419000047445297\n",
            "Epoch 248 completed, average training loss is 0.19049602824573716\n",
            "Testing Acc.: 0.9400000035762787\n",
            "Epoch 249 completed, average training loss is 0.1902437019596497\n",
            "Testing Acc.: 0.9418000024557114\n",
            "Epoch 250 completed, average training loss is 0.189878850368162\n",
            "Testing Acc.: 0.9393000030517578\n",
            "Epoch 251 completed, average training loss is 0.18913301792616646\n",
            "Testing Acc.: 0.9386000055074691\n",
            "Epoch 252 completed, average training loss is 0.18997932127366463\n",
            "Testing Acc.: 0.939200005531311\n",
            "Epoch 253 completed, average training loss is 0.1894497797638178\n",
            "Testing Acc.: 0.9416000026464463\n",
            "Epoch 254 completed, average training loss is 0.18820447903126478\n",
            "Testing Acc.: 0.9368000036478042\n",
            "Epoch 255 completed, average training loss is 0.18866288157800834\n",
            "Testing Acc.: 0.9425000023841857\n",
            "Epoch 256 completed, average training loss is 0.18851734545702736\n",
            "Testing Acc.: 0.9409000027179718\n",
            "Epoch 257 completed, average training loss is 0.18858947503070037\n",
            "Testing Acc.: 0.942200003862381\n",
            "Epoch 258 completed, average training loss is 0.1875195133127272\n",
            "Testing Acc.: 0.9374000030755997\n",
            "Epoch 259 completed, average training loss is 0.18750197731579343\n",
            "Testing Acc.: 0.9412000006437302\n",
            "Epoch 260 completed, average training loss is 0.18855756944045424\n",
            "Testing Acc.: 0.9420000022649765\n",
            "Epoch 261 completed, average training loss is 0.18931321413566668\n",
            "Testing Acc.: 0.9419000053405762\n",
            "Epoch 262 completed, average training loss is 0.18818656414126356\n",
            "Testing Acc.: 0.936500004529953\n",
            "Epoch 263 completed, average training loss is 0.1877601114536325\n",
            "Testing Acc.: 0.9371000063419342\n",
            "Epoch 264 completed, average training loss is 0.18846557607874273\n",
            "Testing Acc.: 0.9426000034809112\n",
            "Epoch 265 completed, average training loss is 0.1876119348096351\n",
            "Testing Acc.: 0.9397000014781952\n",
            "Epoch 266 completed, average training loss is 0.188305229259034\n",
            "Testing Acc.: 0.939900004863739\n",
            "Epoch 267 completed, average training loss is 0.18684437872841955\n",
            "Testing Acc.: 0.9410000032186508\n",
            "Epoch 268 completed, average training loss is 0.18753580143675208\n",
            "Testing Acc.: 0.9411000055074692\n",
            "Epoch 269 completed, average training loss is 0.18644968546306093\n",
            "Testing Acc.: 0.9403000044822692\n",
            "Epoch 270 completed, average training loss is 0.18662037405495843\n",
            "Testing Acc.: 0.9434000033140183\n",
            "Epoch 271 completed, average training loss is 0.18652171493818362\n",
            "Testing Acc.: 0.9420000004768372\n",
            "Epoch 272 completed, average training loss is 0.18609754241382082\n",
            "Testing Acc.: 0.9397000050544739\n",
            "Epoch 273 completed, average training loss is 0.1869111635411779\n",
            "Testing Acc.: 0.9437000024318695\n",
            "Epoch 274 completed, average training loss is 0.18467322283734877\n",
            "Testing Acc.: 0.9393000012636185\n",
            "Epoch 275 completed, average training loss is 0.18459436644489566\n",
            "Testing Acc.: 0.939799998998642\n",
            "Epoch 276 completed, average training loss is 0.18526883471757175\n",
            "Testing Acc.: 0.9385000050067902\n",
            "Epoch 277 completed, average training loss is 0.18502058170114954\n",
            "Testing Acc.: 0.9386000061035156\n",
            "Epoch 278 completed, average training loss is 0.1838873321687182\n",
            "Testing Acc.: 0.9432000041007995\n",
            "Epoch 279 completed, average training loss is 0.18449595278749864\n",
            "Testing Acc.: 0.9425000029802323\n",
            "Epoch 280 completed, average training loss is 0.18323551216162742\n",
            "Testing Acc.: 0.9417000025510788\n",
            "Epoch 281 completed, average training loss is 0.18443843624244133\n",
            "Testing Acc.: 0.9421000045537948\n",
            "Epoch 282 completed, average training loss is 0.18347741270437837\n",
            "Testing Acc.: 0.9419000041484833\n",
            "Epoch 283 completed, average training loss is 0.18393520497406524\n",
            "Testing Acc.: 0.9435000038146972\n",
            "Epoch 284 completed, average training loss is 0.18258459461232027\n",
            "Testing Acc.: 0.9436000007390976\n",
            "Epoch 285 completed, average training loss is 0.18305143690357606\n",
            "Testing Acc.: 0.9443000018596649\n",
            "Epoch 286 completed, average training loss is 0.1817869017397364\n",
            "Testing Acc.: 0.9428000020980835\n",
            "Epoch 287 completed, average training loss is 0.18282975839450955\n",
            "Testing Acc.: 0.9379000020027161\n",
            "Epoch 288 completed, average training loss is 0.18284898754830162\n",
            "Testing Acc.: 0.9442000013589859\n",
            "Epoch 289 completed, average training loss is 0.18191674091542762\n",
            "Testing Acc.: 0.9426000010967255\n",
            "Epoch 290 completed, average training loss is 0.18640968337034186\n",
            "Testing Acc.: 0.9430000007152557\n",
            "Epoch 291 completed, average training loss is 0.1854439629490177\n",
            "Testing Acc.: 0.9427000051736831\n",
            "Epoch 292 completed, average training loss is 0.18542905199651916\n",
            "Testing Acc.: 0.9402000051736832\n",
            "Epoch 293 completed, average training loss is 0.18563909030829867\n",
            "Testing Acc.: 0.9438000035285949\n",
            "Epoch 294 completed, average training loss is 0.18549911610161265\n",
            "Testing Acc.: 0.9384000033140183\n",
            "Epoch 295 completed, average training loss is 0.18512320268899202\n",
            "Testing Acc.: 0.9396000027656555\n",
            "Epoch 296 completed, average training loss is 0.18529013741140565\n",
            "Testing Acc.: 0.9411000019311905\n",
            "Epoch 297 completed, average training loss is 0.18531004442522922\n",
            "Testing Acc.: 0.941800001859665\n",
            "Epoch 298 completed, average training loss is 0.1842755880455176\n",
            "Testing Acc.: 0.9388000059127808\n",
            "Epoch 299 completed, average training loss is 0.1851927578325073\n",
            "Testing Acc.: 0.9435000061988831\n",
            "Epoch 300 completed, average training loss is 0.18435641024261712\n",
            "Testing Acc.: 0.9426000016927719\n",
            "Epoch 301 completed, average training loss is 0.18348630481710038\n",
            "Testing Acc.: 0.9418000036478043\n",
            "Epoch 302 completed, average training loss is 0.184417267087847\n",
            "Testing Acc.: 0.9406000030040741\n",
            "Epoch 303 completed, average training loss is 0.1835777779171864\n",
            "Testing Acc.: 0.9412000000476837\n",
            "Epoch 304 completed, average training loss is 0.1828562209382653\n",
            "Testing Acc.: 0.9403000032901764\n",
            "Epoch 305 completed, average training loss is 0.18327516532192628\n",
            "Testing Acc.: 0.9438999998569488\n",
            "Epoch 306 completed, average training loss is 0.18190507801249622\n",
            "Testing Acc.: 0.9390000027418136\n",
            "Epoch 307 completed, average training loss is 0.18185670549670854\n",
            "Testing Acc.: 0.9457000023126603\n",
            "Epoch 308 completed, average training loss is 0.18186408115550876\n",
            "Testing Acc.: 0.9416000032424927\n",
            "Epoch 309 completed, average training loss is 0.1832532626700898\n",
            "Testing Acc.: 0.940200006365776\n",
            "Epoch 310 completed, average training loss is 0.18240060146898032\n",
            "Testing Acc.: 0.9443000036478043\n",
            "Epoch 311 completed, average training loss is 0.18132405628760656\n",
            "Testing Acc.: 0.9445000022649765\n",
            "Epoch 312 completed, average training loss is 0.18223995512351393\n",
            "Testing Acc.: 0.9405000042915345\n",
            "Epoch 313 completed, average training loss is 0.18188638715073466\n",
            "Testing Acc.: 0.9414000058174133\n",
            "Epoch 314 completed, average training loss is 0.1812066425445179\n",
            "Testing Acc.: 0.9457000023126603\n",
            "Epoch 315 completed, average training loss is 0.18087801037356258\n",
            "Testing Acc.: 0.9424000042676925\n",
            "Epoch 316 completed, average training loss is 0.18048776044820747\n",
            "Testing Acc.: 0.9420000022649765\n",
            "Epoch 317 completed, average training loss is 0.18007385940601428\n",
            "Testing Acc.: 0.9408000016212463\n",
            "Epoch 318 completed, average training loss is 0.1794236106487612\n",
            "Testing Acc.: 0.942100003361702\n",
            "Epoch 319 completed, average training loss is 0.18060335057477156\n",
            "Testing Acc.: 0.9414000022411346\n",
            "Epoch 320 completed, average training loss is 0.18054865600541234\n",
            "Testing Acc.: 0.9454000049829483\n",
            "Epoch 321 completed, average training loss is 0.17989782804002363\n",
            "Testing Acc.: 0.9434000039100647\n",
            "Epoch 322 completed, average training loss is 0.17973856434226035\n",
            "Testing Acc.: 0.938600002527237\n",
            "Epoch 323 completed, average training loss is 0.17985578340788683\n",
            "Testing Acc.: 0.942700002193451\n",
            "Epoch 324 completed, average training loss is 0.17894755536069473\n",
            "Testing Acc.: 0.9443000036478043\n",
            "Epoch 325 completed, average training loss is 0.17897452550629775\n",
            "Testing Acc.: 0.9439000058174133\n",
            "Epoch 326 completed, average training loss is 0.17831919446898004\n",
            "Testing Acc.: 0.9453000015020371\n",
            "Epoch 327 completed, average training loss is 0.1799322763644159\n",
            "Testing Acc.: 0.9449000036716462\n",
            "Epoch 328 completed, average training loss is 0.1796882587360839\n",
            "Testing Acc.: 0.9438000029325485\n",
            "Epoch 329 completed, average training loss is 0.17821010343109567\n",
            "Testing Acc.: 0.9415000051259994\n",
            "Epoch 330 completed, average training loss is 0.17733731013101836\n",
            "Testing Acc.: 0.9441000026464462\n",
            "Epoch 331 completed, average training loss is 0.1788543440339466\n",
            "Testing Acc.: 0.9442000043392181\n",
            "Epoch 332 completed, average training loss is 0.17680665854364633\n",
            "Testing Acc.: 0.9420000028610229\n",
            "Epoch 333 completed, average training loss is 0.17656215025112032\n",
            "Testing Acc.: 0.9409000015258789\n",
            "Epoch 334 completed, average training loss is 0.17749237308899563\n",
            "Testing Acc.: 0.9436000037193298\n",
            "Epoch 335 completed, average training loss is 0.17781320800383885\n",
            "Testing Acc.: 0.9450000017881394\n",
            "Epoch 336 completed, average training loss is 0.1774451300315559\n",
            "Testing Acc.: 0.9437000042200089\n",
            "Epoch 337 completed, average training loss is 0.17644188673856356\n",
            "Testing Acc.: 0.9393000036478043\n",
            "Epoch 338 completed, average training loss is 0.1774358603854974\n",
            "Testing Acc.: 0.9417000043392182\n",
            "Epoch 339 completed, average training loss is 0.17763388595233362\n",
            "Testing Acc.: 0.9375000035762787\n",
            "Epoch 340 completed, average training loss is 0.1772373571122686\n",
            "Testing Acc.: 0.9463000029325486\n",
            "Epoch 341 completed, average training loss is 0.17628525950014592\n",
            "Testing Acc.: 0.942700002193451\n",
            "Epoch 342 completed, average training loss is 0.17750398432525497\n",
            "Testing Acc.: 0.9420000034570694\n",
            "Epoch 343 completed, average training loss is 0.17630202386528254\n",
            "Testing Acc.: 0.9470000022649765\n",
            "Epoch 344 completed, average training loss is 0.17619135626281301\n",
            "Testing Acc.: 0.9457000029087067\n",
            "Epoch 345 completed, average training loss is 0.17587584113081298\n",
            "Testing Acc.: 0.9440999996662139\n",
            "Epoch 346 completed, average training loss is 0.17666323391720654\n",
            "Testing Acc.: 0.9449000030755996\n",
            "Epoch 347 completed, average training loss is 0.17484413156285883\n",
            "Testing Acc.: 0.9431000012159347\n",
            "Epoch 348 completed, average training loss is 0.17519747604926428\n",
            "Testing Acc.: 0.9459000009298325\n",
            "Epoch 349 completed, average training loss is 0.17480960431819162\n",
            "Testing Acc.: 0.9428000020980835\n",
            "Epoch 350 completed, average training loss is 0.17605854324996473\n",
            "Testing Acc.: 0.9461000019311905\n",
            "Epoch 351 completed, average training loss is 0.17508524858703217\n",
            "Testing Acc.: 0.940700004696846\n",
            "Epoch 352 completed, average training loss is 0.1754570110825201\n",
            "Testing Acc.: 0.9444000029563904\n",
            "Epoch 353 completed, average training loss is 0.17481430796285471\n",
            "Testing Acc.: 0.9441000032424927\n",
            "Epoch 354 completed, average training loss is 0.1742913993448019\n",
            "Testing Acc.: 0.9437000030279159\n",
            "Epoch 355 completed, average training loss is 0.17474847345923383\n",
            "Testing Acc.: 0.9443999999761581\n",
            "Epoch 356 completed, average training loss is 0.17437876015280684\n",
            "Testing Acc.: 0.9458000046014786\n",
            "Epoch 357 completed, average training loss is 0.17472273603081703\n",
            "Testing Acc.: 0.9471000009775161\n",
            "Epoch 358 completed, average training loss is 0.17509214229881764\n",
            "Testing Acc.: 0.9422000044584274\n",
            "Epoch 359 completed, average training loss is 0.17509243500729402\n",
            "Testing Acc.: 0.9478000026941299\n",
            "Epoch 360 completed, average training loss is 0.1737821639639636\n",
            "Testing Acc.: 0.9453000020980835\n",
            "Epoch 361 completed, average training loss is 0.17284473260243735\n",
            "Testing Acc.: 0.9476000022888184\n",
            "Epoch 362 completed, average training loss is 0.17363632068037987\n",
            "Testing Acc.: 0.9465000033378601\n",
            "Epoch 363 completed, average training loss is 0.1748075254727155\n",
            "Testing Acc.: 0.9455000042915345\n",
            "Epoch 364 completed, average training loss is 0.17268471290667853\n",
            "Testing Acc.: 0.9456000006198884\n",
            "Epoch 365 completed, average training loss is 0.17387836126610637\n",
            "Testing Acc.: 0.9444000053405762\n",
            "Epoch 366 completed, average training loss is 0.1726869063079357\n",
            "Testing Acc.: 0.939700003862381\n",
            "Epoch 367 completed, average training loss is 0.1744679677983125\n",
            "Testing Acc.: 0.9469000041484833\n",
            "Epoch 368 completed, average training loss is 0.17270153978218636\n",
            "Testing Acc.: 0.9449000012874603\n",
            "Epoch 369 completed, average training loss is 0.17235654558365543\n",
            "Testing Acc.: 0.94530000269413\n",
            "Epoch 370 completed, average training loss is 0.17318257273485263\n",
            "Testing Acc.: 0.9467000019550323\n",
            "Epoch 371 completed, average training loss is 0.172769998361667\n",
            "Testing Acc.: 0.9459000051021575\n",
            "Epoch 372 completed, average training loss is 0.1722391163247327\n",
            "Testing Acc.: 0.9463000017404556\n",
            "Epoch 373 completed, average training loss is 0.17232507682715853\n",
            "Testing Acc.: 0.9446000039577485\n",
            "Epoch 374 completed, average training loss is 0.1706359296788772\n",
            "Testing Acc.: 0.946300002336502\n",
            "Epoch 375 completed, average training loss is 0.17195302582035463\n",
            "Testing Acc.: 0.943400000333786\n",
            "Epoch 376 completed, average training loss is 0.1715425573848188\n",
            "Testing Acc.: 0.9465000033378601\n",
            "Epoch 377 completed, average training loss is 0.1724034599152704\n",
            "Testing Acc.: 0.9488000011444092\n",
            "Epoch 378 completed, average training loss is 0.17089131706704697\n",
            "Testing Acc.: 0.9452000027894973\n",
            "Epoch 379 completed, average training loss is 0.17112270629654328\n",
            "Testing Acc.: 0.9490000027418136\n",
            "Epoch 380 completed, average training loss is 0.17129765465855598\n",
            "Testing Acc.: 0.948100003004074\n",
            "Epoch 381 completed, average training loss is 0.17136320592835547\n",
            "Testing Acc.: 0.9465000015497208\n",
            "Epoch 382 completed, average training loss is 0.17123572598521908\n",
            "Testing Acc.: 0.9457000011205673\n",
            "Epoch 383 completed, average training loss is 0.17094609121171137\n",
            "Testing Acc.: 0.9473000019788742\n",
            "Epoch 384 completed, average training loss is 0.17166288728515308\n",
            "Testing Acc.: 0.9450000017881394\n",
            "Epoch 385 completed, average training loss is 0.17056211084748307\n",
            "Testing Acc.: 0.9475000029802323\n",
            "Epoch 386 completed, average training loss is 0.1715581550511221\n",
            "Testing Acc.: 0.9482000029087067\n",
            "Epoch 387 completed, average training loss is 0.17123007401823997\n",
            "Testing Acc.: 0.9482000023126602\n",
            "Epoch 388 completed, average training loss is 0.1711265610406796\n",
            "Testing Acc.: 0.9469000029563904\n",
            "Epoch 389 completed, average training loss is 0.16908277727042637\n",
            "Testing Acc.: 0.9461000007390976\n",
            "Epoch 390 completed, average training loss is 0.17030878180637957\n",
            "Testing Acc.: 0.9469000035524369\n",
            "Epoch 391 completed, average training loss is 0.17039945150415103\n",
            "Testing Acc.: 0.9437000012397766\n",
            "Epoch 392 completed, average training loss is 0.17063067654768627\n",
            "Testing Acc.: 0.947200002670288\n",
            "Epoch 393 completed, average training loss is 0.1677122259264191\n",
            "Testing Acc.: 0.9438000005483628\n",
            "Epoch 394 completed, average training loss is 0.17088806006436547\n",
            "Testing Acc.: 0.9480000048875808\n",
            "Epoch 395 completed, average training loss is 0.1700118401305129\n",
            "Testing Acc.: 0.9460000020265579\n",
            "Epoch 396 completed, average training loss is 0.16910355720669032\n",
            "Testing Acc.: 0.9461000055074692\n",
            "Epoch 397 completed, average training loss is 0.16838109999895096\n",
            "Testing Acc.: 0.9443000018596649\n",
            "Epoch 398 completed, average training loss is 0.1691997942638894\n",
            "Testing Acc.: 0.9463000005483627\n",
            "Epoch 399 completed, average training loss is 0.16946610692267616\n",
            "Testing Acc.: 0.9472000050544739\n",
            "Epoch 400 completed, average training loss is 0.1703133923187852\n",
            "Testing Acc.: 0.9426000046730042\n",
            "Epoch 401 completed, average training loss is 0.26247299026697873\n",
            "Testing Acc.: 0.9338000053167343\n",
            "Epoch 402 completed, average training loss is 0.20728355415165425\n",
            "Testing Acc.: 0.9431000012159347\n",
            "Epoch 403 completed, average training loss is 0.1966182157645623\n",
            "Testing Acc.: 0.9440000033378602\n",
            "Epoch 404 completed, average training loss is 0.19001473447307946\n",
            "Testing Acc.: 0.9446000027656555\n",
            "Epoch 405 completed, average training loss is 0.1854248877738913\n",
            "Testing Acc.: 0.9435000038146972\n",
            "Epoch 406 completed, average training loss is 0.18090056276569763\n",
            "Testing Acc.: 0.94450000166893\n",
            "Epoch 407 completed, average training loss is 0.17871272789935272\n",
            "Testing Acc.: 0.9453000038862228\n",
            "Epoch 408 completed, average training loss is 0.17624270988007387\n",
            "Testing Acc.: 0.9458999997377395\n",
            "Epoch 409 completed, average training loss is 0.1735095936494569\n",
            "Testing Acc.: 0.9472000056505203\n",
            "Epoch 410 completed, average training loss is 0.16709499847143888\n",
            "Testing Acc.: 0.9494999998807907\n",
            "Epoch 411 completed, average training loss is 0.1627995802462101\n",
            "Testing Acc.: 0.9501000010967254\n",
            "Epoch 412 completed, average training loss is 0.16281673081840078\n",
            "Testing Acc.: 0.9488000041246414\n",
            "Epoch 413 completed, average training loss is 0.16015643364439408\n",
            "Testing Acc.: 0.9522000014781952\n",
            "Epoch 414 completed, average training loss is 0.15934234368304412\n",
            "Testing Acc.: 0.9489000028371811\n",
            "Epoch 415 completed, average training loss is 0.15892026827981073\n",
            "Testing Acc.: 0.9479000037908554\n",
            "Epoch 416 completed, average training loss is 0.15818328601308168\n",
            "Testing Acc.: 0.9513000059127807\n",
            "Epoch 417 completed, average training loss is 0.1582667630041639\n",
            "Testing Acc.: 0.9515000033378601\n",
            "Epoch 418 completed, average training loss is 0.15627494624815882\n",
            "Testing Acc.: 0.9507000035047531\n",
            "Epoch 419 completed, average training loss is 0.155511083137244\n",
            "Testing Acc.: 0.9526000022888184\n",
            "Epoch 420 completed, average training loss is 0.15562966362262765\n",
            "Testing Acc.: 0.9505000025033951\n",
            "Epoch 421 completed, average training loss is 0.1545175723017504\n",
            "Testing Acc.: 0.9531000000238419\n",
            "Epoch 422 completed, average training loss is 0.1545530499642094\n",
            "Testing Acc.: 0.9524000030755997\n",
            "Epoch 423 completed, average training loss is 0.15449433716634908\n",
            "Testing Acc.: 0.9529000014066696\n",
            "Epoch 424 completed, average training loss is 0.153474858161062\n",
            "Testing Acc.: 0.9517000043392181\n",
            "Epoch 425 completed, average training loss is 0.15260621870557467\n",
            "Testing Acc.: 0.9508000046014786\n",
            "Epoch 426 completed, average training loss is 0.15239679646057389\n",
            "Testing Acc.: 0.9516000008583069\n",
            "Epoch 427 completed, average training loss is 0.15146582858636976\n",
            "Testing Acc.: 0.9532000017166138\n",
            "Epoch 428 completed, average training loss is 0.15157118475995957\n",
            "Testing Acc.: 0.9535000044107437\n",
            "Epoch 429 completed, average training loss is 0.15106244933170576\n",
            "Testing Acc.: 0.9522000050544739\n",
            "Epoch 430 completed, average training loss is 0.1508855741440008\n",
            "Testing Acc.: 0.9524000042676926\n",
            "Epoch 431 completed, average training loss is 0.15191713819901148\n",
            "Testing Acc.: 0.9536000037193298\n",
            "Epoch 432 completed, average training loss is 0.1512082502183815\n",
            "Testing Acc.: 0.9527000039815903\n",
            "Epoch 433 completed, average training loss is 0.1504316887507836\n",
            "Testing Acc.: 0.9531000047922135\n",
            "Epoch 434 completed, average training loss is 0.1504938793927431\n",
            "Testing Acc.: 0.951500004529953\n",
            "Epoch 435 completed, average training loss is 0.1494566302591314\n",
            "Testing Acc.: 0.9538000017404556\n",
            "Epoch 436 completed, average training loss is 0.1487256125671168\n",
            "Testing Acc.: 0.9546000033617019\n",
            "Epoch 437 completed, average training loss is 0.14816897414935132\n",
            "Testing Acc.: 0.9541000020503998\n",
            "Epoch 438 completed, average training loss is 0.14860020498124263\n",
            "Testing Acc.: 0.952200003862381\n",
            "Epoch 439 completed, average training loss is 0.14852569898590445\n",
            "Testing Acc.: 0.9514000022411346\n",
            "Epoch 440 completed, average training loss is 0.14748658843028048\n",
            "Testing Acc.: 0.9558000040054321\n",
            "Epoch 441 completed, average training loss is 0.14675490434126306\n",
            "Testing Acc.: 0.9538000011444092\n",
            "Epoch 442 completed, average training loss is 0.1475892841629684\n",
            "Testing Acc.: 0.9533000040054321\n",
            "Epoch 443 completed, average training loss is 0.14787618930762012\n",
            "Testing Acc.: 0.9516000020503997\n",
            "Epoch 444 completed, average training loss is 0.14811920743746063\n",
            "Testing Acc.: 0.9529000031948089\n",
            "Epoch 445 completed, average training loss is 0.14644803188741207\n",
            "Testing Acc.: 0.9521000027656555\n",
            "Epoch 446 completed, average training loss is 0.1463786341715604\n",
            "Testing Acc.: 0.9525000041723252\n",
            "Epoch 447 completed, average training loss is 0.14583999832781652\n",
            "Testing Acc.: 0.9526000034809112\n",
            "Epoch 448 completed, average training loss is 0.14694053948546448\n",
            "Testing Acc.: 0.9547000020742417\n",
            "Epoch 449 completed, average training loss is 0.14655527128527562\n",
            "Testing Acc.: 0.9525000035762787\n",
            "Epoch 450 completed, average training loss is 0.14518897915259005\n",
            "Testing Acc.: 0.9549000036716461\n",
            "Epoch 451 completed, average training loss is 0.1452899558438609\n",
            "Testing Acc.: 0.9548000049591064\n",
            "Epoch 452 completed, average training loss is 0.1455091681269308\n",
            "Testing Acc.: 0.954700003862381\n",
            "Epoch 453 completed, average training loss is 0.14523309448113045\n",
            "Testing Acc.: 0.9534000027179718\n",
            "Epoch 454 completed, average training loss is 0.1440577451636394\n",
            "Testing Acc.: 0.9539000022411347\n",
            "Epoch 455 completed, average training loss is 0.14416840748861431\n",
            "Testing Acc.: 0.9538000029325485\n",
            "Epoch 456 completed, average training loss is 0.14499844392761588\n",
            "Testing Acc.: 0.951600005030632\n",
            "Epoch 457 completed, average training loss is 0.14452593608448902\n",
            "Testing Acc.: 0.9524000024795533\n",
            "Epoch 458 completed, average training loss is 0.14398761141424377\n",
            "Testing Acc.: 0.9559000045061111\n",
            "Epoch 459 completed, average training loss is 0.14402538017680247\n",
            "Testing Acc.: 0.9551000034809113\n",
            "Epoch 460 completed, average training loss is 0.1435490115887175\n",
            "Testing Acc.: 0.95530000269413\n",
            "Epoch 461 completed, average training loss is 0.14391029658416907\n",
            "Testing Acc.: 0.9516000032424927\n",
            "Epoch 462 completed, average training loss is 0.14354260652636489\n",
            "Testing Acc.: 0.955900006890297\n",
            "Epoch 463 completed, average training loss is 0.14262004988268018\n",
            "Testing Acc.: 0.9550000041723251\n",
            "Epoch 464 completed, average training loss is 0.1429843188325564\n",
            "Testing Acc.: 0.9558000040054321\n",
            "Epoch 465 completed, average training loss is 0.1419921599732091\n",
            "Testing Acc.: 0.9550000023841858\n",
            "Epoch 466 completed, average training loss is 0.1432126348496725\n",
            "Testing Acc.: 0.9556000024080277\n",
            "Epoch 467 completed, average training loss is 0.142334493547678\n",
            "Testing Acc.: 0.954300000667572\n",
            "Epoch 468 completed, average training loss is 0.1434577718594422\n",
            "Testing Acc.: 0.952100003361702\n",
            "Epoch 469 completed, average training loss is 0.142791415983811\n",
            "Testing Acc.: 0.9547000014781952\n",
            "Epoch 470 completed, average training loss is 0.1423110437206924\n",
            "Testing Acc.: 0.9544000047445297\n",
            "Epoch 471 completed, average training loss is 0.1419049684330821\n",
            "Testing Acc.: 0.9544000041484832\n",
            "Epoch 472 completed, average training loss is 0.14247984648682177\n",
            "Testing Acc.: 0.9543000018596649\n",
            "Epoch 473 completed, average training loss is 0.14171990547329189\n",
            "Testing Acc.: 0.9572000044584275\n",
            "Epoch 474 completed, average training loss is 0.14129839601305624\n",
            "Testing Acc.: 0.9528000009059906\n",
            "Epoch 475 completed, average training loss is 0.14218396057374774\n",
            "Testing Acc.: 0.9541000002622604\n",
            "Epoch 476 completed, average training loss is 0.1419230540903906\n",
            "Testing Acc.: 0.955400003194809\n",
            "Epoch 477 completed, average training loss is 0.14124575968831776\n",
            "Testing Acc.: 0.9544000029563904\n",
            "Epoch 478 completed, average training loss is 0.14110162892999747\n",
            "Testing Acc.: 0.9537000006437302\n",
            "Epoch 479 completed, average training loss is 0.14208678485515217\n",
            "Testing Acc.: 0.9541000014543534\n",
            "Epoch 480 completed, average training loss is 0.1406539751123637\n",
            "Testing Acc.: 0.9550000017881394\n",
            "Epoch 481 completed, average training loss is 0.140593367703259\n",
            "Testing Acc.: 0.9559000009298324\n",
            "Epoch 482 completed, average training loss is 0.1407269785258298\n",
            "Testing Acc.: 0.9538000047206878\n",
            "Epoch 483 completed, average training loss is 0.14092611911396186\n",
            "Testing Acc.: 0.9552000027894973\n",
            "Epoch 484 completed, average training loss is 0.14066407817726334\n",
            "Testing Acc.: 0.9548000007867813\n",
            "Epoch 485 completed, average training loss is 0.14063413865243396\n",
            "Testing Acc.: 0.9555000042915345\n",
            "Epoch 486 completed, average training loss is 0.1402770893772443\n",
            "Testing Acc.: 0.9526000022888184\n",
            "Epoch 487 completed, average training loss is 0.14047195612142482\n",
            "Testing Acc.: 0.9535000014305115\n",
            "Epoch 488 completed, average training loss is 0.1400527998122076\n",
            "Testing Acc.: 0.952399999499321\n",
            "Epoch 489 completed, average training loss is 0.13979699885783095\n",
            "Testing Acc.: 0.9561000031232834\n",
            "Epoch 490 completed, average training loss is 0.14019959050541123\n",
            "Testing Acc.: 0.9553000032901764\n",
            "Epoch 491 completed, average training loss is 0.14041171866779525\n",
            "Testing Acc.: 0.9572000050544739\n",
            "Epoch 492 completed, average training loss is 0.1407025618106127\n",
            "Testing Acc.: 0.9557000011205673\n",
            "Epoch 493 completed, average training loss is 0.1396524883961926\n",
            "Testing Acc.: 0.9552000045776368\n",
            "Epoch 494 completed, average training loss is 0.1394626897604515\n",
            "Testing Acc.: 0.9549000012874603\n",
            "Epoch 495 completed, average training loss is 0.14015721336007117\n",
            "Testing Acc.: 0.9567000019550324\n",
            "Epoch 496 completed, average training loss is 0.13965402996788423\n",
            "Testing Acc.: 0.9557000029087067\n",
            "Epoch 497 completed, average training loss is 0.13949810371734203\n",
            "Testing Acc.: 0.9566000020503997\n",
            "Epoch 498 completed, average training loss is 0.14009326668145755\n",
            "Testing Acc.: 0.9550000029802322\n",
            "Epoch 499 completed, average training loss is 0.1390757690432171\n",
            "Testing Acc.: 0.9555000013113022\n",
            "Epoch 500 completed, average training loss is 0.13878619825467467\n",
            "Testing Acc.: 0.9550000017881394\n",
            "Epoch 501 completed, average training loss is 0.13909164672096572\n",
            "Testing Acc.: 0.9540000009536743\n",
            "Epoch 502 completed, average training loss is 0.13854524087160824\n",
            "Testing Acc.: 0.9550000029802322\n",
            "Epoch 503 completed, average training loss is 0.13816260358629126\n",
            "Testing Acc.: 0.955\n",
            "Epoch 504 completed, average training loss is 0.13858561845806738\n",
            "Testing Acc.: 0.9567000037431717\n",
            "Epoch 505 completed, average training loss is 0.13853503859291474\n",
            "Testing Acc.: 0.9549000030755996\n",
            "Epoch 506 completed, average training loss is 0.13800359052295486\n",
            "Testing Acc.: 0.9553000050783157\n",
            "Epoch 507 completed, average training loss is 0.1384782691175739\n",
            "Testing Acc.: 0.9569000017642975\n",
            "Epoch 508 completed, average training loss is 0.13905289112590252\n",
            "Testing Acc.: 0.9566000038385392\n",
            "Epoch 509 completed, average training loss is 0.13715479076839984\n",
            "Testing Acc.: 0.9557000023126602\n",
            "Epoch 510 completed, average training loss is 0.13927304794080556\n",
            "Testing Acc.: 0.9548000025749207\n",
            "Epoch 511 completed, average training loss is 0.13850708274481197\n",
            "Testing Acc.: 0.9551000034809113\n",
            "Epoch 512 completed, average training loss is 0.13839942670116823\n",
            "Testing Acc.: 0.9561000013351441\n",
            "Epoch 513 completed, average training loss is 0.13688580955378712\n",
            "Testing Acc.: 0.9560000038146973\n",
            "Epoch 514 completed, average training loss is 0.13811133797901373\n",
            "Testing Acc.: 0.954600002169609\n",
            "Epoch 515 completed, average training loss is 0.13796265652403236\n",
            "Testing Acc.: 0.954400002360344\n",
            "Epoch 516 completed, average training loss is 0.13779685250173013\n",
            "Testing Acc.: 0.9542000049352646\n",
            "Epoch 517 completed, average training loss is 0.13785538273242612\n",
            "Testing Acc.: 0.9552000045776368\n",
            "Epoch 518 completed, average training loss is 0.13848308449300628\n",
            "Testing Acc.: 0.9562000012397767\n",
            "Epoch 519 completed, average training loss is 0.13729070383434494\n",
            "Testing Acc.: 0.9566000038385392\n",
            "Epoch 520 completed, average training loss is 0.13710150428737203\n",
            "Testing Acc.: 0.9560000032186509\n",
            "Epoch 521 completed, average training loss is 0.137362331704547\n",
            "Testing Acc.: 0.9555000048875809\n",
            "Epoch 522 completed, average training loss is 0.13673040205302336\n",
            "Testing Acc.: 0.9555999994277954\n",
            "Epoch 523 completed, average training loss is 0.1370155847351998\n",
            "Testing Acc.: 0.954700003862381\n",
            "Epoch 524 completed, average training loss is 0.1369017254561186\n",
            "Testing Acc.: 0.9561000007390976\n",
            "Epoch 525 completed, average training loss is 0.13738578790177902\n",
            "Testing Acc.: 0.956000000834465\n",
            "Epoch 526 completed, average training loss is 0.137266671055307\n",
            "Testing Acc.: 0.9571000027656555\n",
            "Epoch 527 completed, average training loss is 0.13661745539555947\n",
            "Testing Acc.: 0.9536000007390976\n",
            "Epoch 528 completed, average training loss is 0.13691369091471037\n",
            "Testing Acc.: 0.9570000040531158\n",
            "Epoch 529 completed, average training loss is 0.1363115857448429\n",
            "Testing Acc.: 0.9539000028371811\n",
            "Epoch 530 completed, average training loss is 0.1363021348354717\n",
            "Testing Acc.: 0.9571000015735627\n",
            "Epoch 531 completed, average training loss is 0.13690281194945175\n",
            "Testing Acc.: 0.9561000019311905\n",
            "Epoch 532 completed, average training loss is 0.13741554226415853\n",
            "Testing Acc.: 0.9556000030040741\n",
            "Epoch 533 completed, average training loss is 0.1362296294638266\n",
            "Testing Acc.: 0.9535000020265579\n",
            "Epoch 534 completed, average training loss is 0.13691635673244795\n",
            "Testing Acc.: 0.9554000043869019\n",
            "Epoch 535 completed, average training loss is 0.13647243580780924\n",
            "Testing Acc.: 0.9570000004768372\n",
            "Epoch 536 completed, average training loss is 0.13698807966895402\n",
            "Testing Acc.: 0.9562000012397767\n",
            "Epoch 537 completed, average training loss is 0.13647054238865772\n",
            "Testing Acc.: 0.957700001001358\n",
            "Epoch 538 completed, average training loss is 0.1366694578093787\n",
            "Testing Acc.: 0.9551000040769577\n",
            "Epoch 539 completed, average training loss is 0.13570779778063297\n",
            "Testing Acc.: 0.956600005030632\n",
            "Epoch 540 completed, average training loss is 0.1367641074893375\n",
            "Testing Acc.: 0.9566000038385392\n",
            "Epoch 541 completed, average training loss is 0.13591594809976718\n",
            "Testing Acc.: 0.9551000016927719\n",
            "Epoch 542 completed, average training loss is 0.13632151644056043\n",
            "Testing Acc.: 0.9547000002861022\n",
            "Epoch 543 completed, average training loss is 0.13539518808325132\n",
            "Testing Acc.: 0.9567000043392181\n",
            "Epoch 544 completed, average training loss is 0.13558866371400655\n",
            "Testing Acc.: 0.9569000029563903\n",
            "Epoch 545 completed, average training loss is 0.13532164628307025\n",
            "Testing Acc.: 0.9565000027418137\n",
            "Epoch 546 completed, average training loss is 0.13532412089717885\n",
            "Testing Acc.: 0.9571000015735627\n",
            "Epoch 547 completed, average training loss is 0.13501057367150981\n",
            "Testing Acc.: 0.957900003194809\n",
            "Epoch 548 completed, average training loss is 0.13534878219788274\n",
            "Testing Acc.: 0.9566000014543533\n",
            "Epoch 549 completed, average training loss is 0.13574823948244255\n",
            "Testing Acc.: 0.9562000036239624\n",
            "Epoch 550 completed, average training loss is 0.13520717111726602\n",
            "Testing Acc.: 0.9561000031232834\n",
            "Epoch 551 completed, average training loss is 0.13541257456255457\n",
            "Testing Acc.: 0.956300003528595\n",
            "Epoch 552 completed, average training loss is 0.13571800365423162\n",
            "Testing Acc.: 0.9572000032663346\n",
            "Epoch 553 completed, average training loss is 0.13502035986632108\n",
            "Testing Acc.: 0.9571000003814697\n",
            "Epoch 554 completed, average training loss is 0.13470934628508985\n",
            "Testing Acc.: 0.9569000017642975\n",
            "Epoch 555 completed, average training loss is 0.13528000559347372\n",
            "Testing Acc.: 0.9563999992609024\n",
            "Epoch 556 completed, average training loss is 0.13519811731763184\n",
            "Testing Acc.: 0.9570000034570694\n",
            "Epoch 557 completed, average training loss is 0.1352749500113229\n",
            "Testing Acc.: 0.9567000031471252\n",
            "Epoch 558 completed, average training loss is 0.13522053381117682\n",
            "Testing Acc.: 0.9565000033378601\n",
            "Epoch 559 completed, average training loss is 0.1350877699193855\n",
            "Testing Acc.: 0.9560000014305114\n",
            "Epoch 560 completed, average training loss is 0.1346984431395928\n",
            "Testing Acc.: 0.9555000019073486\n",
            "Epoch 561 completed, average training loss is 0.13462906633814176\n",
            "Testing Acc.: 0.9568000042438507\n",
            "Epoch 562 completed, average training loss is 0.13457898256989817\n",
            "Testing Acc.: 0.9589000022411347\n",
            "Epoch 563 completed, average training loss is 0.13470599272598824\n",
            "Testing Acc.: 0.955400003194809\n",
            "Epoch 564 completed, average training loss is 0.13482439269001284\n",
            "Testing Acc.: 0.9568000042438507\n",
            "Epoch 565 completed, average training loss is 0.13491775842383505\n",
            "Testing Acc.: 0.9578000026941299\n",
            "Epoch 566 completed, average training loss is 0.13521513178944589\n",
            "Testing Acc.: 0.9574000006914138\n",
            "Epoch 567 completed, average training loss is 0.1344764950685203\n",
            "Testing Acc.: 0.9548000019788742\n",
            "Epoch 568 completed, average training loss is 0.1344641606307899\n",
            "Testing Acc.: 0.9576000028848648\n",
            "Epoch 569 completed, average training loss is 0.13486514423352977\n",
            "Testing Acc.: 0.9561000019311905\n",
            "Epoch 570 completed, average training loss is 0.134868258042261\n",
            "Testing Acc.: 0.9581000012159347\n",
            "Epoch 571 completed, average training loss is 0.13386177171953023\n",
            "Testing Acc.: 0.9566000038385392\n",
            "Epoch 572 completed, average training loss is 0.1335006155197819\n",
            "Testing Acc.: 0.9564000010490418\n",
            "Epoch 573 completed, average training loss is 0.134080487874647\n",
            "Testing Acc.: 0.9551000010967254\n",
            "Epoch 574 completed, average training loss is 0.13395421656779946\n",
            "Testing Acc.: 0.9571000039577484\n",
            "Epoch 575 completed, average training loss is 0.1339991314553966\n",
            "Testing Acc.: 0.9571000003814697\n",
            "Epoch 576 completed, average training loss is 0.1336522520830234\n",
            "Testing Acc.: 0.9578000020980835\n",
            "Epoch 577 completed, average training loss is 0.13306825186436375\n",
            "Testing Acc.: 0.9578000044822693\n",
            "Epoch 578 completed, average training loss is 0.13357977479385832\n",
            "Testing Acc.: 0.9559000021219254\n",
            "Epoch 579 completed, average training loss is 0.13392805003561079\n",
            "Testing Acc.: 0.9544000029563904\n",
            "Epoch 580 completed, average training loss is 0.13432978885869185\n",
            "Testing Acc.: 0.9549000030755996\n",
            "Epoch 581 completed, average training loss is 0.13221702344094713\n",
            "Testing Acc.: 0.9569000035524369\n",
            "Epoch 582 completed, average training loss is 0.1335895977045099\n",
            "Testing Acc.: 0.9562000036239624\n",
            "Epoch 583 completed, average training loss is 0.13288857558431724\n",
            "Testing Acc.: 0.9569000029563903\n",
            "Epoch 584 completed, average training loss is 0.13487893564626574\n",
            "Testing Acc.: 0.9576000016927719\n",
            "Epoch 585 completed, average training loss is 0.1336727677596112\n",
            "Testing Acc.: 0.9584000033140182\n",
            "Epoch 586 completed, average training loss is 0.13317242378989855\n",
            "Testing Acc.: 0.9576000016927719\n",
            "Epoch 587 completed, average training loss is 0.133569259972622\n",
            "Testing Acc.: 0.9578000044822693\n",
            "Epoch 588 completed, average training loss is 0.133205485890309\n",
            "Testing Acc.: 0.9590000039339066\n",
            "Epoch 589 completed, average training loss is 0.13348590314233055\n",
            "Testing Acc.: 0.957200003862381\n",
            "Epoch 590 completed, average training loss is 0.13264740861020982\n",
            "Testing Acc.: 0.9576000034809112\n",
            "Epoch 591 completed, average training loss is 0.13280585346743465\n",
            "Testing Acc.: 0.9556000012159348\n",
            "Epoch 592 completed, average training loss is 0.13334998195370037\n",
            "Testing Acc.: 0.9564000040292739\n",
            "Epoch 593 completed, average training loss is 0.1327788587814818\n",
            "Testing Acc.: 0.9563000041246414\n",
            "Epoch 594 completed, average training loss is 0.13322479339937368\n",
            "Testing Acc.: 0.9564000034332275\n",
            "Epoch 595 completed, average training loss is 0.13241563367036482\n",
            "Testing Acc.: 0.9585000038146972\n",
            "Epoch 596 completed, average training loss is 0.13292968262918292\n",
            "Testing Acc.: 0.9583000022172928\n",
            "Epoch 597 completed, average training loss is 0.13305514705988267\n",
            "Testing Acc.: 0.9565000027418137\n",
            "Epoch 598 completed, average training loss is 0.1329563019114236\n",
            "Testing Acc.: 0.9574000006914138\n",
            "Epoch 599 completed, average training loss is 0.1326104398444295\n",
            "Testing Acc.: 0.9584000021219253\n",
            "Epoch 600 completed, average training loss is 0.132512315257142\n",
            "Testing Acc.: 0.9568000030517578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PE8ZuvI1UTfs",
        "colab": {}
      },
      "source": [
        "# ^^^^ Above reads:\n",
        "### Epoch 600 completed, average training loss is 0.12156973460689187\n",
        "### Testing Acc.: 0.9526000034809112\n",
        "\n",
        "# Save model, for not having to fully retrain it each time.\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, '3bits/model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZ02lgi_UTfy",
        "colab": {}
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOLt8Og6UTf3",
        "colab": {}
      },
      "source": [
        "!zip -r 3bits.zip 3bits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NFSjOlRmUTf-"
      },
      "source": [
        "## 3.3 Weights rounding: reg then bin ....% / bin then reg ....%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMnXlxmzUTgA",
        "colab": {}
      },
      "source": [
        "!unzip 3bits.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwbiZPy4UTgE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0aaa2d9-9a31-48b6-d277-3876f79d4c70"
      },
      "source": [
        "# Quantize a [0, +127] value on 3 bits, so that\n",
        "# it fits in 3 DREGs\n",
        "# 3 DREGs for the most significant bits\n",
        "def quantization_3bits_unsigned(tensor):\n",
        "  sign = tf.sign(tensor)\n",
        "  \n",
        "  cast = tf.cast(sign*tensor, tf.int32)\n",
        "  sign = tf.cast(sign, tf.int32)\n",
        "  \n",
        "  digitized = (64*tf.mod(tf.bitwise.right_shift(cast,6), 2) +\n",
        "               32*tf.mod(tf.bitwise.right_shift(cast,5), 2) +\n",
        "               16*tf.mod(tf.bitwise.right_shift(cast,4), 2))\n",
        "  digitized = digitized * sign  \n",
        "  return tf.cast(digitized, tf.float32)\n",
        "\n",
        "# Sigmoids, whose steepness and bias can be adjusted\n",
        "# A very steep sigmoid (high bin_rate) simulates a binarization\n",
        "MAX_BIN_RATE = 50\n",
        "def binarize_tensor_differentiable(input, thresh, bin_rate):\n",
        "  out1 = tf.nn.sigmoid(bin_rate*(input[...,:1] - thresh[0]))\n",
        "  out2 = tf.nn.sigmoid(bin_rate*(input[...,1:2] - thresh[1]))\n",
        "  out3 = tf.nn.sigmoid(bin_rate*(input[...,2:3] - thresh[2]))\n",
        "  return tf.concat([out1, out2, out3], axis=-1)\n",
        "\n",
        "def custom_pooling(conv_bin):\n",
        "  sum1 = tf.reduce_sum(conv_bin[:,5:14,0:9,:], axis=[1,2])\n",
        "  sum2 = tf.reduce_sum(conv_bin[:,14:23,0:9,:], axis=[1,2])\n",
        "  sum3 = tf.reduce_sum(conv_bin[:,0:9,5:14,:], axis=[1,2])\n",
        "  sum4 = tf.reduce_sum(conv_bin[:,5:14,5:14,:], axis=[1,2])\n",
        "  sum5 = tf.reduce_sum(conv_bin[:,14:23,5:14,:], axis=[1,2])\n",
        "  sum6 = tf.reduce_sum(conv_bin[:,19:28,5:14,:], axis=[1,2])\n",
        "  sum7 = tf.reduce_sum(conv_bin[:,0:9,14:23,:], axis=[1,2])\n",
        "  sum8 = tf.reduce_sum(conv_bin[:,5:14,14:23,:], axis=[1,2])\n",
        "  sum9 = tf.reduce_sum(conv_bin[:,14:23,14:23,:], axis=[1,2])\n",
        "  sum10 = tf.reduce_sum(conv_bin[:,19:28,14:23,:], axis=[1,2])\n",
        "  sum11 = tf.reduce_sum(conv_bin[:,5:14,19:28,:], axis=[1,2])\n",
        "  sum12 = tf.reduce_sum(conv_bin[:,14:23,19:28,:], axis=[1,2])\n",
        "  \n",
        "  pool = tf.concat([sum1, sum2, sum3, sum4, sum5, sum6, \n",
        "                       sum7, sum8, sum9, sum10, sum11, sum12], axis=1)\n",
        "  \n",
        "  return pool\n",
        "\n",
        "def network3(input, thresh, bin_rate_ph):\n",
        "  # First Convolution\n",
        "  conv = slim.conv2d(input, 3, [3, 3], rate=1, activation_fn=tf.nn.relu,\n",
        "                     padding='SAME', scope='conv1')\n",
        "  \n",
        "  # Second Convolution\n",
        "  # C2.1, none of the input is quantized\n",
        "  conv2_1 = slim.conv2d(conv, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.1')\n",
        "  \n",
        "  # C2.2, two inputs (out of 3) are quantized\n",
        "  slice_A_2_2 = quantization_3bits_unsigned(conv[...,:2])\n",
        "  slice_B_2_2 = conv[...,2:]\n",
        "  conv2_2_input = tf.concat([slice_A_2_2, slice_B_2_2], axis=-1)\n",
        "  conv2_2 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.2')\n",
        "  \n",
        "  # C2.3, same inputs as for C2.2 (2 feature maps quantified out of 3)\n",
        "  conv2_3 = slim.conv2d(conv2_2_input, 1, [3, 3], rate=1,\n",
        "                        activation_fn=None, biases_initializer=None,\n",
        "                        padding='SAME', scope='conv2.3')\n",
        "  \n",
        "  \n",
        "  conv2 = tf.concat([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "  \n",
        "  # Sigmoid as output binarisation\n",
        "  # Thresholds act as bias here\n",
        "  conv2 = binarize_tensor_differentiable(conv2, thresh, bin_rate_ph)\n",
        "  \n",
        "  # Sum pooling\n",
        "  pool = custom_pooling(conv2)\n",
        "  \n",
        "  # Flatten + dense\n",
        "  flat = tf.layers.flatten(pool)\n",
        "  dense = tf.layers.dense(flat, 50, name='dense1', activation=tf.nn.relu)\n",
        "  out = tf.layers.dense(dense, 10, name='dense2')\n",
        "  \n",
        "  return out\n",
        "\n",
        "## Define the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "in_image_ph = tf.placeholder(tf.float32, [BATCH_SIZE,28,28,1])\n",
        "bin_rate_ph = tf.placeholder(tf.float32, ())\n",
        "gt_label_ph = tf.placeholder(tf.uint8)\n",
        "thresh = tf.Variable(tf.random.normal([3]), name='out_thresholds')\n",
        "\n",
        "out_label_op = network3(in_image_ph, thresh, bin_rate_ph)\n",
        "\n",
        "pred_op = tf.dtypes.cast(\n",
        "            tf.keras.backend.argmax(out_label_op),\n",
        "            tf.uint8)\n",
        "\n",
        "loss_op = tf.reduce_mean(\n",
        "          tf.keras.backend.sparse_categorical_crossentropy(gt_label_ph,\n",
        "                                                           out_label_op,\n",
        "                                                           from_logits=True))\n",
        "\n",
        "acc_op = tf.contrib.metrics.accuracy(gt_label_ph, pred_op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#####################################################\n",
        "#####################################################\n",
        "#####################################################\n",
        "#####################################################\n",
        "# Which convolution kernel weights to load: the 3 or 4 bits version?\n",
        "#ckpt = tf.train.get_checkpoint_state('3bits')\n",
        "ckpt = tf.train.get_checkpoint_state('4bits')\n",
        "saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9703000062704086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bi3F4CMaUTgK",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('conv1', reuse=True) as scope_conv:\n",
        "  w1 = tf.get_variable('weights')\n",
        "  b1 = tf.get_variable('biases')\n",
        "with tf.variable_scope('conv2.1', reuse=True) as scope_conv:\n",
        "  w2_1 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.2', reuse=True) as scope_conv:\n",
        "  w2_2 = tf.get_variable('weights')\n",
        "with tf.variable_scope('conv2.3', reuse=True) as scope_conv:\n",
        "  w2_3 = tf.get_variable('weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMPITVzWUTgN",
        "colab": {}
      },
      "source": [
        "# Define regularizers\n",
        "ROUNDING_STEP_CONV = 0.25\n",
        "ROUNDING_STEP_BIAS = 1.\n",
        "REG_CONSTANT = 4."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dFSlzAzhUTgT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "150bd380-c69b-4286-ed08-1d34d213f10b"
      },
      "source": [
        "# Rounding operation\n",
        "rounding_weights1_op = tf.assign(w1, \n",
        "                          tf.round(w1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_bias1_op = tf.assign(b1,\n",
        "                          tf.round(b1/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "rounding_weights2_1_op = tf.assign(w2_1, \n",
        "                          tf.round(w2_1/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_weights2_2_op = tf.assign(w2_2, \n",
        "                          tf.round(w2_2/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_weights2_3_op = tf.assign(w2_3, \n",
        "                          tf.round(w2_3/ROUNDING_STEP_CONV)*ROUNDING_STEP_CONV)\n",
        "rounding_thresh_op = tf.assign(thresh,\n",
        "                          tf.round(thresh/ROUNDING_STEP_BIAS)*ROUNDING_STEP_BIAS)\n",
        "_ = sess.run([rounding_weights1_op, rounding_bias1_op])\n",
        "_ = sess.run([rounding_weights2_1_op, rounding_weights2_2_op])\n",
        "_ = sess.run([rounding_weights2_3_op, rounding_thresh_op])\n",
        "\n",
        "\n",
        "# Show final distribution of weights\n",
        "w1_values, b1_values = sess.run([w1, b1])\n",
        "w2_1_values, w2_2_values = sess.run([w2_1, w2_2])\n",
        "w2_3_values, thresh_values = sess.run([w2_3, thresh])\n",
        "\n",
        "kernel_values = (list(w1_values.flatten()) + list(b1_values.flatten())\n",
        "                 + list(w2_1_values.flatten())\n",
        "                 + list(w2_2_values.flatten())\n",
        "                 + list(w2_3_values.flatten())\n",
        "                 + list(thresh_values.flatten()))\n",
        "fig = plt.figure()\n",
        "plt.scatter(kernel_values, [1]*len(kernel_values))\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.8209999978542328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dYRvB4DAUTgc"
      },
      "source": [
        "## 3.4 Retrain FC: 94.9% with 3 bits weights, 96.9% when re-using the 4 bits weights with 3 bits quantisation only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-cPJmfxJUTgd",
        "colab": {}
      },
      "source": [
        "LR = 0.001/4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_6QWo8kQUTgg",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('dense1', reuse=True) as scope_conv:\n",
        "  fc1_k = tf.get_variable('kernel')\n",
        "  fc1_b = tf.get_variable('bias')\n",
        "\n",
        "with tf.variable_scope('dense2', reuse=True) as scope_conv:\n",
        "  fc2_k = tf.get_variable('kernel')\n",
        "  fc2_b = tf.get_variable('bias')\n",
        "\n",
        "opt_fc = tf.train.AdamOptimizer(learning_rate=LR, name='Adam_reg')\n",
        "opt_fc_op = opt_fc.minimize(loss_op, var_list=[fc1_k, fc1_b,\n",
        "                                               fc2_k, fc2_b])\n",
        "\n",
        "sess.run(tf.variables_initializer(opt_fc.variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hVZJVxqfUTgj",
        "colab": {}
      },
      "source": [
        "def test_accuracy_bin_rate(bin_rate_feed):\n",
        "  accs = np.zeros(x_test.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_test.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    \n",
        "    xs = np.expand_dims(x_test[start:stop],-1) * INPUT_SCALING\n",
        "    ys = y_test[start:stop]\n",
        "    \n",
        "    current_acc = sess.run(acc_op,\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "    accs[i] = current_acc\n",
        "  \n",
        "  print('Testing Acc.: {}'.format(\n",
        "        accs.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-30YPmdUTgm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3e6c8b1-b326-42e2-a108-39d9061f0a61"
      },
      "source": [
        "bin_rate_feed = MAX_BIN_RATE + 1\n",
        "\n",
        "for epoch in range(EPOCHS*2):\n",
        "\n",
        "  random_perm = np.random.permutation(x_train.shape[0])\n",
        "  losses = np.zeros(x_train.shape[0] // BATCH_SIZE)\n",
        "  for i in range(x_train.shape[0] // BATCH_SIZE):\n",
        "    start = i * BATCH_SIZE\n",
        "    stop = start + BATCH_SIZE\n",
        "    selected = random_perm[start:stop]\n",
        "\n",
        "    xs = np.expand_dims(x_train[selected],-1) * INPUT_SCALING\n",
        "    ys = y_train[selected]\n",
        "\n",
        "    _, current_loss = sess.run([opt_fc_op, loss_op],\n",
        "                       feed_dict={in_image_ph: xs,\n",
        "                                  gt_label_ph: ys,\n",
        "                                  bin_rate_ph: bin_rate_feed})\n",
        "\n",
        "    losses[i] = current_loss\n",
        "\n",
        "  print('Epoch {} completed, average training loss is {}'.format(\n",
        "          epoch+1, losses.mean()))\n",
        "  test_accuracy_bin_rate(bin_rate_feed)\n",
        "\n",
        "test_accuracy_bin_rate(MAX_BIN_RATE + 1)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, average training loss is 0.09279116209130735\n",
            "Testing Acc.: 0.9695000052452087\n",
            "Epoch 2 completed, average training loss is 0.09228452049971869\n",
            "Testing Acc.: 0.9679000067710877\n",
            "Epoch 3 completed, average training loss is 0.09224487382142495\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 4 completed, average training loss is 0.09229470319890727\n",
            "Testing Acc.: 0.969800009727478\n",
            "Epoch 5 completed, average training loss is 0.09240733513142914\n",
            "Testing Acc.: 0.9681000059843063\n",
            "Epoch 6 completed, average training loss is 0.09212511174070338\n",
            "Testing Acc.: 0.9697000080347061\n",
            "Epoch 7 completed, average training loss is 0.0921264547482133\n",
            "Testing Acc.: 0.9664000052213669\n",
            "Epoch 8 completed, average training loss is 0.09226821243297309\n",
            "Testing Acc.: 0.9689000076055527\n",
            "Epoch 9 completed, average training loss is 0.09223237356403842\n",
            "Testing Acc.: 0.9670000070333481\n",
            "Epoch 10 completed, average training loss is 0.09257851402740926\n",
            "Testing Acc.: 0.9691000074148178\n",
            "Epoch 11 completed, average training loss is 0.0921416396057854\n",
            "Testing Acc.: 0.9694000083208084\n",
            "Epoch 12 completed, average training loss is 0.09214453427121043\n",
            "Testing Acc.: 0.9701000076532363\n",
            "Epoch 13 completed, average training loss is 0.09282922237024953\n",
            "Testing Acc.: 0.9680000060796737\n",
            "Epoch 14 completed, average training loss is 0.09261161687939118\n",
            "Testing Acc.: 0.9688000071048737\n",
            "Epoch 15 completed, average training loss is 0.09196121583692729\n",
            "Testing Acc.: 0.9690000051259995\n",
            "Epoch 16 completed, average training loss is 0.09220285282470286\n",
            "Testing Acc.: 0.9701000070571899\n",
            "Epoch 17 completed, average training loss is 0.09245660212822258\n",
            "Testing Acc.: 0.9697000050544738\n",
            "Epoch 18 completed, average training loss is 0.0916324003227055\n",
            "Testing Acc.: 0.9673000037670135\n",
            "Epoch 19 completed, average training loss is 0.09200718365764866\n",
            "Testing Acc.: 0.969400007724762\n",
            "Epoch 20 completed, average training loss is 0.09232204815528045\n",
            "Testing Acc.: 0.9688000065088272\n",
            "Epoch 21 completed, average training loss is 0.0920686380378902\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 22 completed, average training loss is 0.09195937533862888\n",
            "Testing Acc.: 0.9696000063419342\n",
            "Epoch 23 completed, average training loss is 0.09213248210493476\n",
            "Testing Acc.: 0.9694000089168548\n",
            "Epoch 24 completed, average training loss is 0.09114345230938246\n",
            "Testing Acc.: 0.9704000091552735\n",
            "Epoch 25 completed, average training loss is 0.09178582780994475\n",
            "Testing Acc.: 0.9689000076055527\n",
            "Epoch 26 completed, average training loss is 0.09211458364967257\n",
            "Testing Acc.: 0.970400008559227\n",
            "Epoch 27 completed, average training loss is 0.0913700772402808\n",
            "Testing Acc.: 0.9694000053405761\n",
            "Epoch 28 completed, average training loss is 0.09183508040073017\n",
            "Testing Acc.: 0.9695000064373016\n",
            "Epoch 29 completed, average training loss is 0.09176723203544195\n",
            "Testing Acc.: 0.9706000065803528\n",
            "Epoch 30 completed, average training loss is 0.0917080447760721\n",
            "Testing Acc.: 0.9698000103235245\n",
            "Epoch 31 completed, average training loss is 0.0911478526191786\n",
            "Testing Acc.: 0.9672000068426132\n",
            "Epoch 32 completed, average training loss is 0.09172262939934929\n",
            "Testing Acc.: 0.9699000090360641\n",
            "Epoch 33 completed, average training loss is 0.09195824969870349\n",
            "Testing Acc.: 0.9700000077486038\n",
            "Epoch 34 completed, average training loss is 0.09188124250154943\n",
            "Testing Acc.: 0.9686000049114227\n",
            "Epoch 35 completed, average training loss is 0.09196913214555631\n",
            "Testing Acc.: 0.9677000051736832\n",
            "Epoch 36 completed, average training loss is 0.09175415620751058\n",
            "Testing Acc.: 0.9703000062704086\n",
            "Epoch 37 completed, average training loss is 0.0917566991224885\n",
            "Testing Acc.: 0.9696000069379807\n",
            "Epoch 38 completed, average training loss is 0.09153003528869401\n",
            "Testing Acc.: 0.968600006699562\n",
            "Epoch 39 completed, average training loss is 0.09162758148585756\n",
            "Testing Acc.: 0.9687000072002411\n",
            "Epoch 40 completed, average training loss is 0.09144926490727812\n",
            "Testing Acc.: 0.9706000071763993\n",
            "Epoch 41 completed, average training loss is 0.09161288799407581\n",
            "Testing Acc.: 0.9679000073671341\n",
            "Epoch 42 completed, average training loss is 0.09150780210271478\n",
            "Testing Acc.: 0.969500008225441\n",
            "Epoch 43 completed, average training loss is 0.09166212224246313\n",
            "Testing Acc.: 0.9689000070095062\n",
            "Epoch 44 completed, average training loss is 0.09115152748456845\n",
            "Testing Acc.: 0.9696000093221664\n",
            "Epoch 45 completed, average training loss is 0.09182037005200983\n",
            "Testing Acc.: 0.9693000060319901\n",
            "Epoch 46 completed, average training loss is 0.09185675336048008\n",
            "Testing Acc.: 0.9703000074625016\n",
            "Epoch 47 completed, average training loss is 0.09191150305482249\n",
            "Testing Acc.: 0.9692000061273575\n",
            "Epoch 48 completed, average training loss is 0.09090424504751961\n",
            "Testing Acc.: 0.9669000023603439\n",
            "Epoch 49 completed, average training loss is 0.09181047520289819\n",
            "Testing Acc.: 0.9698000073432922\n",
            "Epoch 50 completed, average training loss is 0.09113357338588685\n",
            "Testing Acc.: 0.9689000064134597\n",
            "Epoch 51 completed, average training loss is 0.09134205104239906\n",
            "Testing Acc.: 0.9697000062465668\n",
            "Epoch 52 completed, average training loss is 0.09163485600768279\n",
            "Testing Acc.: 0.9686000061035156\n",
            "Epoch 53 completed, average training loss is 0.09171486970968544\n",
            "Testing Acc.: 0.9680000078678132\n",
            "Epoch 54 completed, average training loss is 0.09110827457703029\n",
            "Testing Acc.: 0.9689000082015992\n",
            "Epoch 55 completed, average training loss is 0.09080743631813676\n",
            "Testing Acc.: 0.9692000037431717\n",
            "Epoch 56 completed, average training loss is 0.09170914895522098\n",
            "Testing Acc.: 0.9686000072956085\n",
            "Epoch 57 completed, average training loss is 0.09232306376254806\n",
            "Testing Acc.: 0.9692000049352646\n",
            "Epoch 58 completed, average training loss is 0.09118491899687797\n",
            "Testing Acc.: 0.9691000086069107\n",
            "Epoch 59 completed, average training loss is 0.09112798611323039\n",
            "Testing Acc.: 0.9680000048875809\n",
            "Epoch 60 completed, average training loss is 0.09092672285158188\n",
            "Testing Acc.: 0.9684000074863434\n",
            "Epoch 61 completed, average training loss is 0.09198648299711445\n",
            "Testing Acc.: 0.9681000059843063\n",
            "Epoch 62 completed, average training loss is 0.09147425622989734\n",
            "Testing Acc.: 0.967700006365776\n",
            "Epoch 63 completed, average training loss is 0.09243985653001194\n",
            "Testing Acc.: 0.96910001039505\n",
            "Epoch 64 completed, average training loss is 0.09090906671325986\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 65 completed, average training loss is 0.09141329812662055\n",
            "Testing Acc.: 0.9701000070571899\n",
            "Epoch 66 completed, average training loss is 0.0914404989623775\n",
            "Testing Acc.: 0.9700000077486038\n",
            "Epoch 67 completed, average training loss is 0.09172427256824449\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 68 completed, average training loss is 0.09135229237688085\n",
            "Testing Acc.: 0.9675000041723252\n",
            "Epoch 69 completed, average training loss is 0.09156460171565413\n",
            "Testing Acc.: 0.9698000067472458\n",
            "Epoch 70 completed, average training loss is 0.09126670393006255\n",
            "Testing Acc.: 0.9693000060319901\n",
            "Epoch 71 completed, average training loss is 0.0912637192569673\n",
            "Testing Acc.: 0.9678000062704086\n",
            "Epoch 72 completed, average training loss is 0.09130563605266313\n",
            "Testing Acc.: 0.9697000068426133\n",
            "Epoch 73 completed, average training loss is 0.09057571262586862\n",
            "Testing Acc.: 0.9690000075101852\n",
            "Epoch 74 completed, average training loss is 0.09101130342266212\n",
            "Testing Acc.: 0.9691000068187714\n",
            "Epoch 75 completed, average training loss is 0.09120030430766443\n",
            "Testing Acc.: 0.9690000069141388\n",
            "Epoch 76 completed, average training loss is 0.09116460313865295\n",
            "Testing Acc.: 0.9640000057220459\n",
            "Epoch 77 completed, average training loss is 0.09101407404212902\n",
            "Testing Acc.: 0.9699000090360641\n",
            "Epoch 78 completed, average training loss is 0.09114668619818986\n",
            "Testing Acc.: 0.9715000069141388\n",
            "Epoch 79 completed, average training loss is 0.09102050650554398\n",
            "Testing Acc.: 0.9688000059127808\n",
            "Epoch 80 completed, average training loss is 0.09115538376073043\n",
            "Testing Acc.: 0.9684000074863434\n",
            "Epoch 81 completed, average training loss is 0.09047844140014301\n",
            "Testing Acc.: 0.968500007390976\n",
            "Epoch 82 completed, average training loss is 0.09101372589046756\n",
            "Testing Acc.: 0.9659000051021576\n",
            "Epoch 83 completed, average training loss is 0.09167481562588364\n",
            "Testing Acc.: 0.9671000075340271\n",
            "Epoch 84 completed, average training loss is 0.09076478577995052\n",
            "Testing Acc.: 0.9703000068664551\n",
            "Epoch 85 completed, average training loss is 0.09061713157687336\n",
            "Testing Acc.: 0.9700000077486038\n",
            "Epoch 86 completed, average training loss is 0.09055161208845675\n",
            "Testing Acc.: 0.9687000066041946\n",
            "Epoch 87 completed, average training loss is 0.090969285525692\n",
            "Testing Acc.: 0.9696000063419342\n",
            "Epoch 88 completed, average training loss is 0.09122280372927587\n",
            "Testing Acc.: 0.9688000041246414\n",
            "Epoch 89 completed, average training loss is 0.09060520773132642\n",
            "Testing Acc.: 0.9694000053405761\n",
            "Epoch 90 completed, average training loss is 0.09105624665816625\n",
            "Testing Acc.: 0.9685000067949295\n",
            "Epoch 91 completed, average training loss is 0.09061957362884035\n",
            "Testing Acc.: 0.970400007367134\n",
            "Epoch 92 completed, average training loss is 0.09042095702219134\n",
            "Testing Acc.: 0.9692000079154969\n",
            "Epoch 93 completed, average training loss is 0.09108188172569498\n",
            "Testing Acc.: 0.9710000103712082\n",
            "Epoch 94 completed, average training loss is 0.09033826525012652\n",
            "Testing Acc.: 0.9698000067472458\n",
            "Epoch 95 completed, average training loss is 0.09084898083160321\n",
            "Testing Acc.: 0.969400007724762\n",
            "Epoch 96 completed, average training loss is 0.09070300998166203\n",
            "Testing Acc.: 0.9686000055074692\n",
            "Epoch 97 completed, average training loss is 0.09105133731073389\n",
            "Testing Acc.: 0.9692000073194503\n",
            "Epoch 98 completed, average training loss is 0.09105172981818517\n",
            "Testing Acc.: 0.9689000058174133\n",
            "Epoch 99 completed, average training loss is 0.09061765208374709\n",
            "Testing Acc.: 0.9673000073432922\n",
            "Epoch 100 completed, average training loss is 0.0905652918231984\n",
            "Testing Acc.: 0.9706000077724457\n",
            "Testing Acc.: 0.9706000077724457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TIgjKPTzUTgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0ada81ab-2d08-4227-edcc-880f8e7598eb"
      },
      "source": [
        "test_accuracy_bin_rate(MAX_BIN_RATE)\n",
        "test_accuracy_bin_rate(5000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc.: 0.9706000077724457\n",
            "Testing Acc.: 0.9690000081062317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5SyoX_LuUTgz",
        "colab": {}
      },
      "source": [
        "# Get final distribution of weights\n",
        "w1_values, b1_values = sess.run([w1, b1])\n",
        "w2_1_values, w2_2_values = sess.run([w2_1, w2_2])\n",
        "w2_3_values, thresh_values = sess.run([w2_3, thresh])\n",
        "\n",
        "with open('NP_KERNELS_WEIGHTS.pck', 'wb') as f:\n",
        "  pickle.dump((w1_values, b1_values, w2_1_values, w2_2_values, w2_3_values, thresh_values), f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}